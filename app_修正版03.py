# -*- coding: utf-8 -*-
"""
ä¸ªäººç¿»è¯‘çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ(ä¿®æ­£ç‰ˆ03)
- Tab1 ğŸ“‚ ç¿»è¯‘é¡¹ç›®ç®¡ç†:æ–°å»ºé¡¹ç›®ã€æ–‡ä»¶ä¸Šä¼ ã€æ‰§è¡Œç¿»è¯‘(DeepSeek API).å¯¼å‡ºå¯¹ç…§/åŸæ ¼å¼.å†™å…¥å†å²
- Tab2 ğŸ“˜ æœ¯è¯­åº“ç®¡ç†:æŸ¥è¯¢/ç¼–è¾‘/åˆ é™¤ã€CSVæ‰¹é‡å¯¼å…¥ã€ç»Ÿè®¡/å¯¼å‡ºã€å¿«é€Ÿæœç´¢ã€æ‰¹é‡æŒ‚æ¥é¡¹ç›®ã€å†å²æŠ½å–æœ¯è¯­ã€åˆ†ç±»ç®¡ç†
- Tab3 ğŸ“Š ç¿»è¯‘å†å²:æŸ¥çœ‹ã€ä¸‹è½½è¯‘æ–‡(ç®€ç‰ˆå ä½.æŒ‰éœ€æ‰©å±•)
- Tab4 ğŸ“š è¯­æ–™åº“ç®¡ç†:æ–°å¢/æ£€ç´¢/åˆå¹¶/Few-shot æ³¨å…¥
- Tab5 âš™ è®¾ç½®: DeepSeek Key æç¤º
"""

# ==== stdlib ====
import os
import re
import io
import sys
import json
import uuid
import time
import sqlite3
import streamlit as st
import pandas as pd
from pathlib import Path
from datetime import datetime

# è®©åŒç›®å½•ä¸‹çš„ kb_dynamic.py å¯è¢«å¯¼å…¥(å¦‚æœå­˜åœ¨)
sys.path.append(os.path.dirname(__file__))

# ======== åŸºæœ¬è·¯å¾„è®¾ç½® ========

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(BASE_DIR, "kb.db")

PROJECT_DIR = os.path.join(BASE_DIR, "uploads")
os.makedirs(PROJECT_DIR, exist_ok=True)

INDEX_DIR = Path(BASE_DIR) / ".cache_index"
INDEX_DIR.mkdir(exist_ok=True)

# ---------- ä¼˜åŒ–:ç»™æœ¯è¯­è¡¨ project_id å»ºç´¢å¼• ----------
def _index_paths(project_id: int):
    base_dir = os.path.join(BASE_DIR, ".cache_index")
    os.makedirs(base_dir, exist_ok=True)
    return (
        os.path.join(base_dir, f"faiss_{project_id}.bin"),
        os.path.join(base_dir, f"vecmap_{project_id}.json"),
        os.path.join(base_dir, f"vectors_{project_id}.npy"),
    )

# ==== third-party ====
try:
    from docx import Document  # åœ¨éœ€è¦å¤„ä»ä¼š try/except
except Exception:
    Document = None

# ========== é¡µé¢è®¾ç½® ==========
st.set_page_config(page_title="ä¸ªäººç¿»è¯‘çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ Â· ä¿®æ­£ç‰ˆ03", layout="wide")

# ========== kb_dynamic (å¯é€‰) ==========
KBEmbedder = None
recommend_for_segment = None
build_prompt_strict = None
build_prompt_soft = None
try:
    from kb_dynamic import (
        KBEmbedder as _KBEmbedder,
        recommend_for_segment as _recommend_for_segment,
        build_prompt_strict as _build_prompt_strict,
        build_prompt_soft as _build_prompt_soft,
    )
    KBEmbedder = _KBEmbedder
    recommend_for_segment = _recommend_for_segment
    build_prompt_strict = _build_prompt_strict
    build_prompt_soft = _build_prompt_soft
except Exception:
    pass  # å…è®¸ç¼ºå¤±;åŠ¨æ€æœ¯è¯­æ¨èåŠŸèƒ½å°†è‡ªåŠ¨é™çº§

# ========== å·¥å…·å‡½æ•° ==========
def make_sk(prefix: str):
    """è¿”å›ä¸€ä¸ªå¸¦æœ‰å‰ç¼€çš„ key ç”Ÿæˆå™¨"""
    return lambda name, id=None: f"{prefix}_{name}_{id}" if id else f"{prefix}_{name}"
# å…¨å±€é»˜è®¤ key ç”Ÿæˆå™¨(æ›¿ä»£è¢«åˆ é™¤çš„è®¡æ•°å™¨ç‰ˆ sk)
sk = make_sk("global")

def render_table(df, *, key=None, hide_index=True, editable=False):
    """
    ç»Ÿä¸€æ¸²æŸ“è¡¨æ ¼(å¯¹æ—§/æ–° Streamlit éƒ½å®‰å…¨):
    - editable=False: åªè¯»(ç”¨ data_editor disabled=True ä»¥ä¿ç•™ key)
    - editable=True : å¯ç¼–è¾‘
    - ä¸å†ä¼  width å‚æ•°.é¿å… 'str' as int çš„æŠ¥é”™
    """
    try:
        if editable:
            return st.data_editor(
                df,
                hide_index=hide_index,
                key=key,
            )
        if key is not None:
            return st.data_editor(
                df,
                hide_index=hide_index,
                disabled=True,
                key=key,
            )
        return st.dataframe(df, hide_index=hide_index)
    except TypeError:
        return st.data_editor(
            df,
            hide_index=hide_index,
            disabled=not editable,
            key=key,
        )

# ======= è·å–æŸæ¡å†å²è®°å½•å¯¹åº”çš„åŸæ–‡(ä¼˜å…ˆ items.body.å…œåº• src_path ä»…ä½œä¸ºæ ‡é¢˜æç¤º)=======
def _get_source_text_for_history(cur, project_id):
    row = cur.execute("SELECT body FROM items WHERE id=?", (project_id,)).fetchone()
    return (row[0] if row and row[0] else "") or ""

# ======= è½»é‡æœ¯è¯­å€™é€‰(ä¸­è‹±éƒ½å¯;ä½ åç»­å¯æ¢æˆ DeepSeek æŠ½å–)=======
def _simple_term_candidates(text, topn=50):
    import re, collections
    # very light: è‹±æ–‡æŒ‰è¯.ä¸­æ–‡ç”¨ç®€å•æ­£åˆ™;åœç”¨è¯å¯æŒ‰éœ€æ‰©å±•
    STOP_EN = set("a an the and or of to in for on with by from as at is are was were be been being this that these those it its".split())
    is_en = len(re.sub(r'[^A-Za-z]', '', text)) >= len(re.sub(r'[^\u4e00-\u9fff]', '', text))
    if is_en:
        toks = [t.lower() for t in re.findall(r"[A-Za-z][A-Za-z\-']+", text)]
        toks = [t for t in toks if t not in STOP_EN and len(t) > 2]
    else:
        import jieba
        toks = [t.strip() for t in jieba.cut(text) if re.search(r"[\u4e00-\u9fff]", t)]
        toks = [t for t in toks if len(t) >= 2 and not re.match(r"^[0-9]+$", t)]
    cnt = collections.Counter(toks)
    return [{"term": k, "freq": v} for k, v in cnt.most_common(topn)]

# ======= å¯¹é½å¹¶å¯¼å‡º(ä¾èµ–ä½ å·²æœ‰çš„ split_blocks / align_export)=======
def _split_bilingual_pairs(split_blocks, src_text, tgt_text):
    src_blocks = split_blocks(src_text, max_len=1200)
    tgt_blocks = split_blocks(tgt_text, max_len=1200)
    # ç®€å•å¯¹é½:æŒ‰ zip å¯¹é½;é•¿åº¦ä¸ç­‰æ—¶ä»¥è¾ƒçŸ­ä¸ºå‡†
    n = min(len(src_blocks), len(tgt_blocks))
    return list(zip(src_blocks[:n], tgt_blocks[:n]))

def quick_diagnose_vectors(pid: int):
    """
    æ‰“å°/æç¤ºé¡¹ç›®å‘é‡ç´¢å¼•çŠ¶æ€.å¸®åŠ©æ’æŸ¥â€œæ£€ç´¢ä¸ºç©º/ç»´åº¦ä¸åŒ¹é…/æœªå»ºç´¢å¼•â€ç­‰é—®é¢˜ã€‚
    """
    try:
        mode, index, mapping, vecs = _load_index(pid)
        if mode == "none":
            st.warning(f"é¡¹ç›® {pid} å°šæœªå»ºç«‹å‘é‡ç´¢å¼•ï¼ˆ.cache_index æ— å¯¹åº”æ–‡ä»¶ï¼‰ã€‚")
            return
        msg = f"ç´¢å¼•æ¨¡å¼: {mode}; æ˜ å°„æ¡æ•°: {len(mapping)}"
        if mode == "faiss" and index is not None:
            msg += f"; FAISS ntotal: {index.ntotal}"
        elif vecs is not None:
            msg += f"; NPY shape: {getattr(vecs, 'shape', None)}"
        st.info(msg)

        # æŠ½æ ·éªŒè¯æ˜ å°„çš„ corpus_id æ˜¯å¦éƒ½èƒ½å›æŸ¥åˆ°æ–‡æœ¬
        bad = 0
        for m in mapping[:10]:
            cid = int(m.get("corpus_id") or -1)
            row = cur.execute("SELECT id FROM corpus WHERE id=?", (cid,)).fetchone()
            if not row:
                bad += 1
        if bad:
            st.warning(f"æ˜ å°„ä¸­æœ‰ {bad} æ¡ corpus_id æ— æ³•å›æŸ¥.è¯·è€ƒè™‘é‡å»ºç´¢å¼•ã€‚")
    except Exception as e:
        st.error(f"å‘é‡è¯Šæ–­å¼‚å¸¸:{e}")

# ---------- æ–‡ä»¶è¯»å–å·¥å…· ----------
def _lazy_docx():
    try:
        import docx  # python-docx
        return docx
    except Exception:
        return None

def _normalize(t: str) -> str:
    if not t: return ""
    t = t.replace("\xa0", " ").replace("\u200b", "")
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{2,}", "\n", t)
    return t.strip()

def read_docx_tables_info(file_like):
    docx = _lazy_docx()
    if not docx: return {}
    try:
        doc = docx.Document(file_like)
    except Exception:
        return {}
    info = {}
    for ti, tbl in enumerate(doc.tables):
        rows = len(tbl.rows)
        cols = len(tbl.columns) if rows else 0
        prev = []
        for r in tbl.rows[: min(6, rows)]:
            prev.append(tuple(_normalize(c.text) for c in r.cells))
        info[ti] = {"rows": rows, "cols": cols, "preview": prev}
    return info

def extract_pairs_from_docx_table(file_like, table_index=0, src_col=0, tgt_col=1,
                                  ffill=True, drop_empty_both=True, dedup=True):
    docx = _lazy_docx()
    if not docx: return []
    try:
        doc = docx.Document(file_like)
    except Exception:
        return []
    if table_index >= len(doc.tables): return []
    tbl = doc.tables[table_index]
    rows = []
    for r in tbl.rows:
        rows.append([_normalize(c.text) for c in r.cells])
    if not rows: return []
    max_cols = max(len(r) for r in rows)
    if src_col >= max_cols or tgt_col >= max_cols: return []

    if ffill:
        for col in (src_col, tgt_col):
            last = ""
            for i in range(len(rows)):
                val = rows[i][col] if col < len(rows[i]) else ""
                if val: last = val
                else: rows[i][col] = last

    pairs = []
    for r in rows:
        s = r[src_col] if src_col < len(r) else ""
        t = r[tgt_col] if tgt_col < len(r) else ""
        s, t = s.strip(), t.strip()
        if drop_empty_both and (not s and not t):
            continue
        pairs.append((s, t))

    if dedup:
        seen, out = set(), []
        for p in pairs:
            if p in seen: continue
            seen.add(p); out.append(p)
        pairs = out
    return pairs

def read_docx_text(file_like) -> str:
    docx = _lazy_docx()
    if not docx: return ""
    try:
        doc = docx.Document(file_like)
    except Exception:
        return ""
    blocks = []
    for p in doc.paragraphs:
        t = _normalize(p.text)
        if t: blocks.append(t)
    # æŠŠè¡¨æ ¼å•å…ƒä¹Ÿæ‹¼æˆè¡Œ.é¿å…æ¼æ‰å†…å®¹
    for tbl in doc.tables:
        for r in tbl.rows:
            line = " ".join(_normalize(c.text) for c in r.cells if _normalize(c.text))
            if line: blocks.append(line)
    return "\n".join(blocks)

def read_txt(file_like_or_bytes) -> str:
    try:
        if hasattr(file_like_or_bytes, "read"):
            data = file_like_or_bytes.read()
        else:
            data = file_like_or_bytes
        if isinstance(data, bytes):
            try: return data.decode("utf-8")
            except: return data.decode("utf-8", errors="ignore")
        return str(data)
    except Exception:
        return ""

def read_csv_two_cols(file_like, col_a=0, col_b=1):
    try:
        df = pd.read_csv(file_like)
        a = df.iloc[:, col_a].astype(str).fillna("").tolist()
        b = df.iloc[:, col_b].astype(str).fillna("").tolist()
        return list(zip(a, b))
    except Exception:
        return []

def read_pdf_text(file_like) -> str:
    # ä¼˜å…ˆ pypdf;å¤±è´¥å†è¯• pdfminer.six
    try:
        from PyPDF2 import PdfReader
        reader = PdfReader(file_like)
        txts = []
        for page in reader.pages:
            t = page.extract_text() or ""
            t = _normalize(t)
            if t: txts.append(t)
        return "\n".join(txts)
    except Exception:
        try:
            from pdfminer.high_level import extract_text
            if hasattr(file_like, "read"):
                data = file_like.read()
            else:
                data = file_like
            return _normalize(extract_text(io.BytesIO(data)))
        except Exception:
            return ""
        
# ========== å‘é‡å¬å›(å¤šåç«¯:Sentence-Transformers â†’ Fastembed â†’ TF-IDF)==========
def _lazy_import_vec():
    import importlib
    np = importlib.import_module("numpy")

    # å°è¯• faiss / faiss_cpu(å¯é€‰)
    try:
        faiss = importlib.import_module("faiss")
    except Exception:
        try:
            faiss = importlib.import_module("faiss_cpu")
        except Exception:
            faiss = None

    # 1) ä¼˜å…ˆ:sentence_transformers(éœ€è¦ torch)
    SentenceTransformer = None
    try:
        SentenceTransformer = importlib.import_module("sentence_transformers").SentenceTransformer
    except Exception:
        SentenceTransformer = None

    # 2) å¤‡é€‰:fastembed(ä¸éœ€è¦ torch.åç«¯æ˜¯ onnxruntime)
    FastEmbedModel = None
    try:
        FastEmbedModel = importlib.import_module("fastembed").TextEmbedding
    except Exception:
        FastEmbedModel = None

    # 3) å…œåº•:scikit-learn TF-IDF
    TfidfVectorizer = None
    NearestNeighbors = None
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore
        from sklearn.neighbors import NearestNeighbors  # type: ignore
    except Exception:
        pass

    return np, faiss, SentenceTransformer, FastEmbedModel, TfidfVectorizer, NearestNeighbors

@st.cache_resource(show_spinner=False)
def get_embedder(model_name: str = "thenlper/gte-multilingual-base"):
    """
    è¿”å› (backend, encoder):
      - backend: "st" | "fastembed" | "tfidf"
      - encoder: å¯è°ƒç”¨å¯¹è±¡.å…·å¤‡ encode(texts)->np.ndarray æ¥å£(tfidf è¿”å› (vectorizer, matrix) çš„æ‰“åŒ…å™¨)
    """
    np, faiss, SentenceTransformer, FastEmbedModel, TfidfVectorizer, _ = _lazy_import_vec()

    # 1) Sentence-Transformers(æˆåŠŸåˆ™ä¼˜å…ˆ)
    if SentenceTransformer is not None:
        try:
            model = SentenceTransformer(model_name)
            def _encode_st(texts):
                return model.encode(texts, normalize_embeddings=True, batch_size=64, convert_to_numpy=True).astype("float32")
            return "st", _encode_st
        except Exception as e:
            # torch/transformers/DLL ç­‰å¤±è´¥åˆ™ç»§ç»­å›é€€
            pass

    # 2) Fastembed(è½»é‡.æ—  torch)
    if FastEmbedModel is not None:
        try:
            fe = FastEmbedModel(model_name="sentence-transformers/all-MiniLM-L6-v2")  # å¤šè¯­è¡¨ç°ä¹Ÿä¸é”™
            def _encode_fe(texts):
                # è¿”å›ç”Ÿæˆå™¨.éœ€è¦å †å ; å‘é‡å·²å½’ä¸€åŒ–
                vecs = [v for v in fe.embed(texts)]
                import numpy as np
                arr = np.asarray(vecs, dtype="float32")
                # fastembed é€šå¸¸å·²æ˜¯ L2 å½’ä¸€;ç¨³å¦¥èµ·è§å†å½’ä¸€é
                norms = np.linalg.norm(arr, axis=1, keepdims=True) + 1e-12
                return (arr / norms).astype("float32")
            return "fastembed", _encode_fe
        except Exception:
            pass

    # 3) å…œåº•:TF-IDF è¿‘ä¼¼(éçœŸæ­£â€œè¯­ä¹‰å‘é‡â€.ä½†èƒ½ç”¨)
    if TfidfVectorizer is not None:
        def _encode_tfidf(texts, _cache={"vec": None, "mat": None}):
            if _cache["vec"] is None:
                vec = TfidfVectorizer(max_features=50000, ngram_range=(1,2))
                mat = vec.fit_transform(texts)
                _cache["vec"], _cache["mat"] = vec, mat
            else:
                vec = _cache["vec"]
                mat = vec.transform(texts)
            # ä¸ºäº†æ¥å£ä¸€è‡´.è¿™é‡Œè¿”å› dense(æ³¨æ„å†…å­˜);å°è§„æ¨¡è¯­æ–™å¯æ¥å—
            import numpy as np
            arr = mat.astype("float32").toarray()
            norms = np.linalg.norm(arr, axis=1, keepdims=True) + 1e-12
            return (arr / norms).astype("float32")
        return "tfidf", _encode_tfidf

    raise RuntimeError("ç¼ºå°‘å‘é‡åç«¯:è¯·å®‰è£… sentence-transformers æˆ– fastembed æˆ– scikit-learnã€‚")

def _load_index(project_id: int):
    np, faiss, *_ = _lazy_import_vec()
    idx_path, map_path, vec_path = _index_paths(project_id)
    mapping = []
    if os.path.exists(map_path):
        with open(map_path, "r", encoding="utf-8") as f:
            mapping = json.load(f)
    # FAISS
    if faiss is not None and os.path.exists(idx_path):
        index = faiss.read_index(idx_path)
        return ("faiss", index, mapping, None)
    # å›é€€:.npy
    if os.path.exists(vec_path):
        vecs = np.load(vec_path).astype("float32")
        return ("fallback", None, mapping, vecs)
    return ("none", None, mapping, None)

def _save_index(project_id: int, mode: str, index, mapping, vecs=None):
    np, faiss, *_ = _lazy_import_vec()
    idx_path, map_path, vec_path = _index_paths(project_id)
    if mode == "faiss" and index is not None:
        faiss.write_index(index, idx_path)
    elif mode == "fallback" and vecs is not None:
        np.save(vec_path, vecs.astype("float32"))
    with open(map_path, "w", encoding="utf-8") as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)

def build_project_vector_index(project_id: int, use_src: bool = True, use_tgt: bool = True):
    """
    ä¸ºæŒ‡å®šé¡¹ç›®æ„å»º/æ›´æ–°å‘é‡ç´¢å¼•ï¼ˆ.cache_indexï¼‰:
    - é»˜è®¤åŒæ—¶å†™å…¥ src/tgt.ä½†æ¨èæ£€ç´¢æ—¶ä¼˜å…ˆç”¨ tgt
    - å†™å…¥æ˜ å°„æ—¶è¡¥å…… domain/title/lang_pair/side
    - ç»Ÿä¸€ L2 å½’ä¸€åŒ–.FAISS ç”¨å†…ç§¯æ£€ç´¢â‰ˆä½™å¼¦
    - å»é‡:åŒä¸€ (corpus_id, side) ä¸é‡å¤
    è¿”å›: {"added": æ–°å¢æ¡æ•°, "total": ç´¢å¼•æ€»æ¡æ•°}
    """
    import numpy as _np
    np, faiss, *_ = _lazy_import_vec()
    backend, encode = get_embedder()

    # ä»…ç´¢å¼•å½“å‰é¡¹ç›®ï¼›éœ€è¦ domain ç”¨äºåç»­â€œåŒé¢†åŸŸâ€è¿‡æ»¤
    rows = cur.execute("""
        SELECT c.id, IFNULL(c.src_text,''), IFNULL(c.tgt_text,''), 
               IFNULL(c.title,''), IFNULL(c.lang_pair,''), 
               IFNULL(c.project_id,0), IFNULL(i.domain,'')
        FROM corpus c
        LEFT JOIN items i ON i.id = c.project_id
        WHERE c.project_id = ?
        ORDER BY c.id ASC
    """, (int(project_id),)).fetchall()

    # ç»„è£…æ–‡æœ¬ä¸å…ƒæ•°æ®ï¼ˆä¼˜å…ˆè¯‘æ–‡ä¾§ï¼›å¯åŒæ—¶å†™å…¥ src/tgtï¼‰
    texts, metas = [], []
    for cid, s, t, ttl, lp, pj, dom in rows:
        s = (s or "").strip()
        t = (t or "").strip()
        if use_tgt and t:
            texts.append(t)
            metas.append({
                "corpus_id": cid,
                "project_id": pj,
                "domain": dom or "",
                "title": ttl,
                "lang_pair": lp,
                "side": "tgt"
            })
        if use_src and s:
            texts.append(s)
            metas.append({
                "corpus_id": cid,
                "project_id": pj,
                "domain": dom or "",
                "title": ttl,
                "lang_pair": lp,
                "side": "src"
            })

    if not texts:
        return {"added": 0, "total": 0}

    # ç¼–ç å¹¶ L2 å½’ä¸€åŒ–ï¼ˆIPâ‰ˆcosï¼‰
    new_vecs = encode(texts)
    if hasattr(new_vecs, "toarray"):  # å…¼å®¹ç¨€ç–
        new_vecs = new_vecs.toarray()
    new_vecs = _np.asarray(new_vecs, dtype="float32")
    new_vecs = new_vecs / (_np.linalg.norm(new_vecs, axis=1, keepdims=True) + 1e-12)

    # è½½å…¥å·²æœ‰ç´¢å¼•
    mode, index, mapping, vecs = _load_index(project_id)
    mapping = list(mapping or [])

    # â€”â€” å»é‡:å·²æœ‰æ˜ å°„çš„ (corpus_id, side) ä¸å†é‡å¤åŠ å…¥
    seen = {(m.get("corpus_id"), m.get("side")) for m in mapping}
    keep_idx = []
    for i, m in enumerate(metas):
        key = (int(m["corpus_id"]), m["side"])
        if key not in seen:
            keep_idx.append(i)
            seen.add(key)

    if not keep_idx:
        # æ²¡æœ‰æ–°å¢
        total = (index.ntotal if (faiss is not None and mode == "faiss" and index is not None)
                 else (vecs.shape[0] if isinstance(vecs, _np.ndarray) else len(mapping)))
        return {"added": 0, "total": int(total)}

    metas = [metas[i] for i in keep_idx]
    new_vecs = new_vecs[keep_idx, :]

    # â€”â€” å†™å…¥ç´¢å¼•:ä¼˜å…ˆ FAISSï¼›å¦åˆ™ .npy å›é€€
    if faiss is not None and backend in ("st", "fastembed"):
        dim = int(new_vecs.shape[1])
        if mode != "faiss" or index is None:
            index = faiss.IndexFlatIP(dim)   # ä½™å¼¦ç­‰ä»·ï¼ˆå‘é‡å·²å½’ä¸€ï¼‰
            mapping = []
        # è‹¥å·²æœ‰ mapping/ç´¢å¼•.ä½†ä¹‹å‰ç»´åº¦ä¸ä¸€è‡´.é‡å»º
        if index.d != dim:
            index = faiss.IndexFlatIP(dim)
            mapping = []
        index.add(new_vecs)
        mapping.extend(metas)
        _save_index(project_id, "faiss", index, mapping)
        total = int(index.ntotal)
    else:
        # å›é€€:æ‹¼æ¥çŸ©é˜µ
        if vecs is None:
            vecs = new_vecs
            mapping = metas
        else:
            # ç»´åº¦ä¸ä¸€è‡´åˆ™é‡å»º
            if vecs.shape[1] != new_vecs.shape[1]:
                vecs = new_vecs
                mapping = metas
            else:
                vecs = _np.concatenate([vecs, new_vecs], axis=0)
                mapping.extend(metas)
        _save_index(project_id, "fallback", None, mapping, vecs=vecs)
        total = int(vecs.shape[0])

    return {"added": len(keep_idx), "total": total}

# =========================
# è¯­ä¹‰å¬å›(æ”¯æŒèŒƒå›´:project/domain/allï¼‰
# =========================
def semantic_retrieve(project_id: int,
                      query_text: str,
                      topk: int = 20,
                      scope: str = "project",
                      min_char: int = 3):
    """
    è¯­æ–™åº“è¯­ä¹‰å¬å›ï¼ˆè‡ªåŠ¨åˆ‡å¥ç‰ˆï¼‰
    è¿”å› [(score, meta, txt)]
    å…¼å®¹ï¼šæ²¿ç”¨å¤–éƒ¨å‘é‡ç´¢å¼• (.cache_index/...) ä¸ä½ ç°æœ‰çš„ mapping ç»“æ„ã€‚
    """

    q = (query_text or "").strip()
    if len(q) < min_char:
        return []

    # --- å·¥å…·ï¼šåˆ‡å¥ï¼ˆä¼˜å…ˆç”¨ä½ å·²æœ‰çš„ split_sentsï¼‰
    def _split(text: str) -> list[str]:
        try:
            if 'split_sents' in globals():
                segs = split_sents(text, lang_hint="auto")
                return [s for s in segs if s and len(s.strip()) >= min_char]
        except Exception:
            pass
        import re
        segs = re.split(r"(?<=[\.\!\?;ã€‚ï¼ï¼Ÿï¼›])\s*", text)
        return [s.strip() for s in segs if s and len(s.strip()) >= min_char]

    # --- å– embedder / ç´¢å¼•
    backend, encode = get_embedder()  # ("st"/"fastembed"/"tfidf", encoder)
    mode, index, mapping, vecs = _load_index(project_id)  # ("faiss"/"fallback"/"none", idx, list, np.array/None)
    if mode == "none" or not mapping:
        return []

    # --- é¡¹ç›®é¢†åŸŸä¿¡æ¯ï¼ˆscope=domain æ—¶ç”¨ï¼‰
    def _get_domain_for_proj(pid: int) -> str:
        row = cur.execute("SELECT IFNULL(domain,'') FROM items WHERE id=?", (pid,)).fetchone()
        return (row[0] if row else "") or ""
    proj_domain = _get_domain_for_proj(project_id) if scope == "domain" else ""

    # --- å•æ¬¡æœç´¢ï¼šç»™ä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µï¼Œè¿”å›å€™é€‰ [(score, idx)]
    def _search_once(piece: str, per_segment_k: int):
        try:
            import numpy as np
            qv = encode([piece]).astype("float32")
            qv = qv[0] if qv.ndim == 2 else qv
            if mode == "faiss" and index is not None:
                D, I = index.search(qv.reshape(1, -1), min(per_segment_k, len(mapping)))
                return list(zip(D[0].tolist(), I[0].tolist()))
            else:
                if vecs is None:
                    return []
                sims = (vecs @ qv.reshape(-1, 1)).ravel()  # å†…ç§¯ï¼ˆå·²å½’ä¸€åŒ–â‰ˆcosï¼‰
                order = sims.argsort()[::-1][:min(per_segment_k, sims.shape[0])]
                return [(float(sims[i]), int(i)) for i in order]
        except Exception:
            return []

    # --- ç‰‡æ®µåŒ–æ£€ç´¢ï¼šæŒ‰ç‰‡æ®µå¹³å‡åˆ†é…å€™é€‰é¢„ç®—
    parts = _split(q)
    if not parts:
        parts = [q]

    # åŸºäºç‰‡æ®µæ•°åŠ¨æ€è°ƒæ•´æ¯ç‰‡å¬å›é¢„ç®—ï¼›ä¹˜ä»¥ 2 é¢„ç•™åˆå¹¶æŸè€—
    import math
    per_k = max(5, math.ceil((topk * 2) / max(1, len(parts))))

    # æ±‡æ€»ï¼šåŒä¸€ä¸ª corpus_id åªä¿ç•™æœ€é«˜å¾—åˆ†
    best_by_cid = {}  # cid -> (score, meta, txt)

    # ç®€å•çš„æƒé‡/æƒ©ç½š
    PROJECT_BONUS = 0.05
    DOMAIN_BONUS = 0.02
    SHORT_LEN = 6
    SHORT_PENALTY = 0.5

    for piece in parts:
        piece = piece.strip()
        if len(piece) < min_char:
            continue

        hits = _search_once(piece, per_k)
        # å¯¹çŸ­ç‰‡æ®µé™æƒ
        length_factor = SHORT_PENALTY if len(piece) < SHORT_LEN else 1.0

        for raw_sc, mi in hits:
            if mi < 0 or mi >= len(mapping):
                continue
            meta = dict(mapping[mi])  # å¯èƒ½å«: corpus_id/side/title/lang_pair
            cid = int(meta.get("corpus_id") or -1)
            if cid < 0:
                continue

            # å›è¡¨æ‹¿é¡¹ç›®/é¢†åŸŸ/æ–‡æœ¬
            row = cur.execute(
                "SELECT IFNULL(project_id,0), IFNULL(domain,''), IFNULL(src_text,''), IFNULL(tgt_text,'') "
                "FROM corpus WHERE id=?", (cid,)
            ).fetchone()
            if not row:
                continue
            pid_c, dom_c, s_txt, t_txt = row
            meta["project_id"] = pid_c
            meta["domain"] = dom_c or ""

            # scope è¿‡æ»¤
            if scope == "project" and int(pid_c or 0) != int(project_id):
                continue
            if scope == "domain":
                # å…è®¸ï¼šåŒé¡¹ç›®ï¼›æˆ–ä¸åŒé¡¹ç›®ä½† domain ç›¸åŒ
                if (int(pid_c or 0) != int(project_id)) and ((dom_c or "") != proj_domain):
                    continue
            # scope == "all": ä¸è¿‡æ»¤

            txt = (t_txt or s_txt or "").strip()
            if not txt:
                continue

            # è®¡ç®—æœ€ç»ˆåˆ†æ•°ï¼šåŸå§‹ç›¸ä¼¼åº¦ Ã— ç‰‡æ®µé•¿åº¦å› å­ + é¡¹ç›®/é¢†åŸŸå¥–åŠ±
            sc = float(raw_sc) * float(length_factor)
            if int(pid_c or 0) == int(project_id):
                sc += PROJECT_BONUS
            elif scope in ("domain", "all") and (dom_c or "") == proj_domain and proj_domain:
                sc += DOMAIN_BONUS

            # åˆå¹¶ï¼šå–åŒä¸€ cid çš„æœ€é«˜åˆ†
            prev = best_by_cid.get(cid)
            if (prev is None) or (sc > prev[0]):
                best_by_cid[cid] = (sc, meta, txt)

    if not best_by_cid:
        return []

    # å½’ä¸€æ’åºå¹¶æˆªæ–­
    merged = sorted(best_by_cid.values(), key=lambda x: x[0], reverse=True)[:topk]
    return merged

def semantic_consistency_report(project_id: int, blocks_src: list, blocks_tgt: list, term_map: dict, topk: int = 3, thr: float = 0.70):
    """
    è¿”å› DataFrame:æ®µå·ã€æœ€ç›¸ä¼¼å‚è€ƒå¾—åˆ†ã€æ˜¯å¦ä½äºé˜ˆå€¼ã€æœªéµå®ˆæœ¯è¯­æ¡ç›®ç­‰
    """
    emb = get_embedder()

    # ä»…å¯¹â€œè¯‘æ–‡ä¾§â€åšå‚è€ƒ(æ›´è´´è¿‘äººå·¥å®¡æ ¡)
    hits_all = []
    for i, (s, t) in enumerate(zip(blocks_src, blocks_tgt), 1):
        hits = semantic_retrieve(project_id, t, topk=topk)  # ç”¨è¯‘æ–‡å»æ£€ç´¢å‚è€ƒè¯‘æ–‡
        top_score = 0.0
        # åªçœ‹ tgt ä¾§
        for sc, meta, txt in hits:
            if meta.get("side") == "tgt":
                top_score = sc
                break

        # æœ¯è¯­éµå®ˆ:ç²—ç•¥æ£€æŸ¥â€œç›®æ ‡è¯‘åæ˜¯å¦å‡ºç°åœ¨è¯‘æ–‡ä¸­â€
        violated = []
        for src_term, tgt_term in (term_map or {}).items():
            if src_term in (s or "") and tgt_term and (tgt_term not in (t or "")):
                violated.append(f"{src_term}->{tgt_term}")

        hits_all.append({
            "æ®µå·": i,
            "ç›¸ä¼¼å‚è€ƒå¾—åˆ†": round(top_score, 2),
            "ä½äºé˜ˆå€¼": (top_score < thr),
            "æœªéµå®ˆæœ¯è¯­": ", ".join(violated) if violated else ""
        })

    return pd.DataFrame(hits_all)

def search_semantic(project_id, query_text, topk: int = 20, scope: str = "project"):
    """
    ç»Ÿä¸€å°è£…ä¸€å±‚ï¼Œå†…éƒ¨ç›´æ¥è°ƒç”¨ semantic_retrieveï¼š
    - project_id: å½“å‰é¡¹ç›®IDï¼ˆint æˆ– "global"ï¼‰
    - query_text: ç”¨æˆ·åœ¨ç•Œé¢è¾“å…¥çš„æŸ¥è¯¢æ–‡æœ¬
    - topk: è¿”å›å¤šå°‘æ¡ç»“æœ
    - scope: "project" / "domain" / "all"

    è¿”å›å€¼ï¼š[(score, meta, txt), ...] â€”â€” ç›´æ¥å¤ç”¨ semantic_retrieve çš„æ ¼å¼
    """
    # å…è®¸ UI å±‚ä¼  "global" æˆ– None è¿›æ¥ï¼Œè¿™é‡Œåšä¸ªç®€å•å…¼å®¹
    if project_id in (None, "", "global"):
        # ä½ è‡ªå·±çš„è¯­ä¹‰æ£€ç´¢å®ç°æ˜¯åŸºäºé¡¹ç›®IDçš„ï¼Œ
        # å¦‚æœä½ æƒ³åšâ€œå…¨å±€æœç´¢â€ï¼Œå¯ä»¥çº¦å®šä¸€ä¸ªç‰¹æ®Šå€¼ï¼Œæ¯”å¦‚ 0ï¼Œ
        # å†åœ¨ semantic_retrieve é‡Œæ ¹æ® scope="all" èµ°å…¨åº“æ£€ç´¢ã€‚
        pid = 0
    else:
        pid = int(project_id)

    return semantic_retrieve(
        project_id=pid,
        query_text=query_text,
        topk=topk,
        scope=scope
    )

# ========== è·¯å¾„/DB ==========
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(BASE_DIR, "kb.db")
PROJECT_DIR = os.path.join(BASE_DIR, "projects")
os.makedirs(PROJECT_DIR, exist_ok=True)

conn = sqlite3.connect(DB_PATH, check_same_thread=False)
cur = conn.cursor()

def ensure_domain_columns_and_backfill(conn, cur, corpus_table="corpus"):
    # items.domain
    cols = [r[1] for r in cur.execute("PRAGMA table_info(items)").fetchall()]
    if "domain" not in cols:
        cur.execute("ALTER TABLE items ADD COLUMN domain TEXT;")
        conn.commit()

    # è¯­æ–™è¡¨ domainï¼ˆå¦‚æœä½ ç”¨ corpus_main å°±æŠŠå‚æ•°æ”¹æˆ corpus_mainï¼‰
    cols = [r[1] for r in cur.execute(f"PRAGMA table_info({corpus_table})").fetchall()]
    if "domain" not in cols:
        cur.execute(f"ALTER TABLE {corpus_table} ADD COLUMN domain TEXT;")
        conn.commit()

    # å›å¡«:ç”¨ items.domain è¡¥ corpus.domainï¼ˆæœ‰ project_id çš„è¡Œï¼‰
    try:
        cur.execute(f"""
            UPDATE {corpus_table}
            SET domain = (
              SELECT i.domain FROM items i WHERE i.id = {corpus_table}.project_id
            )
            WHERE domain IS NULL AND project_id IS NOT NULL;
        """)
        conn.commit()
    except Exception:
        pass

# è°ƒç”¨ï¼ˆè€åº“è¡¨åæ˜¯ corpusï¼‰:
ensure_domain_columns_and_backfill(conn, cur, corpus_table="corpus")
# è‹¥ä½ å·²ç»åˆ‡åˆ° corpus_main / corpus_vec:
# ensure_domain_columns_and_backfill(conn, cur, corpus_table="corpus_main")

try:
    cur.execute("CREATE INDEX IF NOT EXISTS idx_term_ext_project ON term_ext(project_id)")
    conn.commit()
except Exception as e:
    print("ç´¢å¼•åˆ›å»ºè·³è¿‡:", e)

def _has_col(table: str, col: str) -> bool:
    cur.execute(f"PRAGMA table_info({table})")
    return any(r[1] == col for r in cur.fetchall())

def ensure_col(table: str, col: str, col_type: str):
    cur.execute(f"PRAGMA table_info({table})")
    cols = {r[1] for r in cur.fetchall()}
    if col not in cols:
        cur.execute(f"ALTER TABLE {table} ADD COLUMN {col} {col_type}")

# ==== æœ¯è¯­åŠ è½½:é¡¹ç›®ä¼˜å…ˆ.ç¼ºçœç”¨å…¨å±€ ====
def load_terms_for_project(cur, project_id: int | None) -> dict[str, str]:
    """
    è¿”å› {source_term: target_term}ã€‚åŠ è½½é¡ºåº:
      1) å…¨å±€æœ¯è¯­(project_id IS NULL)
      2) æŒ‡å®šé¡¹ç›®æœ¯è¯­(è¦†ç›–åŒåæºè¯)
    """
    out: dict[str, str] = {}
    # å…¨å±€
    for s, t in cur.execute("""
        SELECT source_term, target_term FROM term_ext WHERE project_id IS NULL
    """).fetchall():
        s = (s or "").strip(); t = (t or "").strip()
        if s and t:
            out[s] = t
    # é¡¹ç›®
    if project_id is not None:
        for s, t in cur.execute("""
            SELECT source_term, target_term FROM term_ext WHERE project_id=?
        """, (int(project_id),)).fetchall():
            s = (s or "").strip(); t = (t or "").strip()
            if s and t:
                out[s] = t  # è¦†ç›–å…¨å±€
    return out

# â€”â€” å»ºè¡¨
cur.execute("""
CREATE TABLE IF NOT EXISTS items (
    id INTEGER PRIMARY KEY,
    title TEXT NOT NULL,
    body TEXT,
    tags TEXT,
    domain TEXT,
    type TEXT,
    created_at TEXT DEFAULT (datetime('now'))
);
""")

cur.execute("""
CREATE TABLE IF NOT EXISTS item_ext (
    id INTEGER PRIMARY KEY,
    item_id INTEGER,
    src_path TEXT,
    FOREIGN KEY(item_id) REFERENCES items(id)
);
""")

cur.execute("""
CREATE TABLE IF NOT EXISTS term_ext (
    id INTEGER PRIMARY KEY,
    source_term TEXT NOT NULL,
    target_term TEXT,
    domain TEXT,
    project_id INTEGER,
    strategy TEXT,
    example TEXT,
    category TEXT,
    FOREIGN KEY(project_id) REFERENCES items(id)
);
""")

cur.execute("""
CREATE TABLE IF NOT EXISTS trans_ext (
    id INTEGER PRIMARY KEY,
    project_id INTEGER,
    src_text TEXT,
    tgt_text TEXT,
    mode TEXT,
    segment_count INTEGER,
    created_at TEXT DEFAULT (datetime('now')),
    FOREIGN KEY(project_id) REFERENCES items(id)
);
""")

cur.execute("""
CREATE TABLE IF NOT EXISTS corpus (
    id INTEGER PRIMARY KEY,
    project_id INTEGER,
    text TEXT,
    lang TEXT,
    source TEXT,
    created_at TEXT DEFAULT (datetime('now')),
    FOREIGN KEY(project_id) REFERENCES items(id)
);
""")
conn.commit()


# â€”â€” å…œåº•è¡¥åˆ—
for t, cols in {
    "items": [("type","TEXT"),("tags","TEXT"),("scene","TEXT"),("prompt","TEXT"),
              ("mode","TEXT"),("body","TEXT"),("created_at","TEXT"),("updated_at","TEXT"),("trans_type","TEXT")],
    "item_ext": [("src_path","TEXT")],
    "term_ext": [("domain","TEXT"),("project_id","INTEGER"),("strategy","TEXT"),
                 ("example","TEXT"),("note","TEXT"), ("category","TEXT")],
    "trans_ext": [("stats_json","TEXT"),("segments","INTEGER"),("term_hit_total","INTEGER")],
    "corpus": [("title","TEXT"),("project_id","INTEGER"),("lang_pair","TEXT"),("src_text","TEXT"),("tgt_text","TEXT"),("note","TEXT"),("created_at","TEXT")],
}.items():
    for c, tp in cols:
        ensure_col(t, c, tp)
cur.execute("UPDATE items SET type='project' WHERE IFNULL(type,'')=''")
cur.execute("UPDATE items SET created_at = COALESCE(created_at, strftime('%Y-%m-%d %H:%M:%S','now'))")
conn.commit()
ensure_col("term_ext", "example_vector_id", "INTEGER")

# --- æœ¯è¯­è¡¨å­—æ®µå…¼å®¹:ç¼ºå°‘ project_id æ—¶è¡¥å»º ---
try:
    cur.execute("PRAGMA table_info(term_ext)")
    cols = [c[1] for c in cur.fetchall()]
    if "project_id" not in cols:
        cur.execute("ALTER TABLE term_ext ADD COLUMN project_id INTEGER")
        conn.commit()
except Exception as e:
    st.warning(f"æœ¯è¯­è¡¨ç»“æ„æ£€æŸ¥:{e}")

# ========== DeepSeek å‚æ•°/è°ƒç”¨ ==========
def get_deepseek():
    """
    ä» .streamlit/secrets.toml è¯»å–:
    [deepseek]
    api_key="..."
    model="deepseek-chat"
    """
    try:
        ak = st.secrets["deepseek"]["api_key"]
        model = st.secrets["deepseek"].get("model", "deepseek-chat")
        return ak, model
    except Exception:
        return None, None

        # === æ–°å¢:æœ¯è¯­æç¤º + å‚è€ƒä¾‹å¥ ===
def _build_ref_context(project_id: int,
                       query_text: str,
                       topk: int = 20,
                       min_sim: float = 0.35,
                       prefer_side: str = "tgt",
                       scope: str = "project") -> str:
    """
    è¯­ä¹‰å¬å› â†’ ç»„è£…å‚è€ƒå—ã€‚
    scope: "project"|"domain"|"all"
    prefer_side: "tgt"|"src"|"both"
    è¿”å›ä¸€æ®µå¯ç›´æ¥æ³¨å…¥ Prompt çš„å‚è€ƒæ–‡æœ¬ã€‚
    """
    try:
        hits = semantic_retrieve(project_id, query_text, topk=topk, scope=scope)  # ä¼ å…¥ scope
    except Exception as e:
        try:
            st.warning(f"å‚è€ƒæ£€ç´¢å¤±è´¥:{e}")
        except Exception:
            pass
        return ""

    selected = [(sc, meta, txt) for (sc, meta, txt) in (hits or []) if (sc or 0) >= float(min_sim)]
    if not selected and hits:
        selected = [max(hits, key=lambda x: x[0])]

    ctx_parts, used = [], set()
    for idx, (sc, meta, txt) in enumerate(selected, 1):
        s_txt = (txt or "").strip()
        if not s_txt:
            continue
        key = s_txt[:120]
        if key in used:
            continue
        used.add(key)

        title = meta.get("domain", "") if isinstance(meta, dict) else ""
        side = meta.get("side", "tgt") if isinstance(meta, dict) else "tgt"

        if prefer_side == "both" and isinstance(meta, dict):
            row = cur.execute("SELECT src_text, tgt_text FROM corpus WHERE id=?", (meta.get("corpus_id"),)).fetchone()
            s0, t0 = (row or ["",""])
            snippet = f"ä¾‹å¥ {idx} åŸæ–‡:{(s0 or '').strip()}\nä¾‹å¥ {idx} è¯‘æ–‡:{(t0 or '').strip()}"
            ctx_parts.append(f"[{title}] (sim={sc:.2f})\n{snippet}\n")
        else:
            tag = "å‚è€ƒè¯‘æ–‡" if side == "tgt" else "å‚è€ƒåŸæ–‡"
            snippet = s_txt.replace("\n", " ")[:400]
            ctx_parts.append(f"[{tag}Â·{title}] (sim={sc:.2f}) {snippet}")

        if sum(len(p) for p in ctx_parts) > 1800:
            break

    return "" if not ctx_parts else "(ä»¥ä¸‹ä¸ºç›¸ä¼¼è¯­æ–™.å¯å‚è€ƒæœ¯è¯­ä¸é£æ ¼ï¼‰\n" + "\n".join(ctx_parts)

def build_system_prompt(
    base_prompt: str,
    term_pairs: list,                 # å½¢å¦‚ [(src, tgt), ...]
    lang_pair: str,
    ref_context: str | None = None    # å¤–éƒ¨å·²ç»å‡†å¤‡å¥½çš„è¯­ä¹‰å¬å›/ç¤ºä¾‹å‚è€ƒå—
) -> str:
    """
    æ„é€ æœ€ç»ˆæç¤º:ç¿»è¯‘æ–¹å‘ + æœ¯è¯­çº¦æŸ + å‚è€ƒä¾‹å¥ + é€šç”¨æŒ‡ä»¤ + ç”¨æˆ·æ–‡æœ¬
    è¿”å›å€¼:system_prompt + '\\n\\n' + user_prompt
    """

    # â€”â€” ç¿»è¯‘æ–¹å‘è¯´æ˜ â€”â€”
    lp_note = f"ç¿»è¯‘æ–¹å‘:{lang_pair or 'è‡ªåŠ¨'}ã€‚"

    # â€”â€” æœ¯è¯­å—(æŠŠ term_pairs â†’ æ–‡æœ¬æç¤º) â€”â€”
    term_dict = {s: t for s, t in (term_pairs or []) if s and t}
    if term_dict:
        kb_lines = [f"- {s} -> {t}" for s, t in term_dict.items()]
        kb_block = "è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹æœ¯è¯­å¯¹ç…§.ä¸å¾—æ”¹å†™æˆ–æ›¿æ¢ä¸ºè¿‘ä¹‰è¡¨è¾¾:\n" + "\n".join(kb_lines)
        # è‹¥ä½ é¡¹ç›®é‡Œå·²æœ‰ build_term_hint.å°±å¤ç”¨;æ²¡æœ‰ä¹Ÿå¯åˆ æ‰ term_hint è¿™è¡Œ
        term_hint = build_term_hint(term_dict, lang_pair) or ""
    else:
        kb_block = ""
        term_hint = ""

    # â€”â€” é€šç”¨è§„åˆ™(å¯æ›¿æ¢ä¸ºä½ åŸå…ˆçš„ rules æ–‡æœ¬) â€”â€”
    rules = (
        "é€šç”¨æŒ‡ä»¤:\n"
        "- ä¿æŒæœ¯è¯­/ä¸“æœ‰åè¯ä¸€è‡´.ä¸éšæ„å¢è¯‘/çœè¯‘;\n"
        "- æ•°å­—ã€æ—¶é—´ã€äººåã€åœ°åå‡†ç¡®æ— è¯¯;\n"
        "- è‹¥é‡æœªç™»å½•æœ¯è¯­.ä¿æŒåŸæ–‡+æ‹¬æ³¨(ä»…ç¬¬ä¸€æ¬¡å‡ºç°)ã€‚\n"
    )

    # â€”â€” å‚è€ƒä¾‹å¥(æ¥è‡ªå¤–éƒ¨ä¼ å…¥çš„ ref_context) â€”â€”
    ref_hint = f"å‚è€ƒä¾‹å¥(ä¿æŒæœ¯è¯­/é£æ ¼ä¸€è‡´):\n{ref_context}\n" if ref_context else ""

    # â€”â€” ç›®æ ‡è¯­è¨€ â€”â€”
    target_lang = "ä¸­æ–‡" if (lang_pair or "").startswith("è‹±è¯‘ä¸­") else "è‹±æ–‡"

    # â€”â€” System Prompt (ç³»ç»Ÿæç¤º) â€”â€”
    system_prompt = (
        "ä½ æ˜¯ä¸€åèµ„æ·±ä¸“ä¸šç¬”è¯‘:\n"
        "1) å‡†ç¡®ä¼ è¾¾æœ¯è¯­ä¸äº‹å®;2) æ­£å¼æ¸…æ™°;3) ä¸è‡†é€ ;\n"
        "4) ä¸“æœ‰åè¯ä¿æŒä¸€è‡´;5) åœ¨ä¸æ”¹å˜æœ¯è¯­çš„å‰æä¸‹ä¼˜åŒ–é€šé¡ºã€‚\n"
        f"{lp_note}\n\n{rules}\n{kb_block}\n"
    ).strip()

    # â€”â€” User Prompt(ç”¨æˆ·æç¤º.ç¡®ä¿æ— æ¡ä»¶èµ‹å€¼) â€”â€”
    user_prompt = (
        (term_hint or "") +
        (ref_hint or "") +
        f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘ä¸º{target_lang}:\n\n{(base_prompt or '').strip()}"
    )

    return system_prompt + "\n\n" + user_prompt

def detect_terms_simple(block: str, term_map: dict) -> dict:
    return {k: v for k, v in term_map.items() if k and (k in block)}
# -------- Glossary & Instruction helpers (æ”¾åœ¨ ds_translate ä¸Šæ–¹) --------
def build_term_hint(term_dict: dict, lang_pair: str, max_terms: int = 80) -> str:
    """
    å°†æœ¯è¯­æ˜ å°„è½¬æˆå¯è¯»çš„â€œç¡¬çº¦æŸâ€è§„åˆ™æ–‡æœ¬.æ”¯æŒä»¥ä¸‹å‡ ç§ term_dict ç»“æ„:
      { "contract": "åˆåŒ" }
      { "contract": {"target":"åˆåŒ", "pos":"NOUN", "usage_note":"æ³•å¾‹è¯­å¢ƒ"} }
      { "contract": ("åˆåŒ", "NOUN") }   # å…ƒç»„å½¢å¼ (target, pos)
    ç©ºç›®æ ‡ä¼šè¢«å¿½ç•¥;è‡ªåŠ¨å»é‡å¹¶æœ€å¤šè¾“å‡º max_terms æ¡.é¿å…æç¤ºè¿‡é•¿ã€‚
    """
    lines = []
    seen = set()
    items = list(term_dict.items())[: max_terms * 2]  # ç¨å¤šå–ä¸€äº›.è¿‡æ»¤ç©ºåå†æˆªæ–­

    for src, val in items:
        if src is None: 
            continue
        src = str(src).strip()
        if not src or src in seen:
            continue

        tgt, pos, note = None, None, None
        if isinstance(val, dict):
            tgt  = (val.get("target") or val.get("tgt") or "").strip()
            pos  = (val.get("pos") or "").strip() or None
            note = (val.get("usage_note") or val.get("note") or "").strip() or None
        elif isinstance(val, (list, tuple)) and len(val) >= 1:
            tgt = str(val[0]).strip()
            if len(val) >= 2:
                pos = (str(val[1]).strip() or None)
        else:
            tgt = str(val or "").strip()

        if not tgt:
            continue

        seen.add(src)
        if pos:
            line = f"- When '{src}' is a {pos}, translate it as '{tgt}'."
        else:
            line = f"- Translate '{src}' as '{tgt}'."
        if note:
            line += f" ({note})"
        lines.append(line)

    if not lines:
        return ""  # æ²¡æœ‰å¯ç”¨æœ¯è¯­å°±è¿”å›ç©ºä¸².ä¸è¦å¹²æ‰°æç¤º

    header = "GLOSSARY (STRICT):\n"
    return header + "\n".join(lines[:max_terms]) + "\n"


def build_instruction(lang_pair: str) -> str:
    """
    ç”Ÿæˆç®€æ´çš„ç¿»è¯‘æŒ‡ä»¤ã€‚ä½ ä¹Ÿå¯ä»¥æŒ‰é¡¹ç›®é£æ ¼å†æ‰©å±•ã€‚
    """
    lp = (lang_pair or "").replace(" ", "")
    if "ä¸­â†’è‹±" in lp or "ä¸­->è‹±" in lp or "zh" in lp.lower() and "en" in lp.lower():
        return (
            "Translate the source text from Chinese to English. "
            "Use a professional, natural style; follow the GLOSSARY (STRICT) exactly; "
            "preserve proper nouns and numbers; keep paragraph structure. "
            "Do not add explanations."
        )
    if "è‹±â†’ä¸­" in lp or "è‹±->ä¸­" in lp or "en" in lp.lower() and "zh" in lp.lower():
        return (
            "Translate the source text from English to Chinese. "
            "ç”¨ä¸“ä¸šã€é€šé¡ºã€ç¬¦åˆé¢†åŸŸæ–‡ä½“çš„ä¸­æ–‡è¡¨è¾¾;ä¸¥æ ¼éµå®ˆä¸Šæ–¹ GLOSSARY (STRICT);"
            "ä¸“æœ‰åè¯ã€æ•°å­—ä¸è®¡é‡å•ä½ä¿æŒå‡†ç¡®;æ®µè½ç»“æ„ä¿æŒä¸€è‡´ã€‚ä¸å¾—æ·»åŠ è§£é‡Šã€‚"
        )
    # å…œåº•
    return (
        "Translate the source text. Follow the GLOSSARY (STRICT) exactly. "
        "Keep the original structure and do not add explanations."
    )

def ds_translate(block: str, term_dict: dict, lang_pair: str, ak: str, model: str, ref_context: str = "") -> str:
    term_hint = build_term_hint(term_dict, lang_pair)  # ä½ ç°æœ‰çš„æœ¯è¯­æç¤º
    instr = build_instruction(lang_pair)   # type: ignore

    """
    ä½¿ç”¨ DeepSeek REST API ç¿»è¯‘ä¸€ä¸ªæ–‡æœ¬å—ã€‚term_dict ä¸º {æº: ç›®æ ‡} çš„æ˜ å°„.æ³¨å…¥ä¸ºå¼ºçº¦æŸæç¤ºã€‚
    """
    import requests

    if not block.strip():
        return ""

    if term_dict:
        term_lines = "\n".join([f"- {k} -> {v}" for k, v in term_dict.items()])
        term_hint = (
            "TERMINOLOGY:\n"
            "Use the following mappings EXACTLY and consistently. Do not invent alternatives.\n"
            f"{term_lines}\n"
        )
    else:
        term_hint = "TERMINOLOGY:\nEnsure consistent terminology; avoid paraphrasing fixed terms.\n"

    if lang_pair == "ä¸­è¯‘è‹±":
        instr = "Translate the Chinese text into English with high fidelity and formal style."
    elif lang_pair == "è‹±è¯‘ä¸­":
        instr = "å°†ä¸‹åˆ—è‹±æ–‡å‡†ç¡®è¯‘ä¸ºä¸­æ–‡.è¯­ä½“æ­£å¼ã€ä¸“ä¸šã€‚"
    else:
        instr = "Translate accurately into the other language."

    system_msg = (
        "You are a senior professional translator. Prioritize accuracy, faithfulness, and consistent terminology. "
        "No hallucinations. If a term mapping is provided, follow it strictly."
    )
    
    user_msg = (
    f"{term_hint}"
    + (f"REFERENCE CONTEXT (use if relevant):\n{ref_context}\n\n" if ref_context else "")
    + "INSTRUCTION:\n" + instr + "\n"
    + "RESPONSE FORMAT:\n- Return ONLY the final translation text, no explanations, no backticks.\n\n"
    + "SOURCE:\n" + block
    )


    url = "https://api.deepseek.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {ak}", "Content-Type": "application/json"}
    payload = {
        "model": model or "deepseek-chat",
        "messages": [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        "temperature": 0.2,
    }

    for attempt in range(3):
        try:
            resp = requests.post(url, headers=headers, json=payload, timeout=60)
            if resp.status_code == 200:
                data = resp.json()
                return data["choices"][0]["message"]["content"].strip()
            else:
                txt = f"[DeepSeek {resp.status_code}] {resp.text}"
                if resp.status_code in (429, 500, 502, 503, 504) and attempt < 2:
                    time.sleep(1.5 * (attempt + 1))
                    continue
                return txt
        except Exception as e:
            if attempt < 2:
                time.sleep(1.5 * (attempt + 1))
                continue
            return f"[DeepSeek Request Error] {e}"
    return "[DeepSeek Error] Unknown failure."

def ds_extract_terms(text: str, ak: str, model: str, src_lang: str = "zh", tgt_lang: str = "en"):
    """
    ç”¨ DeepSeek ä»æ–‡æœ¬ä¸­æŠ½å–æœ¯è¯­å¯¹.è¿”å› JSON æ•°ç»„:
    [{"source_term":"...", "target_term":"...", "domain":"...", "strategy":"...", "example":"..."}]
    """
    import requests

    if not text.strip():
        return []

    system_msg = (
        "You are a terminology mining assistant. Extract high-value bilingual term pairs suitable for a project glossary. "
        "Return JSON array only. No extra text."
    )
    user_msg = f"""
Source language: {src_lang}
Target language: {tgt_lang}
ä»»åŠ¡:ä»ç»™å®šæ–‡æœ¬ä¸­æŠ½å–åŒè¯­æœ¯è¯­æ¡ç›®.è¾“å‡º JSON æ•°ç»„ã€‚å­—æ®µåä¸å–å€¼å¿…é¡»æ˜¯ä¸­æ–‡ã€‚
å­—æ®µå®šä¹‰:
- source_term: æºè¯­(ä¸­æ–‡æœ¯è¯­æˆ–ä¸“å)
- target_term: è¯‘æ–‡(è‹±æ–‡)
- domain: é¢†åŸŸ.å–å€¼é›†åˆä¹‹ä¸€:["æ”¿æ²»","ç»æµ","æ–‡åŒ–","æ–‡ç‰©","é‡‘è","æ³•å¾‹","å…¶ä»–"]
- strategy: ç¿»è¯‘ç­–ç•¥.å–å€¼é›†åˆä¹‹ä¸€:["ç›´è¯‘","æ„è¯‘","è½¬è¯‘","éŸ³è¯‘","çœç•¥","å¢è¯‘","è§„èŒƒåŒ–","å…¶ä»–"]
- example: ä¾‹å¥(åŸæ–‡ä¸­åŒ…å«è¯¥æœ¯è¯­çš„ä¸€å¥.å°½é‡ä¿ç•™æ ‡ç‚¹)

è¦æ±‚:
1) ä»…è¾“å‡º JSON.ä¸è¦å¤šä½™è¯´æ˜ã€‚
2) åŒä¸€æœ¯è¯­é‡å¤æ—¶åˆå¹¶.é€‰æ‹©æœ€å…¸å‹çš„ä¾‹å¥ã€‚
3) è‹¥æ— æ³•åˆ¤æ–­ domain/strategy.å¡«â€œå…¶ä»–â€ã€‚

Text:
{text}
"""

    url = "https://api.deepseek.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {ak}", "Content-Type": "application/json"}
    payload = {
        "model": model or "deepseek-chat",
        "messages": [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        "temperature": 0.1,
    }
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=60)
        r.raise_for_status()
        data = r.json()
        txt = data["choices"][0]["message"]["content"].strip()
        # åªä¿ç•™ JSON ç‰‡æ®µ
        start = txt.find("[")
        end = txt.rfind("]")
        if start == -1 or end == -1:
            return []
        arr = json.loads(txt[start:end+1])
        out = []
        for o in arr:
            src = (o.get("source_term") or o.get("source") or "").strip()
            tgt = (o.get("target_term") or o.get("target") or "").strip()
            dom = (o.get("domain") or "").strip() or None
            strat = (o.get("strategy") or "").strip() or None
            ex = (o.get("example") or "").strip() or None
            if src:
                out.append({"source_term": src, "target_term": tgt, "domain": dom, "strategy": strat, "example": ex})
        return out
    except Exception:
        return []

# ========== æ–‡ä»¶è¯»å†™ä¸å¯¼å‡º ==========
def read_source_file(path: str) -> str:
    if not path or not os.path.exists(path):
        return ""
    ext = os.path.splitext(path)[1].lower()
    if ext == ".txt":
        for enc in ["utf-8", "utf-8-sig", "gb18030", "gbk"]:
            try:
                with open(path, "r", encoding=enc) as f:
                    return f.read()
            except Exception:
                continue
        with open(path, "r", errors="ignore") as f:
            return f.read()
    elif ext == ".docx":
        try:
            from docx import Document
            doc = Document(path)
            return "\n".join(p.text for p in doc.paragraphs)
        except Exception:
            return ""
    elif ext == ".xlsx":
        try:
            xls = pd.ExcelFile(path)
            parts = []
            for sheet in xls.sheet_names:
                df = xls.parse(sheet)
                parts.append(df.astype(str).to_csv(sep=" ", index=False, header=False))
            return "\n".join(parts)
        except Exception:
            return ""
    else:
        # å…œåº•å°è¯•æ–‡æœ¬è¯»å–
        try:
            with open(path, "r", encoding="utf-8") as f:
                return f.read()
        except Exception:
            return ""

def build_bilingual_lines(src_text: str, tgt_text: str):
    """
    ç”¨æ®µè½åšå¯¹é½ï¼š
    - æ¯ä¸€æ®µä¸­æ–‡å¯¹åº”ä¸€æ®µè‹±æ–‡
    - æ®µå†…ä¸å†æ‹†å¥ï¼ˆé¿å… CSV / Word é”™ä½ï¼‰
    """
    return pair_paragraphs(src_text, tgt_text)

def export_csv_bilingual(src_text: str, tgt_text: str) -> bytes:
    s, t = build_bilingual_lines(src_text, tgt_text)
    df = pd.DataFrame({"Source": s, "Target": t})
    buf = io.StringIO()
    df.to_csv(buf, index=False)
    return buf.getvalue().encode("utf-8-sig")

def export_docx_bilingual(src_text: str, tgt_text: str) -> bytes:
    try:
        from docx import Document
        from docx.oxml.ns import qn
    except Exception:
        st.error("ç¼ºå°‘ python-docx.è¯·å…ˆå®‰è£…:pip install python-docx")
        return b""
    doc = Document()
    # åŸºç¡€å­—ä½“
    try:
        doc.styles['Normal'].font.name = 'Calibri'
        doc.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), 'å¾®è½¯é›…é»‘')
    except Exception:
        pass
    s, t = build_bilingual_lines(src_text, tgt_text)
    table = doc.add_table(rows=1, cols=2)
    hdr = table.rows[0].cells
    hdr[0].text = "Source"
    hdr[1].text = "Target"
    for a, b in zip(s, t):
        row = table.add_row().cells
        row[0].text = a
        row[1].text = b
    bio = io.BytesIO()
    doc.save(bio)
    return bio.getvalue()

def export_docx_inplace(src_path: str, tgt_text: str) -> bytes:
    """DOCX:åœ¨æ¯ä¸ªåŸæ®µè½ä¸‹æ’å…¥è¯‘æ–‡æ®µ(ç®€æ´ç‰ˆ)"""
    try:
        from docx import Document
        from docx.shared import Pt
    except Exception:
        st.error("ç¼ºå°‘ python-docx.è¯·å®‰è£…:pip install python-docx")
        return b""

    doc = Document(src_path)
    tgt_lines = [x for x in tgt_text.splitlines()]

    i = 0
    for p in doc.paragraphs:
        tr = tgt_lines[i] if i < len(tgt_lines) else ""
        if tr.strip():
            run = doc.add_paragraph().add_run(tr)
            try:
                run.italic = True
                run.font.size = Pt(10)
            except Exception:
                pass
        i += 1
    while i < len(tgt_lines):
        doc.add_paragraph(tgt_lines[i])
        i += 1

    bio = io.BytesIO()
    doc.save(bio)
    return bio.getvalue()
# ==== å·¥å…·:æ–‡ä»¶è¯»å– / åˆ†å¥ / å‘é‡ / å¯¹é½ / ç´¢å¼• ====
import os, re, io, json, numpy as _np

def _lazy_import_doc_pdf():
    docx = pdfplumber = None
    try:
        import docx as _docx
        docx = _docx
    except Exception:
        pass
    try:
        import pdfplumber as _pdfplumber
        pdfplumber = _pdfplumber
    except Exception:
        pass
    return docx, pdfplumber

def read_any_text(path_or_bytes, ext):
    """è¿”å›çº¯æ–‡æœ¬(å•è¯­)ã€‚ext: 'txt/csv/xlsx/docx/pdf'"""
    import pandas as pd
    from pathlib import Path
    docx, pdfplumber = _lazy_import_doc_pdf()

    def _read_docx_plain(fp):
        if not docx: return ""
        doc = docx.Document(fp)
        paras = [p.text.strip() for p in doc.paragraphs if p.text.strip()]
        return "\n".join(paras)

    def _read_pdf_plain(fp):
        if not pdfplumber: return ""
        text_chunks = []
        with pdfplumber.open(fp) as pdf:
            for page in pdf.pages:
                t = (page.extract_text() or "").strip()
                if t: text_chunks.append(t)
        return "\n".join(text_chunks)

    if ext == "txt":
        if isinstance(path_or_bytes, (str, os.PathLike)):
            return open(path_or_bytes, "r", encoding="utf-8", errors="ignore").read()
        else:
            return path_or_bytes.getvalue().decode("utf-8","ignore")

    if ext == "csv":
        df = pd.read_csv(path_or_bytes if not isinstance(path_or_bytes,(str,os.PathLike)) else path_or_bytes, encoding="utf-8", keep_default_na=False)
        return "\n".join([str(x) for x in _np.ravel(df.values) if str(x).strip()])

    if ext == "xlsx":
        df = pd.read_excel(path_or_bytes if not isinstance(path_or_bytes,(str,os.PathLike)) else path_or_bytes)
        return "\n".join([str(x) for x in _np.ravel(df.values) if str(x).strip()])

    if ext == "docx":
        return _read_docx_plain(path_or_bytes)
    if ext == "pdf":
        return _read_pdf_plain(path_or_bytes)
    return ""

def read_bilingual_pairs(path_or_bytes, ext):
    """è¿”å›[(src,tgt)];æ”¯æŒ:CSV/XLSX ä¸¤åˆ—;DOCX è¡¨æ ¼ä¸¤åˆ—;å…¶ä½™è¿”å›ç©º"""
    docx, _ = _lazy_import_doc_pdf()
    pairs = []

    if ext in ("csv","xlsx"):
        df = (pd.read_csv(path_or_bytes, encoding="utf-8", keep_default_na=False)
              if ext=="csv" else pd.read_excel(path_or_bytes))
        cols = [c.strip().lower() for c in df.columns]
        # å°è¯•è‡ªåŠ¨æ‰¾ä¸¤åˆ—
        if len(cols)>=2:
            c1, c2 = 0, 1
            pairs = [(str(df.iloc[i,c1]).strip(), str(df.iloc[i,c2]).strip())
                     for i in range(len(df))
                     if str(df.iloc[i,c1]).strip() or str(df.iloc[i,c2]).strip()]
    elif ext == "docx" and docx:
        doc = docx.Document(path_or_bytes)
        for tbl in doc.tables:
            # åªå¤„ç†ä¸¤åˆ—è¡¨
            if len(tbl.columns) >= 2:
                for r in tbl.rows:
                    c0 = r.cells[0].text.strip()
                    c1 = r.cells[1].text.strip() if len(r.cells)>1 else ""
                    if c0 or c1:
                        pairs.append((c0, c1))
    return [(s,t) for s,t in pairs if s or t]

# æ®µè½åˆ‡åˆ†
def split_paragraphs(text: str) -> list[str]:
    """
    æ®µè½åˆ‡åˆ†ï¼ˆç”¨äºç¿»è¯‘ & å¯¼å‡ºï¼‰ï¼š
    - ç»Ÿä¸€æ¢è¡Œç¬¦
    - ä»¥ã€è‡³å°‘ä¸€ä¸ªç©ºè¡Œã€‘ä½œä¸ºæ®µè½åˆ†éš”
    - æ®µå†…ä¿ç•™å¥å­ï¼Œåªå»æ‰çº¯ç©ºè¡Œ
    """
    text = (text or "").replace("\r\n", "\n").replace("\r", "\n")
    # å¸¸è§æƒ…å†µï¼šç”¨â€œä¸€è¡Œä¸€æ®µâ€çš„ç¨¿å­ï¼Œå®é™…ä¸Šä¸­é—´æ²¡æœ‰ç©ºè¡Œ
    # è¿™ç§å°±æŒ‰å•è¡Œå½“ä½œæ®µè½
    if "\n\n" not in text and "\n \n" not in text:
        lines = [ln.strip() for ln in text.split("\n")]
        return [ln for ln in lines if ln]

    # æ­£å¸¸ï¼šæœ‰ç©ºè¡Œåˆ†æ®µ
    parts = re.split(r"\n\s*\n+", text)
    paras = []
    for p in parts:
        p = p.strip()
        if not p:
            continue
        # æ®µå†…å¦‚æœè¿˜æœ‰è½¯æ¢è¡Œï¼Œå‹æˆç©ºæ ¼ï¼Œé¿å…å¯¼å‡ºæ—¶è¢«æ‹†æˆå¤šè¡Œ
        p = re.sub(r"\s*\n\s*", " ", p)
        paras.append(p)
    return paras

def pair_paragraphs(src_full: str, tgt_full: str) -> tuple[list[str], list[str]]:
    """
    æ ¹æ®å…¨æ–‡ä¸­è‹±ï¼ŒæŒ‰â€œæ®µè½â€é…å¯¹ï¼š
    - æºæ–‡/è¯‘æ–‡å„è‡ªåš split_paragraphs
    - è¡Œæ•°ä¸ä¸€è‡´æ—¶ç”¨ç©ºä¸²è¡¥é½
    - ä¿è¯å¯¼å‡ºæ—¶ä¸€è¡Œä¸­æ–‡å¯¹åº”ä¸€è¡Œè‹±æ–‡
    """
    src_paras = split_paragraphs(src_full or "")
    tgt_paras = split_paragraphs(tgt_full or "")

    n = max(len(src_paras), len(tgt_paras))
    src_paras += [""] * (n - len(src_paras))
    tgt_paras += [""] * (n - len(tgt_paras))
    return src_paras, tgt_paras

# é¢„ç¼–è¯‘ï¼ˆå¯æ”¾å…¨å±€ï¼‰
_RE_WS = re.compile(r"[ \t\u00A0\u200B\u200C\u200D]+")
_RE_ZH_SENT = re.compile(r"(?<=[ã€‚ï¼ï¼Ÿï¼›])\s*")           # ä¸­æ–‡å¥æœ«
_RE_EN_SENT = re.compile(r"(?<=[\.\?\!;:])\s+")          # è‹±æ–‡å¥æœ«ï¼ˆæ”¾å®½ï¼Œä¸å¼ºåˆ¶å¤§å†™ï¼‰
_RE_BLANK_PARA = re.compile(r"\n{2,}")                   # ç©ºè¡Œåˆ†æ®µ

def _norm_text(text: str) -> str:
    t = (text or "").replace("\r\n", "\n").replace("\r", "\n").replace("\x0b", "\n")
    t = _RE_WS.sub(" ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)  # è¿‡å¤šç©ºè¡Œå‹åˆ°ä¸¤ä¸ª
    return t.strip()

def _is_zh(text: str) -> bool:
    # ç®€å•åˆ¤å®šï¼šå«æœ‰è¾ƒå¤šä¸­æ–‡å­—ç¬¦
    zh_hits = len(re.findall(r"[\u4e00-\u9fff]", text))
    en_hits = len(re.findall(r"[A-Za-z]", text))
    return zh_hits >= en_hits

def split_sents(
    text: str,
    lang_hint: str = "auto",
    min_char: int = 1,
    prefer_newline: bool = True,
    **kwargs,
):
    """
    ç»Ÿä¸€çš„åˆ†å¥/åˆ†æ®µå‡½æ•°ï¼š
    - å…¼å®¹æ—§è°ƒç”¨ï¼šsplit_sents(text, lang="zh")
    - æ”¯æŒæ–°å‚æ•°ï¼šprefer_newline=True æ—¶ï¼Œä¼˜å…ˆæŒ‰æ¢è¡Œåˆ‡
    """
    # å…¼å®¹æ—§å‚æ•°å lang=
    lang = kwargs.get("lang", lang_hint)

    t = _norm_text(text)
    if not t:
        return []

    pieces = []

    # A) è‹¥æ–‡æœ¬ä¸­æœ‰æ¢è¡Œ & prefer_newline=Trueï¼šå…ˆæŒ‰è¡Œåˆ‡ï¼Œå†åœ¨è¡Œå†…æŒ‰å¥æœ«ç»†åˆ†
    if prefer_newline and ("\n" in t):
        lines = [ln.strip() for ln in t.split("\n") if ln.strip()]
        for ln in lines:
            if lang == "auto":
                cur_lang = "zh" if _is_zh(ln) else "en"
            else:
                cur_lang = lang

            if cur_lang.startswith(("zh", "cn")):
                sents = [s.strip() for s in _RE_ZH_SENT.split(ln) if s and s.strip()]
            else:
                sents = [s.strip() for s in _RE_EN_SENT.split(ln) if s and s.strip()]

            pieces.extend(sents if sents else [ln])
    else:
        # B) æ²¡æœ‰æ¢è¡Œæˆ–ä¸åå¥½æ¢è¡Œï¼šæ•´å—æŒ‰å¥æœ«æ ‡ç‚¹åˆ‡
        if lang == "auto":
            cur_lang = "zh" if _is_zh(t) else "en"
        else:
            cur_lang = lang

        if cur_lang.startswith(("zh", "cn")):
            pieces = [s.strip() for s in _RE_ZH_SENT.split(t) if s and s.strip()]
        else:
            pieces = [s.strip() for s in _RE_EN_SENT.split(t) if s and s.strip()]

        if not pieces:
            pieces = [t]

    # è¿‡æ»¤è¿‡çŸ­ç‰‡æ®µ
    return [x for x in pieces if len(x) >= min_char]

# å…¼å®¹æ—§å‡½æ•°å
split_sentences = split_sents

def smart_split_blocks(text: str, max_chars: int = 1200, lang_hint: str = "auto"):
    """
    å…ˆç”¨ split_sentsï¼ˆæŒ‰å¥/æŒ‰è¡Œï¼‰åˆ‡å‡ºåŸºæœ¬å•å…ƒï¼Œ
    - è‹¥æ€»é•¿åº¦ä¸è¶…è¿‡ max_charsï¼šæ¯å¥/è¡Œå•ç‹¬ä½œä¸ºä¸€ä¸ª blockï¼›
    - è‹¥æ€»é•¿åº¦è¶…è¿‡ max_charsï¼šå†æŒ‰é•¿åº¦æ‰“åŒ…æˆå¤§å—ï¼Œä¿è¯ä¸æ‹†å¥ã€‚
    """
    sents = split_sents(text, lang_hint=lang_hint, min_char=1, prefer_newline=True)
    if not sents:
        return []

    # è®¡ç®—å…¨éƒ¨å¥å­çš„æ€»é•¿åº¦ï¼ˆåŠ ä¸Šå°‘é‡æ¢è¡Œï¼‰
    total_len = sum(len(s) for s in sents) + max(0, len(sents) - 1)

    # æƒ…å†µä¸€ï¼šæ•´ä½“ä¸é•¿ï¼Œç›´æ¥æŒ‰å¥/è¡Œè¿”å›
    if total_len <= max_chars:
        return sents

    # æƒ…å†µäºŒï¼šæ•´ä½“è¾ƒé•¿ï¼Œå†æŒ‰é•¿åº¦æ‰“åŒ…
    blocks = []
    buf = ""

    for s in sents:
        s = s.strip()
        if not s:
            continue

        if not buf:
            buf = s
            continue

        if len(buf) + 1 + len(s) <= max_chars:
            buf = buf + "\n" + s
        else:
            blocks.append(buf)
            buf = s

    if buf:
        blocks.append(buf)

    return blocks

def split_blocks(text: str, max_len: int = 1200):
    blocks, curbuf = [], ""
    for line in text.splitlines(True):
        if len(curbuf) + len(line) > max_len:
            if curbuf:
                blocks.append(curbuf)
            curbuf = ""
        curbuf += line
    if curbuf:
        blocks.append(curbuf)
    return blocks
# =========================
# æœ¯è¯­æç¤º & è¯‘åä¸€è‡´æ€§æ£€æŸ¥
# =========================
def check_term_consistency(out_text: str, term_dict: dict, source_text: str = "") -> list:
    """
    è¯‘åçš„ä¸€è‡´æ€§ç²—æ£€:å¦‚æœæºæ–‡åŒ…å«æœ¯è¯­é”®.ä½†è¯‘æ–‡æœªå‡ºç°å¯¹åº”å€¼.åˆ™è®°å½•æé†’ã€‚
    ä»…åšæœ€å°ä»£ä»·çš„å­—ç¬¦ä¸²çº§æ£€æŸ¥(ä¸æ”¹åŠ¨åŸè¯‘æ–‡)ã€‚
    è¿”å›å½¢å¦‚ ["contractâ†’åˆåŒ", ...] çš„åˆ—è¡¨;ä¸ºç©ºè¡¨ç¤ºå…¨éƒ¨ç¬¦åˆ/ä¸é€‚ç”¨ã€‚
    """
    if not out_text or not term_dict:
        return []
    warns = []
    s = (source_text or "")[:2000]  # é™é•¿åº¦.é˜²æç«¯é•¿æ–‡æœ¬
    out_low = out_text.lower()
    for k, v in term_dict.items():
        if not k or not v:
            continue
        # å¦‚æœæºæ–‡åŒ…å«è¯¥æœ¯è¯­é”®(å¤§å°å†™å¿½ç•¥è‹±æ–‡;ä¸­æ–‡ç›´æ¥åŒ…å«)
        hit_src = (k.lower() in s.lower()) if any(ord(ch) < 128 for ch in k) else (k in s)
        if hit_src:
            # è¯‘æ–‡æ˜¯å¦å‡ºç°ç›®æ ‡æœ¯è¯­(åŒç†å¤§å°å†™å®¹å¿è‹±æ–‡)
            ok = (v.lower() in out_low) if any(ord(ch) < 128 for ch in v) else (v in out_text)
            if not ok:
                warns.append(f"{k}â†’{v}")
    return warns

def _lazy_embedder():
    # ä¼˜å…ˆ sentence-transformers;å¤±è´¥é€€åŒ–åˆ° TF-IDF
    try:
        from sentence_transformers import SentenceTransformer
        import numpy as np
        mdl = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        def _emb(texts):
            arr = mdl.encode(texts, normalize_embeddings=True)
            return arr.astype("float32")
        return _emb, "sbert"
    except Exception:
        from sklearn.feature_extraction.text import TfidfVectorizer
        import numpy as np
        def _emb(texts):
            vec = TfidfVectorizer(min_df=1).fit_transform(texts)
            # å½’ä¸€åŒ–
            norms = _np.sqrt((vec.multiply(vec)).sum(axis=1)).A.ravel() + 1e-8
            vec = vec.multiply(1/norms[:,None])
            return vec
        return _emb, "tfidf"

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def align_semantic(src_sents, tgt_sents, max_jump=3):
    """ç®€å•è´ªå¿ƒ + æ»‘çª—çš„ 1-1 å¥å¯¹é½.è¿”å› [(src, tgt, score)]"""
    if not src_sents or not tgt_sents:
        return []

    emb, kind = _lazy_embedder()

    # === ä¼˜å…ˆä½¿ç”¨ SBERT ===
    if kind == "sbert":
        E1 = emb(src_sents)
        E2 = emb(tgt_sents)
        sims = E1 @ E2.T  # (n, m)
    else:
        # === TF-IDF å›é€€:ç¡®ä¿åŒä¸€è¯è¡¨ç»´åº¦ ===
        vec = TfidfVectorizer(
            analyzer="char_wb",  # å­—ç¬¦ n-gram å¯¹ä¸­è‹±æ··åˆæœ€ç¨³
            ngram_range=(1, 2),
            min_df=1
        )
        combo = src_sents + tgt_sents
        X = vec.fit_transform(combo)
        n = len(src_sents)
        E1 = X[:n, :]
        E2 = X[n:, :]

        # ç¨€ç–â†’ç›¸ä¼¼åº¦çŸ©é˜µ
        sims = cosine_similarity(E1, E2, dense_output=True)  # shape (n, m)

    # === è´ªå¿ƒå¯¹é½ ===
    i = j = 0
    n, m = len(src_sents), len(tgt_sents)
    pairs = []
    while i < n and j < m:
        j_min = max(0, j - max_jump)
        j_max = min(m, j + max_jump + 1)
        window = sims[i, j_min:j_max]
        if window.size == 0:
            break
        k = int(window.argmax())
        j_sel = j_min + k
        score = float(sims[i, j_sel])
        pairs.append((src_sents[i], tgt_sents[j_sel], score))
        i += 1
        j = j_sel + 1
    return pairs

# ===== å‘é‡ç´¢å¼•(FAISSä¼˜å…ˆ.é™çº§ä¸º NumPy) =====
def _lazy_faiss():
    try:
        import faiss
        return faiss
    except Exception:
        return None

def save_semantic_index(index_dir, pid, texts, vectors):
    os.makedirs(index_dir, exist_ok=True)
    _np.save(os.path.join(index_dir, f"{pid}_texts.npy"), _np.array(texts, dtype=object))
    faiss = _lazy_faiss()
    if faiss is not None and isinstance(vectors, _np.ndarray):
        idx = faiss.IndexFlatIP(vectors.shape[1])
        idx.add(vectors)
        faiss.write_index(idx, os.path.join(index_dir, f"{pid}.faiss"))
    else:
        _np.save(os.path.join(index_dir, f"{pid}_vecs.npy"), vectors)

# ========== æœ¯è¯­åº“ç®¡ç† ==========
def render_term_management(st, cur, conn, base_dir, key_prefix="term"):
    sk = make_sk(key_prefix)

    st.subheader("ğŸ“˜ æœ¯è¯­åº“ç®¡ç†")
    sub_tabs = st.tabs(["æŸ¥è¯¢ä¸ç¼–è¾‘", "æ‰¹é‡å¯¼å…¥ CSV", "ç»Ÿè®¡ä¸å¯¼å‡º", "å¿«é€Ÿæœç´¢", "æ‰¹é‡æŒ‚æ¥é¡¹ç›®", "ä»å†å²æå–æœ¯è¯­", "åˆ†ç±»ç®¡ç†"])

    # â€”â€” æŸ¥è¯¢ä¸ç¼–è¾‘
    with sub_tabs[0]:
        sk0 = lambda name: f"{key_prefix}_t0_{name}"

        c1, c2, c3, c4 = st.columns(4)
        with c1: kw = st.text_input("å…³é”®è¯(æº/ç›®æ ‡/ä¾‹å¥)", "", key=sk("kw_example"))
        with c2: dom = st.text_input("é¢†åŸŸ", "", key=sk("dom"))
        with c3: strat = st.text_input("ç­–ç•¥", "", key=sk("strat"))
        with c4: pid = st.text_input("é¡¹ç›®IDè¿‡æ»¤", "", key=sk("pid"))
        cat = st.text_input("åˆ†ç±»(æ”¯æŒå­ä¸²)", "", key=sk("cat"))

        # â€”â€” å…¼å®¹è€åº“:å¦‚æ—  category åˆ—åˆ™è¡¥åˆ—(å·²æœ‰ä¼šå¿½ç•¥)
        try:
            cur.execute("ALTER TABLE term_ext ADD COLUMN category TEXT;")
            conn.commit()
        except Exception:
            pass

        # 1) ä»¥æ•°æ®åº“çœŸå®åˆ—ä¸ºå‡†æ£€æµ‹æ˜¯å¦å­˜åœ¨ category
        cols_db = [c[1].lower() for c in cur.execute("PRAGMA table_info(term_ext);").fetchall()]
        has_category = ("category" in cols_db)

        # 2) æ‹¼ SQL(åªåœ¨ DB çœŸçš„æœ‰è¯¥åˆ—æ—¶æ‰ SELECT category)
        base_cols = "id, source_term, target_term, domain, project_id, strategy, example"
        sel_cols = base_cols + (", category" if has_category else "")
        sql = f"SELECT {sel_cols} FROM term_ext WHERE 1=1"
        params = []

        if kw:
            like = f"%{kw}%"
            sql += " AND (IFNULL(source_term,'') LIKE ? OR IFNULL(target_term,'') LIKE ? OR IFNULL(example,'') LIKE ?)"
            params += [like, like, like]

        if dom:
            sql += " AND IFNULL(domain,'') LIKE ?"
            params += [f"%{dom}%"]

        if strat:
            sql += " AND IFNULL(strategy,'') LIKE ?"
            params += [f"%{strat}%"]

        if pid and str(pid).isdigit():
            sql += " AND IFNULL(project_id,0) = ?"
            params += [int(pid)]

        if has_category and cat:
            sql += " AND IFNULL(category,'') LIKE ?"
            params += [f"%{cat}%"]

        sql += " ORDER BY source_term COLLATE NOCASE LIMIT 1000"

        # 3) æŸ¥è¯¢å¹¶æ„é€  DataFrame(è¡¨å¤´ä¸å®é™…åˆ—å¯¹é½)
        rows = cur.execute(sql, params).fetchall()
        headers = ["ID","æºæœ¯è¯­","ç›®æ ‡æœ¯è¯­","é¢†åŸŸ","é¡¹ç›®ID","ç­–ç•¥","ä¾‹å¥"]
        if has_category:
            headers.append("åˆ†ç±»")

        df = pd.DataFrame(rows, columns=headers)
        st.caption(f"å½“å‰æŸ¥è¯¢è¿”å›:{len(df)} æ¡")

        # 4) ç©ºæ•°æ®å°±ä¸æ¸²æŸ“ç¼–è¾‘å™¨
        if df.empty:
            st.info("æ²¡æœ‰åŒ¹é…çš„æœ¯è¯­ã€‚")
        else:
            # æ²¡æœ‰ sel åˆ—åˆ™æ’å…¥
            if "sel" not in df.columns:
                df.insert(0, "sel", False)

            # åŠ¨æ€æ„å»ºåˆ—é…ç½®.åªæœ‰å½“ DB çœŸæœ‰â€œåˆ†ç±»â€æ—¶æ‰åŠ å…¥
            col_cfg = {
                "ID": st.column_config.NumberColumn("ID", disabled=True),
                "sel": st.column_config.CheckboxColumn("é€‰æ‹©"),
                "æºæœ¯è¯­": "æºæœ¯è¯­",
                "ç›®æ ‡æœ¯è¯­": "ç›®æ ‡æœ¯è¯­",
                "é¢†åŸŸ": "é¢†åŸŸ",
                "é¡¹ç›®ID": st.column_config.NumberColumn("é¡¹ç›®ID", step=1, required=False),
                "ç­–ç•¥": "ç­–ç•¥",
                "ä¾‹å¥": st.column_config.TextColumn("ä¾‹å¥"),
            }
            if has_category:
                col_cfg["åˆ†ç±»"] = st.column_config.TextColumn("åˆ†ç±»")

            edited = st.data_editor(
                df,
                num_rows="dynamic",
                key=sk0("editor"),
                column_config=col_cfg,
            )

            c1, c2, c3 = st.columns([1, 1, 2])
            with c1:
                if st.button("ğŸ’¾ ä¿å­˜ä¿®æ”¹", type="primary", key=sk("save_terms")):
                    updated = inserted = 0
                    for _, row in edited.iterrows():
                        if pd.notna(row["ID"]):  # æ›´æ–°
                            if has_category:
                                cur.execute("""
                                    UPDATE term_ext
                                    SET source_term=?, target_term=?, domain=?, project_id=?, strategy=?, example=?, category=?
                                    WHERE id=?;
                                """, (
                                    (row["æºæœ¯è¯­"] or "").strip(),
                                    (row["ç›®æ ‡æœ¯è¯­"] or None),
                                    (row["é¢†åŸŸ"] or None),
                                    (int(row["é¡¹ç›®ID"]) if pd.notna(row["é¡¹ç›®ID"]) else None),
                                    (row["ç­–ç•¥"] or None),
                                    (row["ä¾‹å¥"] or None),
                                    (row.get("åˆ†ç±»") or None),
                                    int(row["ID"])
                                ))
                            else:
                                cur.execute("""
                                    UPDATE term_ext
                                    SET source_term=?, target_term=?, domain=?, project_id=?, strategy=?, example=?
                                    WHERE id=?;
                                """, (
                                    (row["æºæœ¯è¯­"] or "").strip(),
                                    (row["ç›®æ ‡æœ¯è¯­"] or None),
                                    (row["é¢†åŸŸ"] or None),
                                    (int(row["é¡¹ç›®ID"]) if pd.notna(row["é¡¹ç›®ID"]) else None),
                                    (row["ç­–ç•¥"] or None),
                                    (row["ä¾‹å¥"] or None),
                                    int(row["ID"])
                                ))
                            updated += cur.rowcount
                        else:  # æ–°å¢
                            if str(row["æºæœ¯è¯­"]).strip():
                                if has_category:
                                    cur.execute("""
                                        INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example, category)
                                        VALUES (?, ?, ?, ?, ?, ?, ?)
                                    """, (
                                        (row["æºæœ¯è¯­"] or "").strip(),
                                        (row["ç›®æ ‡æœ¯è¯­"] or None),
                                        (row["é¢†åŸŸ"] or None),
                                        (int(row["é¡¹ç›®ID"]) if pd.notna(row["é¡¹ç›®ID"]) else None),
                                        (row["ç­–ç•¥"] or None),
                                        (row["ä¾‹å¥"] or None),
                                        (row.get("åˆ†ç±»") or None)
                                    ))
                                else:
                                    cur.execute("""
                                        INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example)
                                        VALUES (?, ?, ?, ?, ?, ?)
                                    """, (
                                        (row["æºæœ¯è¯­"] or "").strip(),
                                        (row["ç›®æ ‡æœ¯è¯­"] or None),
                                        (row["é¢†åŸŸ"] or None),
                                        (int(row["é¡¹ç›®ID"]) if pd.notna(row["é¡¹ç›®ID"]) else None),
                                        (row["ç­–ç•¥"] or None),
                                        (row["ä¾‹å¥"] or None),
                                    ))
                                inserted += 1
                    conn.commit()
                    st.success(f"âœ… å·²ä¿å­˜:æ›´æ–° {updated} æ¡.æ–°å¢ {inserted} æ¡ã€‚")
                    st.rerun()

                with c2:
                    cc2a, cc2b, cc2c = st.columns([1, 1, 2])
                    if "sel" not in edited.columns:
                        edited.insert(0, "sel", False)
                    if cc2a.button("å…¨é€‰", key=sk("sel_all")):
                        if "ID" in edited.columns:
                            all_ids = edited[pd.notna(edited["ID"])]["ID"].astype(int).tolist()
                            edited.loc[:, "sel"] = True
                            st.session_state[sk("selected_ids")] = set(all_ids)
                        st.rerun()
                    if cc2b.button("æ¸…ç©º", key=sk("sel_clear")):
                        edited.loc[:, "sel"] = False
                        st.session_state[sk("selected_ids")] = set()
                        st.rerun()
                    if cc2c.button("ğŸ—‘ï¸ åˆ é™¤å·²å‹¾é€‰", key=sk("del_sel")):
                        if "ID" in edited.columns:
                            to_delete = edited[(edited["sel"] == True) & pd.notna(edited["ID"])]["ID"].astype(int).tolist()
                        else:
                            to_delete = []
                        if not to_delete:
                            st.warning("æœªå‹¾é€‰ä»»ä½•è®°å½•")
                        else:
                            cur.executemany("DELETE FROM term_ext WHERE id=?", [(i,) for i in to_delete])
                            conn.commit()
                            st.success(f"ğŸ—‘ï¸ å·²åˆ é™¤ {len(to_delete)} æ¡")
                            st.rerun()

                with c3:
                    proj_opts = cur.execute("SELECT id, title FROM items ORDER BY id DESC").fetchall()
                    proj_map = {"(ä¸æŒ‚æ¥/ç½®ç©º)": None, **{f"#{i} {t}": i for (i, t) in proj_opts}}

                    cc3a, cc3b = st.columns([2, 1])
                    target_proj_label = cc3a.selectbox("æ‰¹é‡æŒ‚æ¥åˆ°é¡¹ç›®", list(proj_map.keys()), key=sk("bind_proj_sel"))
                    if cc3b.button("æ‰§è¡ŒæŒ‚æ¥", type="primary", key=sk("bind_apply")):
                        if "ID" in edited.columns:
                            to_update = edited[(edited["sel"] == True) & pd.notna(edited["ID"])]["ID"].astype(int).tolist()
                        else:
                            to_update = []
                        if not to_update:
                            st.warning("æœªå‹¾é€‰ä»»ä½•è®°å½•")
                        else:
                            pid_val = proj_map.get(target_proj_label)
                            q_marks = ",".join("?" for _ in to_update)
                            cur.execute(f"UPDATE term_ext SET project_id=? WHERE id IN ({q_marks})", (pid_val, *to_update))
                            conn.commit()
                            st.success(f"âœ… å·²æŒ‚æ¥ {len(to_update)} æ¡åˆ°é¡¹ç›®:{target_proj_label or '(ç©º)'}")
                            st.rerun()
                    else:
                        st.info("æš‚æ— æœ¯è¯­è®°å½•")

            st.markdown("### å•æ¡æ–°å¢ / ç¼–è¾‘")
            with st.form(sk0("term_edit")):
                col1, col2, col3 = st.columns(3)
                with col1:
                    rid_edit = st.text_input("è¦ç¼–è¾‘çš„è®°å½• ID(ç•™ç©ºåˆ™æ–°å¢)", "", key=sk("rid_edit"))
                    source_term = st.text_input("æºè¯­è¨€æœ¯è¯­(å¿…å¡«)*", key=sk("source_term"))
                    target_term = st.text_input("ç›®æ ‡è¯­è¨€æœ¯è¯­", key=sk("target_term"))
                with col2:
                    domain = st.text_input("é¢†åŸŸ", key=sk("domain"))
                    project_id = st.text_input("é¡¹ç›®ID(å¯ç©º)", key=sk("project_id"))
                    strategy = st.text_input("ç­–ç•¥(ç›´è¯‘/æ„è¯‘/è½¬è¯‘/éŸ³è¯‘/çœç•¥/å¢è¯‘/è§„èŒƒåŒ–â€¦)", key=sk("strategy"))
                with col3:
                    example = st.text_area("ä¾‹å¥", height=80, key=sk("example"))
                    category = st.text_input("åˆ†ç±»(å¯é€‰)", key=sk("category"))

                b1, b2 = st.columns(2)
                add = b1.form_submit_button("ä¿å­˜(æ–°å¢æˆ–æ›´æ–°)")
                delbtn = b2.form_submit_button("åˆ é™¤(æŒ‰ ID)")

            if add:
                if not source_term.strip():
                    st.error("æºæœ¯è¯­å¿…å¡«")
                else:
                    if rid_edit and rid_edit.isdigit():
                        if _has_col("term_ext", "category"):
                            cur.execute("""
                            UPDATE term_ext
                            SET source_term=?, target_term=?, domain=?, project_id=?, strategy=?, example=?, category=?
                            WHERE id=?;
                            """, (source_term.strip(), target_term.strip() or None, domain or None,
                                int(project_id) if project_id.isdigit() else None, strategy or None, example or None,
                                category or None, int(rid_edit)))
                        else:
                            cur.execute("""
                            UPDATE term_ext
                            SET source_term=?, target_term=?, domain=?, project_id=?, strategy=?, example=?
                            WHERE id=?;
                            """, (source_term.strip(), target_term.strip() or None, domain or None,
                                int(project_id) if project_id.isdigit() else None, strategy or None, example or None,
                                int(rid_edit)))
                        conn.commit(); st.success("âœ… å·²æ›´æ–°"); st.rerun()
                    else:
                        if _has_col("term_ext", "category"):
                            cur.execute("""
                            INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example, category)
                            VALUES (?, ?, ?, ?, ?, ?, ?)
                            """, (
                                source_term.strip(),
                                (target_term.strip() or None) if target_term else None,
                                (domain or None),
                                (int(project_id) if project_id.isdigit() else None) if project_id else None,
                                (strategy or None),
                                (example or None),
                                (category or None)
                            ))
                        else:
                            cur.execute("""
                            INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example)
                            VALUES (?, ?, ?, ?, ?, ?)
                            """, (
                                source_term.strip(),
                                (target_term.strip() or None) if target_term else None,
                                (domain or None),
                                (int(project_id) if project_id.isdigit() else None) if project_id else None,
                                (strategy or None),
                                (example or None),
                            ))
                        conn.commit(); st.success("âœ… å·²æ–°å¢"); st.rerun()

            if delbtn:
                if rid_edit and rid_edit.isdigit():
                    cur.execute("DELETE FROM term_ext WHERE id=?", (int(rid_edit),))
                    conn.commit(); st.success("ğŸ—‘ï¸ å·²åˆ é™¤"); st.rerun()
                else:
                    st.error("è¯·å¡«å†™è¦åˆ é™¤çš„ ID")

    # â€”â€” æ‰¹é‡å¯¼å…¥ CSV(å¢å¼ºç‰ˆ:åˆ—åè§„èŒƒåŒ– / åŠ¨æ€å¸¦æˆ–ä¸å¸¦ category / å»é‡æˆ–Upsert / é€è¡Œå®¹é”™)
    with sub_tabs[1]:
        sk1 = lambda n: f"{key_prefix}_t1_{n}"
        st.caption("CSV æ¨èåˆ—:source_term, target_term, domain, project_id, strategy, example(å¯é€‰:category / åˆ†ç±»)")
        up = st.file_uploader("ä¸Šä¼  CSV æ–‡ä»¶", type=["csv"], key=sk1("csv"))

        # å°å·¥å…·:è¡¥åˆ— + å”¯ä¸€ç´¢å¼• + è§„èŒƒå‡½æ•°
        def _ensure_col(table, col, type_):
            try:
                cur.execute(f"ALTER TABLE {table} ADD COLUMN {col} {type_};")
                conn.commit()
            except Exception:
                pass

        def _ensure_unique_index():
            try:
                cur.execute("""
                CREATE UNIQUE INDEX IF NOT EXISTS ux_term_proj
                ON term_ext(LOWER(TRIM(source_term)), IFNULL(project_id,-1));
                """)
                conn.commit()
            except Exception:
                pass

        def _norm_cols(df):
            # åˆ—å:å»BOM/ä¸¤ç«¯ç©ºæ ¼ -> å°å†™ -> æ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿ -> ä¸­è‹±åˆ—åæ˜ å°„
            df.columns = [str(c).replace("\ufeff","").strip() for c in df.columns]
            df.columns = [c.lower().replace(" ", "_") for c in df.columns]
            mapping = {
                "æºæœ¯è¯­": "source_term", "source": "source_term",
                "ç›®æ ‡æœ¯è¯­": "target_term", "target": "target_term",
                "é¢†åŸŸ": "domain",
                "é¡¹ç›®id": "project_id", "é¡¹ç›®_id": "project_id",
                "ç­–ç•¥": "strategy",
                "ä¾‹å¥": "example",
                "åˆ†ç±»": "category",
            }
            df = df.rename(columns={k.lower(): v for k, v in mapping.items()})
            return df

        def _norm(v):
            if v is None: return None
            v = str(v).strip().replace("\u3000", " ")
            return v if v else None

        def _to_int(v):
            try:
                return int(v) if v is not None and str(v).strip() != "" else None
            except Exception:
                return None

        if up is not None:
            try:
                df_up = pd.read_csv(up, encoding="utf-8-sig")
            except Exception:
                df_up = pd.read_csv(up, encoding="utf-8", errors="ignore")

            df_up = _norm_cols(df_up)
            st.write("æ£€æµ‹åˆ°åˆ—:", list(df_up.columns))
            render_table(df_up.head(10), key=sk("csv_preview"))

            # DB ä¾§ç¡®ä¿æœ‰ category åˆ—(å…¼å®¹è€åº“;è‹¥å·²æœ‰ä¼šå¿½ç•¥)
            _ensure_col("term_ext", "category", "TEXT")

            # ä¾§è¾¹é€‰é¡¹
            c1, c2, c3 = st.columns(3)
            with c1:
                dedup = st.checkbox("å»é‡(æºæœ¯è¯­+é¡¹ç›®ID)", value=True, key=sk1("dedup"))
            with c2:
                use_upsert = st.checkbox("å·²å­˜åœ¨åˆ™æ›´æ–°(Upsert)", value=False, key=sk1("upsert"))
            with c3:
                skip_empty = st.checkbox("è·³è¿‡ç©ºè¯‘æ–‡", value=False, key=sk1("skip_empty"))

            # Upsert éœ€è¦å”¯ä¸€ç´¢å¼•
            if use_upsert:
                _ensure_unique_index()

            if st.button("å¯¼å…¥æœ¯è¯­åº“", key=sk1("import_btn")):
                # æ˜¯å¦åŒ…å« category åˆ—(ä»¥CSVä¸ºå‡†;DBå·²æœ‰ä¸å¼ºåˆ¶CSVå¿…é¡»æœ‰)
                has_category_col = ("category" in df_up.columns)

                # å»é‡ç¼“å­˜(ä»…åœ¨éUpsertæ¨¡å¼ä¸‹ä½¿ç”¨)
                existing = set()
                if dedup and not use_upsert:
                    rows_exist = cur.execute("""
                        SELECT LOWER(TRIM(source_term)), IFNULL(project_id,-1)
                        FROM term_ext
                    """).fetchall()
                    existing = set(rows_exist)

                ins = skp = upd = 0
                errors = []

                for idx, row in df_up.iterrows():
                    src = _norm(row.get("source_term"))
                    if not src:
                        skp += 1
                        continue

                    tgt = _norm(row.get("target_term"))
                    if skip_empty and not tgt:
                        skp += 1
                        continue

                    dom = _norm(row.get("domain"))
                    pid = _to_int(row.get("project_id"))
                    stg = _norm(row.get("strategy"))
                    exa = _norm(row.get("example"))
                    cat = _norm(row.get("category")) if has_category_col else None

                    try:
                        if use_upsert:
                            # Upsert åˆ†æ”¯(éœ€è¦å”¯ä¸€ç´¢å¼•)
                            if has_category_col:
                                cur.execute("""
                                INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example, category)
                                VALUES (?,?,?,?,?,?,?)
                                ON CONFLICT(LOWER(TRIM(source_term)), IFNULL(project_id,-1))
                                DO UPDATE SET
                                    target_term=COALESCE(excluded.target_term, term_ext.target_term),
                                    domain     =COALESCE(excluded.domain,      term_ext.domain),
                                    strategy   =COALESCE(excluded.strategy,    term_ext.strategy),
                                    example    =COALESCE(excluded.example,     term_ext.example),
                                    category   =COALESCE(excluded.category,    term_ext.category);
                                """, (src, tgt, dom, pid, stg, exa, cat))
                            else:
                                cur.execute("""
                                INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example)
                                VALUES (?,?,?,?,?,?)
                                ON CONFLICT(LOWER(TRIM(source_term)), IFNULL(project_id,-1))
                                DO UPDATE SET
                                    target_term=COALESCE(excluded.target_term, term_ext.target_term),
                                    domain     =COALESCE(excluded.domain,      term_ext.domain),
                                    strategy   =COALESCE(excluded.strategy,    term_ext.strategy),
                                    example    =COALESCE(excluded.example,     term_ext.example);
                                """, (src, tgt, dom, pid, stg, exa))
                            upd += 1  # è®¡ä¸ºâ€œå¤„ç†æˆåŠŸâ€.ä¸åŒºåˆ†æ–°æ—§
                        else:
                            # é Upsert:å»é‡(æºæœ¯è¯­+é¡¹ç›®ID)
                            key = (src.lower(), pid if pid is not None else -1)
                            if dedup and key in existing:
                                skp += 1
                                continue

                            if has_category_col:
                                cur.execute("""
                                    INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example, category)
                                    VALUES (?,?,?,?,?,?,?)
                                """, (src, tgt, dom, pid, stg, exa, cat))
                            else:
                                cur.execute("""
                                    INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example)
                                    VALUES (?,?,?,?,?,?)
                                """, (src, tgt, dom, pid, stg, exa))
                            ins += 1
                            if dedup:
                                existing.add(key)

                    except Exception as e:
                        errors.append((idx+1, src, str(e)))
                        skp += 1
                        continue

                conn.commit()

                # ç»“æœæç¤º
                if use_upsert:
                    st.success(f"âœ… å·²å¤„ç† {ins+upd} æ¡(å…¶ä¸­å¯èƒ½å«æ–°å¢+æ›´æ–°).è·³è¿‡ {skp} æ¡ã€‚")
                else:
                    st.success(f"âœ… æ–°å¢ {ins} æ¡.è·³è¿‡ {skp} æ¡ã€‚")

                if errors:
                    with st.expander("â— è¡Œçº§é”™è¯¯æ˜ç»†(ä¸å½±å“å…¶ä»–è¡Œå†™å…¥)", expanded=False):
                        for i, s, e in errors:
                            st.write(f"ç¬¬ {i} è¡Œ({s}):{e}")


    # â€”â€” ç»Ÿè®¡ä¸å¯¼å‡º
    with sub_tabs[2]:
        sk2 = lambda n: f"{key_prefix}_t2_{n}"
        st.markdown("#### æœ¯è¯­ç»Ÿè®¡")
        df_stats = pd.read_sql_query("SELECT strategy, domain, category FROM term_ext WHERE source_term IS NOT NULL", conn)
        if df_stats.empty:
            st.info("æœ¯è¯­åº“ä¸ºç©º.è¯·å…ˆæ·»åŠ æˆ–å¯¼å…¥")
        else:
            strat_count = df_stats["strategy"].fillna("æœªæ ‡æ³¨").value_counts().reset_index()
            strat_count.columns = ["strategy","count"]
            render_table(strat_count, hide_index=True, key=sk2("strat_tbl"))

            dom_count = df_stats["domain"].fillna("æœªæ ‡æ³¨").value_counts().reset_index()
            dom_count.columns = ["domain","count"]
            render_table(dom_count, hide_index=True, key=sk2("dom_tbl"), editable=True)

        st.markdown("---")
        st.markdown("#### å¯¼å‡ºæœ¯è¯­è¡¨")
        df_exp = pd.read_sql_query("""
            SELECT source_term AS 'æºæœ¯è¯­',
                   target_term AS 'ç›®æ ‡æœ¯è¯­',
                   domain AS 'é¢†åŸŸ',
                   strategy AS 'ç¿»è¯‘ç­–ç•¥',
                   example AS 'ç¤ºä¾‹å¥',
                   category AS 'åˆ†ç±»'
            FROM term_ext ORDER BY source_term COLLATE NOCASE
        """, conn)

        from io import BytesIO
        buff = BytesIO()
        with pd.ExcelWriter(buff, engine="xlsxwriter") as writer:
            df_exp.to_excel(writer, index=False, sheet_name="æœ¯è¯­åº“")
        st.download_button("ğŸ“¥ ä¸‹è½½ Excel",
                           buff.getvalue(),
                           file_name="terms.xlsx",
                           mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                           key=sk2("dl_terms"))

    # â€”â€” å¿«é€Ÿæœç´¢
    with sub_tabs[3]:
        sk3 = lambda n: f"{key_prefix}_t3_{n}"
        q = st.text_input("å¿«é€Ÿæœç´¢(å‰ç¼€/å­ä¸²)", "", key=sk3("q"))
        limit = st.number_input("è¿”å›ä¸Šé™", 1, 5000, 1000, 100, key=sk3("limit"))
        if st.button("æœç´¢", key=sk3("q_btn")):
            if q:
                like = f"%{q}%"
                rows = cur.execute("""
                    SELECT id, source_term, target_term, domain, project_id
                    FROM term_ext
                    WHERE source_term LIKE ? OR target_term LIKE ?
                    ORDER BY id DESC
                    LIMIT ?
                """, (like, like, int(limit))).fetchall()
                render_table(pd.DataFrame(rows, columns=["ID","æºæœ¯è¯­","ç›®æ ‡æœ¯è¯­","é¢†åŸŸ","é¡¹ç›®"]),
                             key=sk3("q_grid"),editable=True)
            else:
                st.warning("è¯·è¾“å…¥å…³é”®è¯")

    # â€”â€” æ‰¹é‡æŒ‚æ¥é¡¹ç›®
    with sub_tabs[4]:
        sk4 = lambda n: f"{key_prefix}_t4_{n}"
        st.caption("å°†ä¸€æ‰¹æœ¯è¯­ç»Ÿä¸€è®¾ç½® project_id.ä¾¿äºé¡¹ç›®å†…ä¼˜å…ˆåŒ¹é…")
        ids_txt = st.text_area("æœ¯è¯­IDåˆ—è¡¨(é€—å·/ç©ºæ ¼/æ¢è¡Œåˆ†éš”)", key=sk4("ids"))
        pid_to = st.text_input("ç›®æ ‡é¡¹ç›®ID", key=sk4("pid_to"))
        if st.button("æ‰¹é‡æŒ‚æ¥", key=sk4("batch_btn")):
            import re
            if not pid_to.isdigit():
                st.error("é¡¹ç›®IDéœ€ä¸ºæ•°å­—")
            else:
                raw = re.split(r"[,\s]+", ids_txt.strip())
                ids = [int(x) for x in raw if x.isdigit()]
                if not ids:
                    st.warning("æœªè¯†åˆ«åˆ°æœ‰æ•ˆID")
                else:
                    qmarks = ",".join(["?"]*len(ids))
                    cur.execute(f"UPDATE term_ext SET project_id=? WHERE id IN ({qmarks})", (int(pid_to), *ids))
                    conn.commit()
                    st.success(f"âœ… å·²æŒ‚æ¥ {len(ids)} æ¡åˆ°é¡¹ç›® {pid_to}")

    # â€”â€” ä»å†å²æå–æœ¯è¯­
    with sub_tabs[5]:
        sk5 = lambda n: f"{key_prefix}_t5_{n}"
        st.markdown("#### ä»ç¿»è¯‘å†å²è®°å½•æŠ½å–æœ¯è¯­(DeepSeek)")
        ak, model = get_deepseek()
        if not ak:
            st.warning("æœªæ£€æµ‹åˆ° DeepSeek Key.è¯·å…ˆåœ¨â€œè®¾ç½®â€ä¸­é…ç½®ã€‚")
        else:
            mode_pick = st.radio(
                "é€‰æ‹©æ¥æº",
                ["æŒ‰é¡¹ç›®æŠ½å–(åˆå¹¶å¤šæ¡)", "æŒ‰å•æ¡è®°å½•æŠ½å–"],
                horizontal=True,
                key=sk5("ext_mode"),
            )
            if mode_pick == "æŒ‰é¡¹ç›®æŠ½å–(åˆå¹¶å¤šæ¡)":
                pid_ext = st.text_input("é¡¹ç›®ID", key=sk5("ext_pid"))
                max_chars = st.number_input("é‡‡æ ·æœ€å¤§å­—ç¬¦æ•°", 1000, 20000, 5000, 500, key=sk5("ext_max"))
                if st.button("å¼€å§‹æŠ½å–", key=sk5("ext_go_proj")):
                    if pid_ext.isdigit():
                        rows = cur.execute(
                            "SELECT src_path, output_text FROM trans_ext WHERE project_id=? ORDER BY id DESC LIMIT 10",
                            (int(pid_ext),),
                        ).fetchall()
                        buf = []
                        total = 0
                        for sp, ot in rows:
                            src = read_source_file(sp) if sp else ""
                            txt = (src + "\n" + (ot or "")).strip()
                            if not txt:
                                continue
                            if total + len(txt) > int(max_chars):
                                remain = max(0, int(max_chars) - total)
                                buf.append(txt[:remain])
                                break
                            else:
                                buf.append(txt)
                                total += len(txt)
                        big = "\n\n".join(buf)
                        res = ds_extract_terms(big, ak, model, src_lang="zh", tgt_lang="en")
                        if not res:
                            st.info("æœªæŠ½å–åˆ°æœ¯è¯­æˆ–è§£æå¤±è´¥")
                        else:
                            st.success(f"æŠ½å–åˆ° {len(res)} æ¡.å‡†å¤‡æ‰¹é‡å†™å…¥â€¦â€¦")
                            ins = 0
                            for o in res:
                                cur.execute(
                                    "INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example) VALUES (?, ?, ?, ?, ?, ?)",
                                    (o["source_term"], o.get("target_term") or None, o.get("domain"), int(pid_ext), o.get("strategy"), o.get("example")),
                                )
                                ins += 1
                            conn.commit()
                            st.success(f"âœ… å·²å†™å…¥æœ¯è¯­åº“ {ins} æ¡")
                    else:
                        st.warning("è¯·è¾“å…¥æ•°å­—å‹é¡¹ç›®ID")
            else:
                rid_ext = st.text_input("å†å²è®°å½•ID", key=sk5("ext_rid"))
                if st.button("å¼€å§‹æŠ½å–", key=sk5("ext_go_rec")):
                    if rid_ext.isdigit():
                        row = cur.execute(
                            "SELECT src_path, output_text, project_id FROM trans_ext WHERE id=?",
                            (int(rid_ext),),
                        ).fetchone()
                        if not row:
                            st.warning("æœªæ‰¾åˆ°è¯¥è®°å½•")
                        else:
                            sp, ot, pid0 = row
                            src = read_source_file(sp) if sp else ""
                            big = (src + "\n" + (ot or "")).strip()
                            res = ds_extract_terms(big, ak, model, src_lang="zh", tgt_lang="en")
                            if not res:
                                st.info("æœªæŠ½å–åˆ°æœ¯è¯­æˆ–è§£æå¤±è´¥")
                            else:
                                st.success(f"æŠ½å–åˆ° {len(res)} æ¡.å‡†å¤‡æ‰¹é‡å†™å…¥â€¦â€¦")
                                ins = 0
                                for o in res:
                                    cur.execute(
                                        "INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example) VALUES (?, ?, ?, ?, ?, ?)",
                                        (o["source_term"], o.get("target_term") or None, o.get("domain"), pid0, o.get("strategy"), o.get("example")),
                                    )
                                    ins += 1
                                conn.commit()
                                st.success(f"âœ… å·²å†™å…¥æœ¯è¯­åº“ {ins} æ¡(project_id={pid0})")

    # â€”â€” åˆ†ç±»ç®¡ç†
    with sub_tabs[6]:
        sk6 = lambda n: f"{key_prefix}_t6_{n}"
        st.markdown("#### åˆ†ç±»ç®¡ç†")
        c1, c2 = st.columns(2)
        with c1:
            ids_txt = st.text_area("æŒ‰ ID æ‰¹é‡è®¾ç½®åˆ†ç±»(é€—å·/ç©ºæ ¼/æ¢è¡Œåˆ†éš”)", key=sk6("cat_ids"))
            cat_to = st.text_input("è¦è®¾ç½®çš„åˆ†ç±»å", key=sk6("cat_name"))
            if st.button("æ‰¹é‡è®¾ç½®åˆ†ç±»", key=sk6("cat_set_ids")):
                import re
                raw = re.split(r"[,\s]+", (ids_txt or "").strip())
                ids = [int(x) for x in raw if x.isdigit()]
                if not ids or not cat_to.strip():
                    st.warning("è¯·å¡«å…¥IDåˆ—è¡¨ä¸åˆ†ç±»åç§°")
                else:
                    qmarks = ",".join(["?"] * len(ids))
                    cur.execute(f"UPDATE term_ext SET category=? WHERE id IN ({qmarks})", (cat_to.strip(), *ids))
                    conn.commit()
                    st.success(f"âœ… å·²è®¾ç½® {len(ids)} æ¡ä¸ºåˆ†ç±»:{cat_to.strip()}")

        with c2:
            pid_cat = st.text_input("å°†æŸé¡¹ç›®IDå…¨éƒ¨æœ¯è¯­è®¾ç½®ä¸ºåˆ†ç±»", key=sk6("cat_pid"))
            cat2_to = st.text_input("åˆ†ç±»å", key=sk6("cat2_name"))
            if st.button("æŒ‰é¡¹ç›®IDç»Ÿä¸€åˆ†ç±»", key=sk6("cat_set_pid")):
                if pid_cat.isdigit() and cat2_to.strip():
                    cur.execute("UPDATE term_ext SET category=? WHERE project_id=?", (cat2_to.strip(), int(pid_cat)))
                    conn.commit()
                    st.success(f"âœ… å·²å°†é¡¹ç›® {pid_cat} çš„æœ¯è¯­åˆ†ç±»è®¾ä¸º:{cat2_to.strip()}")
                else:
                    st.warning("è¯·å¡«å†™é¡¹ç›®IDä¸åˆ†ç±»å")

# ========== Session åˆå§‹åŒ– ==========
if "kb_embedder" not in st.session_state and KBEmbedder:
    st.session_state["kb_embedder"] = KBEmbedder(lazy=True)

# ========== é¡µé¢ç»“æ„ ==========
st.title("ğŸ§  ä¸ªäººç¿»è¯‘çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ Â· ä¿®æ­£ç‰ˆ03")
tabs = st.tabs(["ğŸ“‚ ç¿»è¯‘é¡¹ç›®ç®¡ç†", "ğŸ“˜ æœ¯è¯­åº“ç®¡ç†", "ğŸ“Š ç¿»è¯‘å†å²", "ğŸ“š è¯­æ–™åº“ç®¡ç†", "âš™ è®¾ç½®"])

# ========== Tab1:ç¿»è¯‘é¡¹ç›®ç®¡ç† ==========
with tabs[0]:
    st.subheader("ç¿»è¯‘é¡¹ç›®ç®¡ç†")
    with st.form("new_project"):
        TAG_OPTIONS = ["æ”¿æ²»", "ç»æµ", "æ–‡åŒ–", "æ–‡ç‰©", "é‡‘è", "æ³•å¾‹"]
        SCENE_OPTIONS = ["å­¦æœ¯", "é…éŸ³ç¨¿", "æ­£å¼ä¼šè®®"]
        use_semantic = st.checkbox("åœ¨ç¿»è¯‘æ—¶å¯ç”¨è¯­ä¹‰å¬å›å‚è€ƒ", value=True)
        
        # === è¯­ä¹‰å¬å›èŒƒå›´é€‰æ‹© ===
        scope_label = "è¯­ä¹‰å¬å›èŒƒå›´"
        scope_options = {
            "ä»…å½“å‰é¡¹ç›®": "project",
            "åŒé¢†åŸŸ + å½“å‰é¡¹ç›®": "domain",
            "å…¨åº“": "all"
        }
        default_scope = "ä»…å½“å‰é¡¹ç›®"
        sel = st.selectbox(
            scope_label,
            list(scope_options.keys()),
            index=list(scope_options.keys()).index(default_scope),
            key="scope_sel_newproj"
        )
        st.session_state["scope_newproj"] = scope_options[sel]

        c1, c2 = st.columns([3, 2])
        with c1:
            title = st.text_input("é¡¹ç›®åç§°")
            tags_sel = st.multiselect("é¡¹ç›®æ ‡ç­¾(å¯å¤šé€‰)", TAG_OPTIONS)
            scene_sel = st.selectbox("åœºåˆ", SCENE_OPTIONS, index=0)
        with c2:
            translation_type = st.selectbox("ç¿»è¯‘æ–¹å¼", ["ä½¿ç”¨æœ¯è¯­åº“", "çº¯æœºå™¨ç¿»è¯‘"])
            translation_mode = st.radio("æ¨¡å¼", ["æ ‡å‡†æ¨¡å¼", "æœ¯è¯­çº¦æŸæ¨¡å¼"], horizontal=True)
            prompt_text = st.text_area(
                "ç¿»è¯‘æç¤º(æ³¨å…¥æ¨¡å‹ System Prompt)",
                placeholder="å†™ä¸‹å¯¹ DeepSeek çš„ç¡¬æ€§/ä¼˜å…ˆçº§æŒ‡ä»¤.å¦‚:æ—¶æ€ç»Ÿä¸€ä¸ºè¿‡å»å¼.ä¸“æœ‰åè¯ä¿æŒåŸæ–‡â€¦â€¦",
                height=120,
                key="new_proj_prompt"
            )
        # === é¢†åŸŸè‡ªåŠ¨ç»‘å®šé€»è¾‘ ===
        # è‹¥ç”¨æˆ·é€‰æ‹©äº†å¤šä¸ªæ ‡ç­¾.åˆ™é»˜è®¤ä»¥ç¬¬ä¸€ä¸ªæ ‡ç­¾ä¸ºé¢†åŸŸ
        domain_val = tags_sel[0] if tags_sel else None

        # === æäº¤æŒ‰é’® ===
        submitted = st.form_submit_button("ğŸ’¾ åˆ›å»ºé¡¹ç›®")
        if submitted:
            if not title:
                st.error("è¯·å¡«å†™é¡¹ç›®åç§°")
            else:
                try:
                    # ç¡®ä¿ items è¡¨å­˜åœ¨ domain å­—æ®µ
                    cur.execute("PRAGMA table_info(items);")
                    cols = [r[1] for r in cur.fetchall()]
                    if "domain" not in cols:
                        cur.execute("ALTER TABLE items ADD COLUMN domain TEXT;")
                        conn.commit()

                    # æ’å…¥æ–°é¡¹ç›®ï¼ˆå« domainï¼‰
                    cur.execute("""
                        INSERT INTO items(title, body, tags, domain,type)
                        VALUES (?, ?, ?, ?, 'project')
                    """, (
                        title,
                        prompt_text or "",
                        ",".join(tags_sel or []),
                        domain_val
                    ))
                    conn.commit()

                    st.success(f"âœ… é¡¹ç›® '{title}' å·²åˆ›å»ºï¼ˆé¢†åŸŸ:{domain_val or 'æœªæŒ‡å®š'}ï¼‰")
                except Exception as e:
                    st.error(f"âŒ åˆ›å»ºé¡¹ç›®å¤±è´¥: {e}")

    rows = cur.execute("""
        SELECT
            i.id,
            i.title,
            COALESCE(i.tags,'')         AS tags,
            COALESCE(e.src_path,'')     AS src_path,
            COALESCE(i.created_at,'')   AS created_at,
            COALESCE(i.scene,'')        AS scene,
            COALESCE(i.prompt,'')       AS prompt,
            COALESCE(i.mode,'')         AS mode,
            COALESCE(i.trans_type,'')   AS trans_type
        FROM items i
        LEFT JOIN item_ext e ON e.item_id = i.id
        WHERE COALESCE(i.type,'')='project'
        ORDER BY i.id DESC
    """).fetchall()

    if not rows:
        st.info("æš‚æ— é¡¹ç›®")
    else:
        for pid, title, tags_str, path, ct, scene, prompt_ro, mode, trans_type in rows:
            tag_display = tags_str or "æ— "
            file_display = os.path.basename(path) if path else "æ— "

            with st.expander(f"{title}ï½œæ–¹å¼:{mode or 'æœªè®¾'}ï½œæ ‡ç­¾:{tag_display}ï½œåœºåˆ:{scene or 'æœªå¡«'}ï½œæ–‡ä»¶:{file_display}ï½œåˆ›å»º:{ct}"):
                c1, c2, c3 = st.columns([2, 2, 1])
                with c1:
                    st.selectbox("ç¿»è¯‘æ–¹å‘", ["ä¸­è¯‘è‹±", "è‹±è¯‘ä¸­"], key=f"lang_{pid}")
                with c2:
                    max_len = st.number_input("åˆ†å—é•¿åº¦", 600, 2000, 1200, 100, key=f"len_{pid}")
                with c3:
                    use_terms = st.checkbox("ä½¿ç”¨æœ¯è¯­åº“", value=(mode == "æœ¯è¯­çº¦æŸæ¨¡å¼"), key=f"ut_{pid}")

                st.caption(f"æ ‡ç­¾:{tag_display}")
                st.caption(f"åœºåˆ:{scene or 'æœªå¡«å†™'}")
               
                # === é¢†åŸŸï¼ˆdomainï¼‰è®¾ç½®:è·Ÿéšç¬¬ä¸€ä¸ªæ ‡ç­¾ æˆ– æ‰‹åŠ¨é€‰æ‹© ===
                # è¯»å–å½“å‰é¡¹ç›®çš„ domain / tags
                # ä¿åº•:items è¡¨è‹¥æ²¡æœ‰ domain åˆ—.åŠ¨æ€è¡¥åˆ—ï¼ˆå…¼å®¹æ—§åº“ï¼‰
                cols_items = [r[1] for r in cur.execute("PRAGMA table_info(items)").fetchall()]
                if "domain" not in cols_items:
                    try:
                        cur.execute("ALTER TABLE items ADD COLUMN domain TEXT;")
                        conn.commit()
                    except Exception:
                        pass  # å¹¶å‘æˆ–å·²æœ‰åˆ—æ—¶å¿½ç•¥
 
                row = cur.execute(
                    "SELECT IFNULL(domain,''), IFNULL(tags,'') FROM items WHERE id=?",
                    (pid,)
                ).fetchone()
                domain0, tags0 = (row or ["", ""])
                tags_list = [t for t in (tags0.split(",") if tags0 else []) if t]

                DOMAIN_OPTIONS = ["æ”¿æ²»", "ç»æµ", "æ–‡åŒ–", "æ–‡ç‰©", "é‡‘è", "æ³•å¾‹"]

                dom_mode = st.radio(
                    "é¢†åŸŸè®¾ç½®æ–¹å¼",
                    ["è·Ÿéšç¬¬ä¸€ä¸ªæ ‡ç­¾", "æ‰‹åŠ¨é€‰æ‹©"],
                    horizontal=True,
                    key=f"dom_mode_{pid}"
                )

                if dom_mode == "è·Ÿéšç¬¬ä¸€ä¸ªæ ‡ç­¾":
                    domain_val = (tags_list[0] if tags_list else (domain0 or None))
                    st.caption(f"å½“å‰é¢†åŸŸï¼ˆè‡ªåŠ¨ï¼‰:{domain_val or 'æœªæŒ‡å®š'}ï¼ˆç”±ç¬¬ä¸€ä¸ªæ ‡ç­¾å†³å®šï¼‰")
                else:
                    idx = DOMAIN_OPTIONS.index(domain0) if domain0 in DOMAIN_OPTIONS else 0
                    domain_val = st.selectbox(
                        "é¢†åŸŸï¼ˆæ‰‹åŠ¨é€‰æ‹©ï¼‰",
                        DOMAIN_OPTIONS,
                        index=idx,
                        key=f"dom_sel_{pid}"
                    )

                sync_corpus = st.checkbox(
                    "åŒæ—¶å›å¡«è¯¥é¡¹ç›®ä¸‹è¯­æ–™çš„é¢†åŸŸï¼ˆä»…è¡¥ç©ºæˆ–åŸé¢†åŸŸç›¸åŒæ—¶è¦†ç›–ï¼‰",
                    value=True,
                    key=f"sync_corpus_{pid}"
                )

                if st.button("ğŸ’¾ ä¿å­˜é¢†åŸŸè®¾ç½®", key=f"save_dom_{pid}", type="secondary"):
                    try:
                        # ç¡®ä¿ items.domain å­˜åœ¨
                        cols_items = [r[1] for r in cur.execute("PRAGMA table_info(items)").fetchall()]
                        if "domain" not in cols_items:
                            cur.execute("ALTER TABLE items ADD COLUMN domain TEXT;")
                            conn.commit()

                        # æ›´æ–° items.domain
                        cur.execute("UPDATE items SET domain=? WHERE id=?", (domain_val, pid))
                        conn.commit()

                        # åŒæ­¥è¯­æ–™åº“çš„ domainï¼ˆä¼˜å…ˆ corpus_main.é€€å› corpusï¼‰
                        def _table_exists(tb: str) -> bool:
                            return bool(cur.execute(
                                "SELECT name FROM sqlite_master WHERE type='table' AND name=?;", (tb,)
                            ).fetchone())

                        corpus_tb = "corpus_main" if _table_exists("corpus_main") else ("corpus" if _table_exists("corpus") else None)
                        if sync_corpus and corpus_tb and domain_val:
                            # ç¡®ä¿åˆ—å­˜åœ¨
                            cols_corpus = [r[1] for r in cur.execute(f"PRAGMA table_info({corpus_tb})").fetchall()]
                            if "domain" not in cols_corpus:
                                cur.execute(f"ALTER TABLE {corpus_tb} ADD COLUMN domain TEXT;")
                                conn.commit()

                            # ä»…è¡¥ç©ºæˆ–åŸ domain ä¸ domain0 ç›¸åŒæ—¶è¦†ç›–.é¿å…è¯¯ä¼¤è·¨é¢†åŸŸæ•°æ®
                            cur.execute(f"""
                                UPDATE {corpus_tb}
                                SET domain = ?
                                WHERE project_id = ?
                                AND (domain IS NULL OR TRIM(domain)='' OR domain = ?)
                            """, (domain_val, pid, domain0 or ""))
                            conn.commit()

                        st.success("âœ… å·²ä¿å­˜é¢†åŸŸè®¾ç½®")
                        st.rerun()

                    except Exception as e:
                        st.error(f"âŒ ä¿å­˜å¤±è´¥:{e}")

                if prompt_ro:
                    try:
                        cur.execute("SELECT IFNULL(prompt, '') FROM items WHERE id=?", (pid,))
                        prompt_ro = (cur.fetchone() or [""])[0]
                    except Exception:
                        prompt_ro = ""

                    st.text_area("ç¿»è¯‘æç¤º(åªè¯»)", prompt_ro or "", height=120, key=f"proj_prompt_ro_{pid}")

                colf1, colf2, colf3 = st.columns([1, 1, 2])
                if path and os.path.exists(path):
                    st.caption(f"æºæ–‡ä»¶:{path}")
                    if colf1.button("åˆ é™¤æ–‡ä»¶", key=f"del_file_{pid}"):
                        try:
                            os.remove(path)
                        except Exception:
                            pass
                        cur.execute("UPDATE item_ext SET src_path='' WHERE item_id=?", (pid,))
                        conn.commit()
                        st.success("å·²åˆ é™¤æ–‡ä»¶")
                        st.rerun()

                    if colf2.button("åˆ é™¤é¡¹ç›®", key=f"del_proj_{pid}"):
                        try:
                            if path and os.path.exists(path):
                                os.remove(path)
                        except Exception:
                            pass
                        cur.execute("DELETE FROM items WHERE id=?", (pid,))
                        cur.execute("DELETE FROM item_ext WHERE item_id=?", (pid,))
                        conn.commit()
                        st.success("é¡¹ç›®å·²åˆ é™¤")
                        st.rerun()

                else:
                    up2 = st.file_uploader("è¡¥ä¼ æ–‡ä»¶", type=["txt","docx","xlsx"], key=f"up2_{pid}")
                    if up2:
                        new_path = os.path.join(PROJECT_DIR, f"{pid}_{up2.name}")
                        with open(new_path, "wb") as f:
                            f.write(up2.read())
                        cur.execute("SELECT id FROM item_ext WHERE item_id=?", (pid,))
                        r = cur.fetchone()
                        if r:
                            cur.execute("UPDATE item_ext SET src_path=? WHERE id=?", (new_path, r[0]))
                        else:
                            cur.execute("INSERT INTO item_ext (item_id, src_path) VALUES (?, ?)", (pid, new_path))
                        conn.commit()
                        st.success("âœ… å·²ä¸Šä¼ å¹¶å…³è”")
                        st.rerun()

                # â€”â€” æ‰§è¡Œç¿»è¯‘
                if st.button("æ‰§è¡Œç¿»è¯‘", key=f"run_{pid}", type="primary"):
                    # 1) ç»“æœç¼“å­˜åˆå§‹åŒ–(ç»Ÿä¸€ç”¨ session_stateï¼‰
                    if "all_results" not in st.session_state:
                        st.session_state["all_results"] = []
                    st.session_state["all_results"].clear()

                    # 2) ç¯å¢ƒæ£€æŸ¥
                    ak, model = get_deepseek()
                    if not ak:
                        st.error("æœªæ£€æµ‹åˆ° DeepSeek Key.è¯·åœ¨ `.streamlit/secrets.toml` é…ç½® [deepseek]")
                        st.stop()
                    if not path or not os.path.exists(path):
                        st.error("ç¼ºå°‘æºæ–‡ä»¶")
                        st.stop()

                    # 3) è¯»å–ä¸åˆ†æ®µ
                    src_text = read_source_file(path)
                    st.code(repr(src_text[:400]))                     # çœ‹å­—ç¬¦ä¸²é‡Œæœ‰æ²¡æœ‰ '\n'
                    st.write({"len": len(src_text), "nl": src_text.count("\n"), "cr": src_text.count("\r")})
                    st.write({"preview_lines": src_text.splitlines()[:3]})
                    
                    # ç”¨ç»Ÿä¸€çš„ split_paragraphs åšåˆ‡åˆ†
                    blocks = split_paragraphs(src_text)
                    if not blocks:
                        st.error("æºæ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œæˆ–æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ®µè½")
                        st.stop()

                    st.info(f"æŒ‰æ®µè½åˆ‡åˆ†ï¼Œå…± {len(blocks)} æ®µï¼Œå¼€å§‹ç¿»è¯‘â€¦")

                    lang_pair_val = st.session_state.get(f"lang_{pid}", "ä¸­è¯‘è‹±")
                    use_semantic  = bool(st.session_state.get(f"use_sem_{pid}", True))
                    scope_val     = st.session_state.get(f"scope_{pid}", st.session_state.get("scope_newproj", "project"))

                    # 4) å¾ªç¯ç¿»è¯‘
                    proj_terms_all = load_terms_for_project(cur, pid)  # ä¸€æ¬¡å–å…¨(å…¨å±€+é¡¹ç›®ï¼‰.å¾ªç¯å†…åšå‘½ä¸­æ”¶ç¼©

                    def _detect_hits(block_text: str, term_map: dict[str, str]) -> dict[str, str]:
                        bt_low = (block_text or "").lower()
                        out = {}
                        for k, v in (term_map or {}).items():
                            if not k:
                                continue
                            key_low = k.lower()
                            if key_low in bt_low or k in block_text:
                                out[k] = v
                        return out

                    for i, blk in enumerate(blocks, start=1):
                        blk = str(blk or "").strip()
                        if not blk:
                            continue

                        # æœ¯è¯­:å¥å†…å‘½ä¸­ + (è‹¥æœ‰ï¼‰åŠ¨æ€è¦†ç›–
                        hits = _detect_hits(blk, proj_terms_all)
                        dyn_map = dyn_map if "dyn_map" in locals() and isinstance(dyn_map, dict) else {}
                        merged_terms = {**hits, **dyn_map}

                        # â€”â€” å‚è€ƒä¾‹å¥ï¼ˆä»æ¥è‡ªè¯­æ–™åº“ corpusï¼‰â€”â€”
                        if use_semantic:
                            try:
                                # ç”¨ä½ è‡ªå·±çš„ split_sentsï¼šä¸­è‹±æ–‡éƒ½èƒ½åˆ‡
                                sents_blk = split_sents(
                                    blk,
                                    lang_hint="auto",
                                    min_char=3,
                                    prefer_newline=True
                                )
                            except TypeError:
                                # å¦‚æœä½ ç°åœ¨çš„ split_sents è¿˜æ²¡æœ‰è¿™äº›å‚æ•°ï¼Œå°±é€€å›æœ€ç®€å•ç‰ˆæœ¬
                                try:
                                    sents_blk = split_sents(blk, lang_hint="auto")
                                except Exception:
                                    sents_blk = [blk]

                            if sents_blk:
                                # å–æœ€å 2â€“3 å¥åšâ€œå±€éƒ¨è¯­ä¹‰ç„¦ç‚¹â€
                                if len(sents_blk) > 3:
                                    focus_text = "\n".join(sents_blk[-3:])
                                else:
                                    focus_text = "\n".join(sents_blk)
                            else:
                                focus_text = blk

                            ref_context = _build_ref_context(
                                pid,
                                focus_text,
                                topk=20,
                                min_sim=0.35,
                                prefer_side="both",
                                scope=scope_val
                            )
                        else:
                            ref_context = ""


                        # é¢„è§ˆ(å¯é€‰ï¼‰
                        with st.expander(f"ğŸ” æœ¬æ®µ({i}) æœ¯è¯­æ³¨å…¥é¢„è§ˆ", expanded=False):
                            st.code((build_term_hint(merged_terms, lang_pair_val) or "")[:1200])
                        with st.expander("ğŸ”— é¡¹ç›®+åŠ¨æ€æœ¯è¯­(åˆå¹¶åæ˜ å°„ï¼‰", expanded=False):
                            st.dataframe(pd.DataFrame([(k, v) for k, v in merged_terms.items()],
                                                    columns=["source_term", "target_term"]),
                                        use_container_width=True)
                        with st.expander("ğŸ“š å‚è€ƒä¾‹å¥å—(æ³¨å…¥å‰ï¼‰", expanded=False):
                            st.text(ref_context[:1500])

                        # â€”â€” çœŸæ­£è°ƒç”¨ç¿»è¯‘(åªè°ƒç”¨ä¸€æ¬¡ï¼‰â€”â€”
                        out_text = ds_translate(
                            block=blk,
                            term_dict=merged_terms,        # å…³é”®:æŠŠæœ¯è¯­æ˜ å°„ä¼ è¿›å»
                            lang_pair=lang_pair_val,
                            ak=ak,
                            model=model,
                            ref_context=ref_context
                        )

                        # è®°å½•ç»“æœ(ç»Ÿä¸€ç”¨ session_stateï¼‰
                        st.session_state["all_results"].append(out_text)
                        st.write(f"âœ… ç¬¬ {i} æ®µå®Œæˆ")

                        # è¯‘åä¸€è‡´æ€§æ£€æŸ¥(ä»…æ£€æŸ¥.ä¸è§¦å‘äºŒæ¬¡ç¿»è¯‘ï¼‰
                        violated = check_term_consistency(out_text, merged_terms, blk)
                        if violated:
                            st.warning("ä»¥ä¸‹æœ¯è¯­æœªåœ¨è¯‘æ–‡ä¸­å‡ºç°(å»ºè®®äººå·¥æ ¸å¯¹ï¼‰: " + ".".join(violated))

                    # ===== å¾ªç¯ç»“æŸå:æ±‡æ€»è¾“å‡º =====

                    # ç»“æœä¸æºæ®µè½å…œåº•ä¸å¯¹é½
                    all_results_safe = list(st.session_state.get("all_results", []))
                    blocks_src_safe  = list(blocks if 'blocks' in locals() else [])
                    if len(blocks_src_safe) != len(all_results_safe):
                        n = min(len(blocks_src_safe), len(all_results_safe))
                        blocks_src_safe  = blocks_src_safe[:n]
                        all_results_safe = all_results_safe[:n]

                    # æ–‡æœ¬æ±‡æ€»
                    final_text = "\n\n".join(all_results_safe)

                    # ä¸€è‡´æ€§æŠ¥å‘Š(è¯­ä¹‰+æœ¯è¯­ï¼‰
                    try:
                        term_map_report = proj_terms_all  # ç›´æ¥ç”¨åˆšæ‰åŠ è½½çš„â€œå…¨å±€+é¡¹ç›®â€æœ¯è¯­
                        df_rep = semantic_consistency_report(
                            project_id=pid,
                            blocks_src=blocks_src_safe,
                            blocks_tgt=all_results_safe,
                            term_map=term_map_report,
                            topk=3,
                            thr=0.70
                        )
                        st.markdown("### ğŸ” è¯‘åä¸€è‡´æ€§æŠ¥å‘Š(è¯­ä¹‰+æœ¯è¯­)")
                        st.dataframe(df_rep, use_container_width=True)
                    except Exception as e:
                        st.caption(f"(ä¸€è‡´æ€§æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e})")

                    # ä¸‹è½½æŒ‰é’®(TXTï¼‰
                    proj_title = (title if 'title' in locals() and title else f"project_{pid}")
                    st.download_button(
                        "â¬‡ï¸ ä¸‹è½½ç¿»è¯‘ç»“æœ (TXT)",
                        final_text or "",
                        file_name=f"{proj_title}_ç¿»è¯‘ç»“æœ.txt",
                        mime="text/plain",
                        key=f"dl_txt_{pid}"
                    )

                    # å†™å…¥å†å² trans_ext
                    try:
                        # å…¼å®¹å–å€¼ï¼ˆéƒ½åšäº†å…œåº•.é˜²æ­¢æœªå®šä¹‰ï¼‰
                        src_path = path if ('path' in locals() and path) else None
                        mode_val = mode if ('mode' in locals() and mode) else "æ ‡å‡†æ¨¡å¼"
                        lang_pair_val = lang_pair if ('lang_pair' in locals() and lang_pair) else "è‡ªåŠ¨"

                        # è®¡ç®—æ®µæ•°:å»ºè®®ä»¥ã€Œæºæ–‡æœ¬ã€ç»Ÿè®¡ï¼›è‹¥æ— åˆ™é€€å›æœ€ç»ˆè¯‘æ–‡
                        src_for_seg = src_text if ('src_text' in locals() and src_text) else final_text
                        seg_count = len(split_sents(src_for_seg, lang_hint="auto"))

                        cur.execute("""
                            INSERT INTO trans_ext (
                                project_id, src_path, lang_pair, mode, output_text,
                                stats_json, segments, term_hit_total, created_at
                            )
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))
                        """, (
                            pid,             # é¡¹ç›®ID
                            src_path,        # æºæ–‡ä»¶è·¯å¾„ï¼ˆå¯ä¸ºç©ºï¼‰
                            lang_pair_val,   # è¯­å¯¹
                            mode_val,        # æ¨¡å¼
                            final_text,      # è¾“å‡ºæ–‡æœ¬ï¼ˆæœ€ç»ˆè¯‘æ–‡ï¼‰
                            None,            # ç»Ÿè®¡JSONï¼ˆå ä½ï¼‰
                            seg_count,       # æ®µæ•°ï¼ˆä¿®å¤:ä¸ç”¨ blocks_src_safeï¼‰
                            None             # æœ¯è¯­å‘½ä¸­æ•°ï¼ˆå ä½.å¯åç»­å¡«çœŸå®å€¼ï¼‰
                        ))
                        conn.commit()
                        st.success("ğŸ“ å·²å†™å…¥ç¿»è¯‘å†å²")
                    except Exception as e:
                        st.warning(f"å†™å…¥ç¿»è¯‘å†å²å¤±è´¥: {e}")



# ========== Tab2:æœ¯è¯­åº“ç®¡ç† ==========
with tabs[1]:
    render_term_management(st, cur, conn, BASE_DIR, key_prefix="term")

# ========== Tab3:ç¿»è¯‘å†å²(å¢å¼ºç‰ˆ) ==========
with tabs[2]:
    st.subheader("ğŸ“Š ç¿»è¯‘å†å²è®°å½•(å¯å†™å…¥è¯­æ–™ / æŠ½å–æœ¯è¯­ / ä¸‹è½½å¯¹ç…§ / åˆ é™¤)")

    rows = cur.execute("""
        SELECT id, project_id, lang_pair,
               substr(IFNULL(output_text,''),1,120) AS prev, created_at
        FROM trans_ext
        ORDER BY datetime(created_at) DESC
        LIMIT 200
    """).fetchall()

    if not rows:
        st.info("æš‚æ— å†å²è®°å½•ã€‚")
    else:
        for rid, pid, lp, prev, ts in rows:
            # é¡¹ç›®æ ‡é¢˜(åšè¯­æ–™æ ‡é¢˜/å±•ç¤º)
            ttl_row = cur.execute("SELECT IFNULL(title,'') FROM items WHERE id=?", (pid,)).fetchone()
            proj_title = (ttl_row or [""])[0] or f"project#{pid}"

            with st.expander(f"#{rid}ï½œé¡¹ç›® {pid}ï½œ{proj_title}ï½œ{lp}ï½œ{ts}", expanded=False):
                # è¯‘æ–‡å…¨æ–‡ & æºæ–‡ä»¶è·¯å¾„
                det = cur.execute("SELECT output_text, src_path FROM trans_ext WHERE id=?", (rid,)).fetchone()
                tgt_full, src_path = det or ("", "")
                st.code(prev or "", language="text")
                st.text_area("è¯‘æ–‡å…¨æ–‡", tgt_full or "", height=220, key=f"hist_full_{rid}")

                # å°è¯•è¯»å–åŸæ–‡(å¦‚æœå½“æ—¶ä¿å­˜äº†æºæ–‡ä»¶è·¯å¾„)
                try:
                    src_full = read_source_file(src_path) if src_path else ""
                except Exception:
                    src_full = ""

                with st.expander("åŸæ–‡é¢„è§ˆ(è‹¥ä¸Šä¼ äº†æºæ–‡ä»¶)", expanded=False):
                    st.text_area("åŸæ–‡å…¨æ–‡", src_full or "(æœªä¿å­˜/æœªä¸Šä¼ æºæ–‡ä»¶)", height=160, key=f"hist_src_{rid}")

                c1, c2, c3, c4, c5 = st.columns(5)

                # 1) æ·»åŠ è¿›è¯­æ–™åº“
                with c1:
                    if st.button("â• æ·»åŠ è¿›è¯­æ–™åº“", key=f"hist_add_corpus_{rid}"):
                        cur.execute("""
                            INSERT INTO corpus (title, project_id, lang_pair, src_text, tgt_text, note, created_at)
                            VALUES (?, ?, ?, ?, ?, ?, datetime('now'))
                        """, (f"{proj_title} Â· history#{rid}", pid, lp, src_full or None, tgt_full or "", f"from trans_ext#{rid}",))
                        conn.commit()
                        st.success("âœ… å·²å†™å…¥è¯­æ–™åº“")

                # 2) æå–æœ¯è¯­(èµ°ä½ ç°æœ‰çš„ DeepSeek æŠ½å–å‡½æ•°)
                with c2:
                    if st.button("ğŸ§  æå–æœ¯è¯­", key=f"hist_extract_terms_{rid}"):
                        ak, model = get_deepseek()
                        if not ak:
                            st.warning("æœªæ£€æµ‹åˆ° DeepSeek Key(è¯·åˆ°â€œè®¾ç½®â€é¡µé…ç½®)")
                        else:
                            # åˆå¹¶åŸæ–‡+è¯‘æ–‡.æé«˜å€™é€‰è´¨é‡
                            big = ((src_full or "") + "\n" + (tgt_full or "")).strip()
                            res = ds_extract_terms(big, ak, model, src_lang="zh", tgt_lang="en")
                            if not res:
                                st.info("æœªæŠ½å–åˆ°æœ¯è¯­æˆ–è§£æå¤±è´¥")
                            else:
                                ins = 0
                                for o in res:
                                    cur.execute("""
                                        INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example)
                                        VALUES (?, ?, ?, ?, ?, ?)
                                    """, (
                                        o.get("source_term") or "",
                                        (o.get("target_term") or None),
                                        (o.get("domain") or None),
                                        pid,
                                        (o.get("strategy") or "history-extract"),
                                        (o.get("example") or None),
                                    ))
                                    ins += 1
                                conn.commit()
                                st.success(f"âœ… å·²å†™å…¥æœ¯è¯­åº“ {ins} æ¡")

                # 3) ä¸‹è½½åŒè¯­å¯¹ç…§(CSV / DOCX)
                with c3:
                    if st.button("â¬‡ï¸ CSV å¯¹ç…§", key=f"hist_dl_bicsv_btn_{rid}"):
                        if not src_full:
                            st.warning("æ‰¾ä¸åˆ°åŸæ–‡(æœªä¸Šä¼ æºæ–‡ä»¶).æ— æ³•ç”Ÿæˆ CSV å¯¹ç…§")
                        else:
                            try:
                                csv_name = f"bilingual_history_{rid}.csv"
                                csv_bytes = export_csv_bilingual((src_full, tgt_full),
                                    list(zip(
                                        [s for s in src_full.splitlines() if s.strip()],
                                        [t for t in tgt_full.splitlines() if t.strip()]
                                    )),
                                    filename=f"bilingual_history_{rid}.csv"
                                )
                            except TypeError:
                                # å¦‚æœä½ çš„å¯¼å‡ºå‡½æ•°æ˜¯ textâ†’bytes ç‰ˆæœ¬
                                csv_name = f"bilingual_history_{rid}.csv"
                                csv_bytes = export_csv_bilingual(src_full, tgt_full)
                            st.download_button("ä¸‹è½½ CSV", data=csv_bytes,
                                               file_name=csv_name, mime="text/csv",
                                               key=f"hist_dl_bicsv_{rid}")

                with c4:
                    if st.button("â¬‡ï¸ DOCX å¯¹ç…§", key=f"hist_dl_bidocx_btn_{rid}"):
                        if not src_full:
                            st.warning("æ‰¾ä¸åˆ°åŸæ–‡(æœªä¸Šä¼ æºæ–‡ä»¶).æ— æ³•ç”Ÿæˆ DOCX å¯¹ç…§")
                        else:
                            try:
                                docx_path = export_docx_bilingual(
                                    list(zip(
                                        [s for s in src_full.splitlines() if s.strip()],
                                        [t for t in tgt_full.splitlines() if t.strip()]
                                    )),
                                    filename=f"bilingual_history_{rid}.docx"
                                )
                                with open(docx_path, "rb") as f:
                                    data_docx = f.read()
                            except TypeError:
                                # å¦‚æœä½ çš„å¯¼å‡ºå‡½æ•°æ˜¯ textâ†’bytes ç‰ˆæœ¬
                                data_docx = export_docx_bilingual(src_full, tgt_full)
                            st.download_button("ä¸‹è½½ DOCX", data=data_docx,
                                               file_name=f"bilingual_history_{rid}.docx",
                                               mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                               key=f"hist_dl_bidocx_{rid}")

                # 4) ğŸ—‘ åˆ é™¤æœ¬æ¡å†å²(å®‰å…¨ç¡®è®¤)
                with c5:
                    with st.expander("ğŸ—‘ åˆ é™¤æœ¬æ¡å†å²(ä¸å¯æ¢å¤)", expanded=False):
                        st.warning("æ­¤æ“ä½œå°†æ°¸ä¹…åˆ é™¤è¯¥æ¡ trans_ext è®°å½•(ä¸å½±å“å·²å†™å…¥è¯­æ–™åº“/æœ¯è¯­è¡¨çš„æ•°æ®)ã€‚")
                        ok = st.checkbox(f"æˆ‘ç¡®è®¤åˆ é™¤ #{rid}", key=f"hist_del_ck_{rid}")
                        if st.button("ç¡®è®¤åˆ é™¤", key=f"hist_del_btn_{rid}") and ok:
                            cur.execute("DELETE FROM trans_ext WHERE id=?", (rid,))
                            conn.commit()
                            st.success("å·²åˆ é™¤.è¯·åˆ·æ–°é¡µé¢æŸ¥çœ‹ç»“æœã€‚")
                            st.stop()  # ç»ˆæ­¢æœ¬æ¬¡æ¸²æŸ“.é¿å…åœ¨å·²åˆ é™¤æ•°æ®ä¸Šç»§ç»­æ“ä½œ

                # åŸæœ‰çš„â€œä¸‹è½½è¯‘æ–‡ TXTâ€
                st.download_button("ä¸‹è½½è¯‘æ–‡ (TXT)", tgt_full or "",
                                   file_name=f"history_{rid}.txt",
                                   mime="text/plain",
                                   key=f"hist_dl_txt_{rid}")

# ========== Tab4:è¯­æ–™åº“ç®¡ç† ==========
def render_corpus_manager(st, cur, conn, pid_prefix="corpus"):
    st.header("ğŸ“š è¯­æ–™åº“ç®¡ç†")
    sk = make_sk(pid_prefix)

with tabs[3]:
    render_corpus_manager(st, cur, conn)

    # è¯­æ–™åº“è¯­ä¹‰ç´¢å¼•ç”¨åŒä¸€ä¸ªç¼“å­˜ç›®å½•
    index_dir = INDEX_DIR  # INDEX_DIR å‰é¢å…¨å±€å·²ç»å®šä¹‰ = Path(BASE_DIR) / ".cache_index"
    sub = st.tabs(["æ–°å»ºè¯­æ–™", "æµè§ˆ/æ£€ç´¢", "ä½¿ç”¨ä¸å¯¼å‡º"])
    # -------- æ–°å»ºè¯­æ–™ --------
    with sub[0]:
        st.subheader("ğŸ“¥ ä¸Šä¼  / å¯¹é½ / å…¥åº“")

        colA, colB = st.columns(2)
        with colA:
            one_file = st.file_uploader("â‘  å•ä¸ªæ–‡ä»¶(DOCX è¡¨æ ¼å¯¹ç…§ / å•è¯­ DOCX/TXT/PDF)",
                                        type=["docx", "txt", "pdf"], key="up_one")
        with colB:
            two_src = st.file_uploader("â‘¡ åŸæ–‡æ–‡ä»¶(å¯é€‰:ä¸ â‘¢ æ­é…åšå¯¹é½)",
                                    type=["docx", "txt", "csv", "pdf"], key="up_src")
            two_tgt = st.file_uploader("â‘¢ è¯‘æ–‡æ–‡ä»¶(å¯é€‰:ä¸ â‘¡ æ­é…åšå¯¹é½)",
                                    type=["docx", "txt", "csv", "pdf"], key="up_tgt")

        st.divider()
        meta1, meta2, meta3 = st.columns([2,1,1])
        with meta1:
            title = st.text_input("è¯­æ–™æ ‡é¢˜", value="æœªå‘½åè¯­æ–™")
        with meta2:
            lp = st.selectbox("æ–¹å‘", ["è‡ªåŠ¨", "ä¸­è¯‘è‹±", "è‹±è¯‘ä¸­"])
        with meta3:
            pid_val = st.text_input("é¡¹ç›®ID(å¯ç•™ç©º)")
        pid = int(pid_val) if pid_val.strip().isdigit() else None

        ins = 0
        pairs = []
        src_text = tgt_text = ""
        preview_df = None

        # ========== è·¯å¾„ A:å•ä¸ªæ–‡ä»¶ ==========
        if one_file is not None and (two_src is None and two_tgt is None):
            ext = (one_file.name.split(".")[-1] or "").lower()
            bio = io.BytesIO(one_file.getvalue())

            if ext == "docx":
                # å…ˆå°è¯•â€œè¡¨æ ¼å¯¹ç…§â€
                tables = read_docx_tables_info(io.BytesIO(bio.getvalue()))
                if tables:
                    st.caption("æ£€æµ‹åˆ° DOCX è¡¨æ ¼.ä¼˜å…ˆä½œä¸ºåŒè¯­å¯¹ç…§å¯¼å…¥ã€‚")
                    # ç®€å•èµ·è§.é»˜è®¤ç¬¬ 0 å¼ è¡¨çš„ç¬¬ 0/1 åˆ—;ä½ ä¹Ÿå¯ä»¥åŠ å…¥ä¸‹æ‹‰é€‰æ‹©
                    pairs = extract_pairs_from_docx_table(io.BytesIO(bio.getvalue()),
                                                        table_index=0, src_col=0, tgt_col=1,
                                                        ffill=True, drop_empty_both=True, dedup=True)
                    if not pairs:
                        # æ— æ³•æŠ½åˆ°å¯¹ç…§ â†’ å½“ä½œå•è¯­æ–‡æœ¬
                        src_text = read_docx_text(io.BytesIO(bio.getvalue()))
                else:
                    # æ²¡æœ‰è¡¨æ ¼ â†’ å•è¯­æ–‡æœ¬
                    src_text = read_docx_text(io.BytesIO(bio.getvalue()))

            elif ext == "txt":
                src_text = read_txt(bio)

            elif ext == "pdf":
                src_text = read_pdf_text(io.BytesIO(bio.getvalue()))

        # ========== è·¯å¾„ B:ä¸¤ä¸ªæ–‡ä»¶(åŸæ–‡ + è¯‘æ–‡)==========
        elif two_src is not None and two_tgt is not None:
            def read_any(f):
                e = (f.name.split(".")[-1] or "").lower()
                b = io.BytesIO(f.getvalue())
                if e == "docx":  return read_docx_text(b)
                if e == "txt":   return read_txt(b)
                if e == "csv":   # å‡å®šé¦–åˆ—æ–‡æœ¬
                    try:
                        df = pd.read_csv(b)
                        return "\n".join(df.iloc[:,0].astype(str).fillna(""))
                    except Exception:
                        return ""
                if e == "pdf":   return read_pdf_text(b)
                return ""
            src_text = read_any(two_src)
            tgt_text = read_any(two_tgt)

        # ========== é¢„è§ˆä¸å†³å®šå…¥åº“æ–¹å¼ ==========
        # æƒ…å†µ 1:æœ‰ pairs(æ¥è‡ª DOCX è¡¨æ ¼)
        if pairs:
            st.success(f"è§£æåˆ° {len(pairs)} å¯¹(DOCX è¡¨æ ¼)")
            preview_df = pd.DataFrame(pairs[:200], columns=["æºå¥","ç›®æ ‡å¥"])

        # æƒ…å†µ 2:æ²¡æœ‰ pairs.ä½†æ‹¿åˆ°äº† src/tgt æ–‡æœ¬ â†’ åˆ‡å¥/å¯¹é½
        elif src_text and tgt_text:
            sents_src = split_sents(src_text, "zh" if lp.startswith("ä¸­") else "auto")
            sents_tgt = split_sents(tgt_text, "en" if lp.startswith("è‹±") else "auto")
            st.caption(f"å°†å¯¹é½:src={len(sents_src)}  tgt={len(sents_tgt)}")
            if st.button("ğŸ” æ‰§è¡Œè¯­ä¹‰å¯¹é½", key="do_align"):
                pairs_aligned = align_semantic(sents_src, sents_tgt, max_jump=5)
                st.info(f"å¯¹é½å¾—åˆ° {len(pairs_aligned)} å¯¹")
                pairs = [(s,t) for (s,t,score) in pairs_aligned]
                if pairs:
                    preview_df = pd.DataFrame(pairs[:200], columns=["æºå¥","ç›®æ ‡å¥"])

        # æƒ…å†µ 3:åªæœ‰å•è¯­æ–‡æœ¬(PDF/DOCX/TXT)
        elif src_text and not tgt_text:
            sents_src = split_sents(src_text, "zh" if lp.startswith("ä¸­") else "auto")
            st.info(f"æ£€æµ‹åˆ°å•è¯­æ–‡æœ¬.å…± {len(sents_src)} å¥;å°†ä»…å†™å…¥ src_textã€‚")
            preview_df = pd.DataFrame([[s, ""] for s in sents_src[:200]], columns=["æºå¥","ç›®æ ‡å¥"])
            pairs = [(s, "") for s in sents_src]

        # é¢„è§ˆ
        if preview_df is not None:
            st.dataframe(preview_df, use_container_width=True)

        # â€”â€” æŒ‰é’® + é€‰é¡¹:å¯¼å…¥è¯­æ–™åº“ | åŒæ—¶å»ºç«‹å‘é‡
        c_imp, c_opt, c_build = st.columns([1,1,1])
        do_import = c_imp.button("ğŸ“¥ å†™å…¥è¯­æ–™åº“", type="primary", key=sk("write_pairs_btn"))
        do_build_opt = c_opt.checkbox("åŒæ—¶å»ºç«‹å‘é‡", value=True, key=sk("build_vec_opt"))
        only_build_now = c_build.button("ğŸ§  ä»…å»ºç«‹å‘é‡(ä¸å¯¼å…¥)", key=sk("only_build"))

        # â€”â€” å°å·¥å…·:æŠŠ [(s,t,score)] / [(s,t)] ç»Ÿä¸€æˆ [(s,t)]
        def normalize_pairs_to2(pairs):
            out = []
            for p in (pairs or []):
                if isinstance(p, (list, tuple)) and len(p) >= 2:
                    out.append((p[0], p[1]))
            return out

        # 4) å†™å…¥è¯­æ–™åº“ +(å¯é€‰)å»ºç´¢å¼•
        if pairs and do_import:
            pairs2 = normalize_pairs_to2(pairs)
            ins = 0
            for s, t in pairs2:
                s = (s or "").strip()
                t = (t or "").strip()
                if not (s or t):
                    continue
                cur.execute("""
                    INSERT INTO corpus(title, project_id, lang_pair, src_text, tgt_text, note, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, datetime('now'))
                """, (title or (one_file.name if one_file else two_src.name),
                    pid, lp, s or None, t or None, "auto-import"))
                ins += 1
            conn.commit()
            st.success(f"âœ… å·²å†™å…¥è¯­æ–™åº“ {ins} æ¡ã€‚")

            # â€”â€” å¯é€‰:å»ºç«‹å‘é‡(åªç”¨ç›®æ ‡å¥.ä¹Ÿå¯æ”¹ä¸ºæºå¥.æŒ‰ä½ çš„æ£€ç´¢ä¹ æƒ¯)
            if do_build_opt:
                texts = [t for _, t in pairs2 if t]   # â† ç”¨ pairs2.é¿å…ä¸‰å…ƒç»„è§£åŒ…é”™è¯¯
                if texts:
                    try:
                        emb, kind = _lazy_embedder()  # ä½ ç°æœ‰çš„æ‡’åŠ è½½(SBERT/TF-IDF)
                        # å¯¼å…¥å/æŒ‰é’®ç‚¹å‡»æ—¶:
                        res = build_project_vector_index(int(pid) if pid is not None else 0,
                                                        use_src=True, use_tgt=True)
                        st.success(f"ğŸ§  å‘é‡ç´¢å¼•å·²æ›´æ–°:æ–°å¢ {res['added']}.æ€»é‡ {res['total']}")
                    except Exception as e:
                        st.warning(f"ç´¢å¼•æœªæ›´æ–°:{e}")

        elif (src_text and not tgt_text) and do_import:
            # â€”â€” å•è¯­:æŒ‰å¥åˆ‡åˆ†å†™å…¥ src_text.tgt_text ç½®ç©º
            sents = split_sents(src_text, "zh" if lp.startswith("ä¸­") else "en")
            ins = 0
            for s in sents:
                s = (s or "").strip()
                if not s: continue
                cur.execute("""
                    INSERT INTO corpus(title, project_id, lang_pair, src_text, tgt_text, note, created_at)
                    VALUES (?, ?, ?, ?, NULL, ?, datetime('now'))
                """, (title or (two_src.name if two_src else "mono"), pid, lp, s, "mono"))
                ins += 1
            conn.commit()
            st.success(f"âœ… å·²å†™å…¥è¯­æ–™åº“ {ins} æ¡ã€‚")

            if do_build_opt and sents:
                try:
                    emb, kind = _lazy_embedder()
                    V = emb(sents)
                    save_semantic_index(str(index_dir), pid or "global", sents, V)
                    st.success(f"ğŸ§  å‘é‡ç´¢å¼•å·²æ›´æ–°({len(sents)} æ¡)ã€‚")
                except Exception as e:
                    st.warning(f"ç´¢å¼•æœªæ›´æ–°:{e}")

        if only_build_now:
            # ä»»é€‰â€œæŒ‰é¡¹ç›®å»ºç´¢å¼•â€æˆ–â€œå…¨åº“å»ºç´¢å¼•â€
            st.info("æ­£åœ¨è¯»å–è¯­æ–™å¹¶å»ºç«‹å‘é‡ç´¢å¼•â€¦")
            rows = cur.execute("SELECT id, IFNULL(src_text,''), IFNULL(tgt_text,'') FROM corpus").fetchall()
            texts = []
            for _, s, t in rows:
                txt = (t or s).strip()   # æŒ‰ä½ çš„æ£€ç´¢ä¹ æƒ¯:ä¼˜å…ˆç”¨è¯‘æ–‡;ä¹Ÿå¯æ¢æˆ s
                if txt:
                    texts.append(txt)
            if not texts:
                st.warning("æ²¡æœ‰å¯å‘é‡åŒ–çš„è¯­æ–™ã€‚")
            else:
                try:
                    emb, kind = _lazy_embedder()
                    V = emb(texts)
                    save_semantic_index(str(index_dir), pid or "global", texts, V)
                    st.success(f"ğŸ§  å·²é‡å»ºå‘é‡ç´¢å¼•({len(texts)} æ¡)ã€‚")
                except Exception as e:
                    st.error(f"é‡å»ºå¤±è´¥:{e}")

        # 5) è¯­ä¹‰æ£€ç´¢(å¯¹å½“å‰é¡¹ç›®æˆ–å…¨å±€)
        st.subheader("ğŸ” è¯­ä¹‰æ£€ç´¢(ä¾‹å¥å¬å›)")
        colq1, colq2 = st.columns([3,1])
        with colq1:
            q = st.text_input("è¾“å…¥è¦æ£€ç´¢çš„çŸ­è¯­/å¥å­", key="corpus_sem_q")
        with colq2:
            topk = st.number_input("TopK", min_value=1, max_value=50, value=5)

        if st.button("ğŸ” è¯­ä¹‰æ£€ç´¢", type="primary"):
            hits = search_semantic(pid, q, topk=int(topk), scope="project")
            for sc, meta, txt in hits:
                st.write(f"å¾—åˆ†: {sc:.3f}")
                st.write(f"æ¥æº: {meta}")
                st.write(txt)
                st.markdown("---")

    # -------- æµè§ˆ/æ£€ç´¢ --------
    with sub[1]:
        st.subheader("ğŸ” æµè§ˆ/æ£€ç´¢")
        k1, k2, k3 = st.columns([2,1,1])
        with k1:
            kw = st.text_input("å…³é”®è¯(æ ‡é¢˜/å¤‡æ³¨/è¯‘æ–‡)", "", key=sk("kw"))
        with k2:
            lp_filter = st.selectbox("æ–¹å‘è¿‡æ»¤", ["å…¨éƒ¨","ä¸­è¯‘è‹±","è‹±è¯‘ä¸­","è‡ªåŠ¨"], key=sk("lp_filter"))
        with k3:
            limit = st.number_input("æ¡æ•°", min_value=10, max_value=1000, value=200, step=10, key=sk("limit"))

        sql = "SELECT id, title, IFNULL(project_id,''), IFNULL(lang_pair,''), substr(IFNULL(tgt_text,''),1,80), created_at FROM corpus WHERE 1=1"
        params = []
        if kw.strip():
            like = f"%{kw.strip()}%"
            sql += " AND (title LIKE ? OR IFNULL(note,'') LIKE ? OR IFNULL(tgt_text,'') LIKE ?)"
            params += [like, like, like]
        if lp_filter != "å…¨éƒ¨":
            sql += " AND IFNULL(lang_pair,'') = ?"; params += [lp_filter]
        sql += " ORDER BY id DESC LIMIT ?"; params += [int(limit)]
        rows = cur.execute(sql, params).fetchall()

        if not rows:
            st.info("æš‚æ— æ•°æ®æˆ–æœªå‘½ä¸­æ£€ç´¢æ¡ä»¶")
        else:
            for rid, t, pid, lpv, prev, ts in rows:
                with st.expander(f"#{rid}ï½œ{t}ï½œ{lpv or 'â€”'}ï½œ{ts}"):
                    st.caption(f"å…³è”é¡¹ç›®:{pid or '(æ— )'}")
                    st.code(prev or "", language="text")
                    c1, c2, c3, c4 = st.columns(4)
                    with c1:
                        if st.button("æŸ¥çœ‹å…¨æ–‡", key=sk(f"view_{rid}")):
                            det = cur.execute("SELECT src_text, tgt_text FROM corpus WHERE id=?", (rid,)).fetchone()
                            st.text_area("æºæ–‡", det[0] or "(æœªä¿å­˜)", height=180, key=sk(f"cor_src_{rid}"))
                            st.text_area("è¯‘æ–‡", det[1] or "", height=220, key=sk(f"cor_tgt_{rid}"))
                    with c2:
                        if st.button("æ ‡è®°ä¸ºå‚è€ƒ", key=sk(f"ref_{rid}")):
                            st.session_state.setdefault("corpus_refs", set())
                            st.session_state["corpus_refs"].add(rid)
                            st.success("âœ… å·²åŠ å…¥å‚è€ƒé›†åˆ(å³ä¾§â€œä½¿ç”¨ä¸å¯¼å‡ºâ€æŸ¥çœ‹)")
                    with c3:
                        if st.button("å¯¼å‡ºTXT", key=sk(f"cor_txt_{rid}")):
                            det = cur.execute("SELECT tgt_text FROM corpus WHERE id=?", (rid,)).fetchone()
                            st.download_button("ä¸‹è½½è¯‘æ–‡TXT", det[0] or "", file_name=f"corpus_{rid}.txt", mime="text/plain", key=sk(f"cor_txt_dl_{rid}"))
                    with c4:
                        if st.button("åˆ é™¤", key=sk(f"del_{rid}")):
                            cur.execute("DELETE FROM corpus WHERE id=?", (rid,))
                            conn.commit()
                            st.warning("ğŸ—‘ï¸ å·²åˆ é™¤.åˆ·æ–°åç”Ÿæ•ˆ")
                            st.rerun()

    # -------- ä½¿ç”¨ä¸å¯¼å‡º --------
    with sub[2]:
        st.subheader("ğŸ§© ä½¿ç”¨ä¸å¯¼å‡º")
        ids = list(st.session_state.get("corpus_refs", []))
        st.caption(f"å·²é€‰å‚è€ƒæ•°:{len(ids)}")
        if ids:
            qmarks = ",".join(["?"] * len(ids))
            dets = cur.execute(f"SELECT id, title, lang_pair, IFNULL(src_text,''), IFNULL(tgt_text,'') FROM corpus WHERE id IN ({qmarks})", ids).fetchall()
            merged_demo = "\n\n---\n\n".join([f"\n\næºæ–‡:\n{src}\n\nè¯‘æ–‡:\n{tgt}" for (i,t,lp,src,tgt) in dets])
            st.text_area("åˆå¹¶é¢„è§ˆ", merged_demo, height=240, key=sk("merge_preview"))

            cxa, cxb = st.columns(2)
            with cxa:
                if st.button("æ¸…ç©ºå‚è€ƒé›†åˆ", key=sk("clear_refs")):
                    st.session_state["corpus_refs"] = set()
                    st.info("å·²æ¸…ç©º")
            with cxb:
                st.download_button("â¬‡ï¸ å¯¼å‡ºåˆå¹¶TXT", merged_demo, file_name="corpus_refs_merged.txt", mime="text/plain", key=sk("merge_dl"))

            st.markdown("---")
            st.subheader("ğŸ”— ç”¨ä½œåç»­ç¿»è¯‘å‚è€ƒ(Few-shot/ç¤ºä¾‹ä¸Šä¸‹æ–‡)")
            st.caption("å‹¾é€‰æ­¤é¡¹å.ç³»ç»Ÿä¼šåœ¨è°ƒç”¨ DeepSeek æ—¶è‡ªåŠ¨æ³¨å…¥è¿™äº›å‚è€ƒç‰‡æ®µ(ä»¥â€œå‚è€ƒç¤ºä¾‹â€å—åŠ å…¥ system prompt)ã€‚")

            use_as_fewshot = st.checkbox("å¯ç”¨å‚è€ƒç¤ºä¾‹æ³¨å…¥", value=True, key="cor_use_ref")
            if st.button("ä¿å­˜å‚è€ƒæ³¨å…¥å¼€å…³", key=sk("save_switch")):
                st.session_state["cor_use_ref"] = bool(use_as_fewshot)
                st.success("âœ… å·²ä¿å­˜(å¯¹åç»­æ–°ç¿»è¯‘ç”Ÿæ•ˆ)")
        else:
            st.info("æš‚æ— é€‰ä¸­å‚è€ƒã€‚è¯·åˆ°ã€æµè§ˆ/æ£€ç´¢ã€‘é¡µå‹¾é€‰â€œæ ‡è®°ä¸ºå‚è€ƒâ€ã€‚")
        
        st.markdown("---")
        st.subheader("ğŸ” å‘é‡ç´¢å¼•(è¯­ä¹‰å¬å›)")
        pid_opts = cur.execute("SELECT id, title FROM items ORDER BY id DESC").fetchall()
        pid_map = {f"#{i} {t}": i for (i, t) in pid_opts}
        proj_sel = st.selectbox("é€‰æ‹©é¡¹ç›®ä»¥æ„å»º/æ›´æ–°ç´¢å¼•", ["(è¯·é€‰æ‹©)"] + list(pid_map.keys()), key=sk("vec_proj"))
        if st.button("âš™ï¸ æ„å»º/æ›´æ–°å‘é‡ç´¢å¼•", key=sk("build_vec")):
            if proj_sel != "(è¯·é€‰æ‹©)":
                res = build_project_vector_index(pid_map[proj_sel], use_src=True, use_tgt=True)
                st.success(f"ç´¢å¼•å·²æ›´æ–°:æ–°å¢ {res['added']}.æ€»é‡ {res['total']}")
            else:
                st.warning("è¯·å…ˆé€‰æ‹©é¡¹ç›®")

        q_demo = st.text_area("è¯•æœä¸€å¥è¯(å°†ä»¥è¯­ä¹‰ç›¸ä¼¼æ£€ç´¢å‚è€ƒ)", "", height=80, key=sk("q_demo"))
        topk = st.number_input("Top-K", 1, 10, 5, key=sk("q_topk"))
        if st.button("ğŸ” è¯­ä¹‰å¬å›æµ‹è¯•", key=sk("q_vec")):
            if proj_sel != "(è¯·é€‰æ‹©)" and q_demo.strip():
                hits = semantic_retrieve(pid_map[proj_sel], q_demo.strip(), topk=int(topk))
                if not hits:
                    st.info("ç´¢å¼•ä¸ºç©ºæˆ–æœªå‘½ä¸­ã€‚è¯·å…ˆæ„å»ºç´¢å¼•ã€‚")
                else:
                    for sc, m, txt in hits:
                        st.write(f"**{m['side']}** | {m['title']} | ç›¸ä¼¼åº¦:{sc:.2f}")
                        st.code(txt, language="text")
            else:
                st.warning("è¯·é€‰æ‹©é¡¹ç›®å¹¶è¾“å…¥æŸ¥è¯¢å¥")

        with st.expander("ğŸ§ª ç´¢å¼•/è¯­æ–™å¥åº·æ£€æŸ¥", expanded=False):
            if proj_sel == "(è¯·é€‰æ‹©)":
                st.info("å…ˆåœ¨ä¸Šé¢é€‰æ‹©ä¸€ä¸ªé¡¹ç›®ã€‚")
            else:
                _pid = pid_map[proj_sel]
                # 1) è¯¥é¡¹ç›®è¯­æ–™æ¡æ•°
                try:
                    cnt = cur.execute("SELECT COUNT(*) FROM corpus WHERE project_id=?", (_pid,)).fetchone()[0]
                except Exception as e:
                    st.error(f"æŸ¥è¯¢ corpus å¤±è´¥:{e}")
                    cnt = None

                st.write(f"é¡¹ç›® {_pid} çš„è¯­æ–™æ¡æ•°:**{cnt}**")

                # 2) æ˜¯å¦å­˜åœ¨ç´¢å¼•æ–‡ä»¶ä¸æ˜ å°„æ¡æ•°
                try:
                    from pathlib import Path
                    import json
                    idx_dir = INDEX_DIR  # å³ Path(BASE_DIR) / ".cache_index"
                    f_map = idx_dir / f"vecmap_{_pid}.json"
                    f_faiss = idx_dir / f"faiss_{_pid}.bin"
                    f_npy = idx_dir / f"vectors_{_pid}.npy"

                    if f_map.exists():
                        data = json.loads(f_map.read_text(encoding="utf-8") or "[]")
                        st.write(f"æ˜ å°„æ–‡ä»¶:{f_map.name}ï¼ˆæ¡æ•°:**{len(data)}**ï¼‰")
                    else:
                        st.warning("æœªæ‰¾åˆ°æ˜ å°„æ–‡ä»¶ vecmap_*.json")

                    st.write(f"FAISS ç´¢å¼•æ–‡ä»¶å­˜åœ¨:{f_faiss.exists()}")
                    st.write(f"NPY å‘é‡æ–‡ä»¶å­˜åœ¨:{f_npy.exists()}")

                except Exception as e:
                    st.error(f"ç´¢å¼•æ–‡ä»¶æ£€æŸ¥å¤±è´¥:{e}")

# ========== Tab5:è®¾ç½® ==========
with tabs[4]:
    st.subheader("âš™ DeepSeek Key é…ç½®è¯´æ˜")
    st.markdown("""
åœ¨ `.streamlit/secrets.toml` ä¸­åŠ å…¥:
```
[deepseek]
api_key = "ä½ çš„KEY"
model = "deepseek-chat"
```
    """)
    ak, model = get_deepseek()
    if ak:
        st.success(f"å·²æ£€æµ‹åˆ° DeepSeek Key(æ¨¡å‹:{model})")
    else:
        st.warning("æœªæ£€æµ‹åˆ° DeepSeek Key")

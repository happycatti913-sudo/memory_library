# -*- coding: utf-8 -*-
"""
ä¸ªäººç¿»è¯‘çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ(ä¿®æ­£ç‰ˆ03)
- Tab1 ğŸ“‚ ç¿»è¯‘é¡¹ç›®ç®¡ç†:æ–°å»ºé¡¹ç›®ã€æ–‡ä»¶ä¸Šä¼ ã€æ‰§è¡Œç¿»è¯‘(DeepSeek API).å¯¼å‡ºå¯¹ç…§/åŸæ ¼å¼.å†™å…¥å†å²
- Tab2 ğŸ“˜ æœ¯è¯­åº“ç®¡ç†:æŸ¥è¯¢/ç¼–è¾‘/åˆ é™¤ã€CSVæ‰¹é‡å¯¼å…¥ã€ç»Ÿè®¡/å¯¼å‡ºã€å¿«é€Ÿæœç´¢ã€æ‰¹é‡æŒ‚æ¥é¡¹ç›®ã€å†å²æŠ½å–æœ¯è¯­ã€åˆ†ç±»ç®¡ç†
- Tab3 ğŸ“Š ç¿»è¯‘å†å²:æŸ¥çœ‹ã€ä¸‹è½½è¯‘æ–‡
- Tab4 ğŸ“š è¯­æ–™åº“ç®¡ç†:æ–°å¢/æ£€ç´¢/åˆå¹¶/Few-shot æ³¨å…¥
"""

# ==== stdlib ====
import os
import re
import io
import sys
import json
import uuid
import time
import sqlite3
import streamlit as st
import pandas as pd
from datetime import datetime
import altair as alt


# è®©åŒç›®å½•ä¸‹çš„ kb_dynamic.py å¯è¢«å¯¼å…¥(å¦‚æœå­˜åœ¨)
sys.path.append(os.path.dirname(__file__))

# ======== åŸºæœ¬è·¯å¾„è®¾ç½® ========
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(BASE_DIR, "kb.db")

PROJECT_DIR = os.path.join(BASE_DIR, "uploads")
os.makedirs(PROJECT_DIR, exist_ok=True)

# ç»Ÿä¸€çš„è¯­ä¹‰ç´¢å¼•æ ¹ç›®å½•: semantic_index/{project_id}/...
SEM_INDEX_ROOT = os.path.join(BASE_DIR, "semantic_index")
os.makedirs(SEM_INDEX_ROOT, exist_ok=True)

def _norm_domain_key(raw: str | None) -> str:
    """
    æŠŠæ•°æ®åº“é‡Œçš„ domain å­—æ®µè½¬æˆé€‚åˆä½œä¸ºæ–‡ä»¶å¤¹åçš„ keyï¼š
    - None/ç©º â†’ "æœªåˆ†ç±»"
    - å»æ‰é¦–å°¾ç©ºæ ¼
    - æ›¿æ¢æ‰ä¸é€‚åˆä½œä¸ºè·¯å¾„çš„å­—ç¬¦(Windows ä¸‹çš„ä¿ç•™å­—ç¬¦)
    """
    s = (raw or "").strip()
    if not s:
        s = "æœªåˆ†ç±»"
    for ch in r'\\/:"*?<>|':
        s = s.replace(ch, "_")
    return s


# ---------- è¯­ä¹‰ç´¢å¼•è·¯å¾„:æŒ‰â€œé¢†åŸŸ â†’ ç±»å‹â€å½’ç±» ----------
def _index_paths(project_id: int):
    """
    ç»Ÿä¸€çš„è¯­ä¹‰ç´¢å¼•è·¯å¾„(æŒ‰â€œé¢†åŸŸ/ç±»å‹â€å½’ç±»):

        BASE_DIR / semantic_index / {domain_key} / bilingual / index.faiss
                                                       / mapping.json
                                                       / vectors.npy

    ç›®å‰ä»…æ”¯æŒåŒè¯­å¯¹ç…§(bilingual)ç´¢å¼•;
    æœªæ¥å¦‚æœå¢åŠ ç¿»è¯‘ç­–ç•¥(strategy), å¯ä»¥åœ¨è¿™é‡Œæ‰©å±• kb_type å‚æ•°.
    """
    # å°è¯•æ ¹æ®é¡¹ç›®æ¨æ–­é¢†åŸŸ; æ‹¿ä¸åˆ°æ—¶å½’å…¥â€œæœªåˆ†ç±»â€
    domain_raw = None
    try:
        if "cur" in globals():
            row = cur.execute(
                "SELECT IFNULL(domain,'') FROM items WHERE id=?",
                (int(project_id),)
            ).fetchone()
            if row:
                domain_raw = (row[0] or "").strip()
    except Exception:
        domain_raw = None

    domain_key = _norm_domain_key(domain_raw)
    kb_type = "bilingual"

    base_dir = os.path.join(BASE_DIR, "semantic_index", domain_key, kb_type)
    os.makedirs(base_dir, exist_ok=True)

    idx_path = os.path.join(base_dir, "index.faiss")
    map_path = os.path.join(base_dir, "mapping.json")
    vec_path = os.path.join(base_dir, "vectors.npy")

    return idx_path, map_path, vec_path


def _project_domain(pid: int | None) -> str | None:
    """å®‰å…¨è·å–é¡¹ç›®çš„é¢†åŸŸæ ‡ç­¾ã€‚"""
    if not pid:
        return None
    try:
        row = cur.execute("SELECT IFNULL(domain,'') FROM items WHERE id=?", (int(pid),)).fetchone()
        dom = (row[0] if row else "").strip()
        return dom or None
    except Exception:
        return None


def dedup_terms_against_db(
    cur,
    terms: list[dict],
    project_id: int | None,
):
    """
    æŒ‰ (source_term, domain) å»é‡ï¼Œè¿‡æ»¤å·²å­˜åœ¨æˆ–æœ¬æ¬¡é‡å¤çš„æœ¯è¯­ã€‚

    - ä¸ term_ext ä¸­åŒä¸€é¡¹ç›®æˆ–å…¨å±€æœ¯è¯­é‡å¤æ—¶è·³è¿‡ã€‚
    - domain ä¸ºç©ºæ—¶æŒ‰ç©ºä¸²å‚ä¸å»é‡ï¼Œç¡®ä¿åŒæºæœ¯è¯­ä»…ä¿ç•™ä¸€æ¬¡ã€‚
    è¿”å› (filtered, skipped)ã€‚
    """

    if not terms:
        return [], []

    try:
        rows = cur.execute(
            """
            SELECT source_term, domain
            FROM term_ext
            WHERE project_id IS NULL OR project_id = ?
            """,
            (project_id if project_id is not None else -1,),
        ).fetchall()
    except Exception:
        rows = []

    existing = {
        ((s or "").strip().lower(), (d or "").strip().lower())
        for s, d in rows
        if (s or "").strip()
    }

    filtered, skipped = [], []
    for item in terms:
        src = (item.get("source_term") or "").strip()
        dom = (item.get("domain") or "").strip()
        if not src:
            skipped.append(item)
            continue
        key = (src.lower(), dom.lower())
        if key in existing:
            skipped.append(item)
            continue
        existing.add(key)
        filtered.append(item | {"source_term": src, "domain": dom})
    return filtered, skipped

# ======== è½»é‡æ—¥å¿—æœºåˆ¶ ========
LOG_DIR = os.path.join(BASE_DIR, "logs")
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, "app.log")


def log_event(level: str, message: str, **extra):
    """
    è½»é‡æ—¥å¿—è®°å½•:
        level  : "INFO" / "WARNING" / "ERROR"
        message: ç®€çŸ­æè¿°
        extra  : å¯é€‰çš„ç»“æ„åŒ–å­—æ®µ, ä¼šä¸€èµ·å†™å…¥ JSON
    å†™å…¥è·¯å¾„: BASE_DIR/logs/app.log
    """
    try:
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        record = {
            "time": ts,
            "level": (level or "INFO").upper(),
            "message": str(message),
        }
        if extra:
            record["extra"] = extra
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
    except Exception:
        # æ—¥å¿—æœ¬èº«æ°¸è¿œä¸èƒ½ç‚¸åº”ç”¨ï¼Œé™é»˜å¤±è´¥
        pass

# ==== third-party ====
try:
    from docx import Document  # åœ¨éœ€è¦å¤„ä»ä¼š try/except
except Exception:
    Document = None

# ========== é¡µé¢è®¾ç½® ==========
st.set_page_config(page_title="ä¸ªäººç¿»è¯‘çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ3.0", layout="wide")

# ========== kb_dynamic (å¯é€‰) ==========
KBEmbedder = None
recommend_for_segment = None
build_prompt_strict = None
build_prompt_soft = None
try:
    from kb_dynamic import (
        KBEmbedder as _KBEmbedder,
        recommend_for_segment as _recommend_for_segment,
        build_prompt_strict as _build_prompt_strict,
        build_prompt_soft as _build_prompt_soft,
    )
    KBEmbedder = _KBEmbedder
    recommend_for_segment = _recommend_for_segment
    build_prompt_strict = _build_prompt_strict
    build_prompt_soft = _build_prompt_soft
except Exception:
    pass  # å…è®¸ç¼ºå¤±;åŠ¨æ€æœ¯è¯­æ¨èåŠŸèƒ½å°†è‡ªåŠ¨é™çº§

# ========== å·¥å…·å‡½æ•° ==========
def make_sk(prefix: str):
    """è¿”å›ä¸€ä¸ªå¸¦æœ‰å‰ç¼€çš„ key ç”Ÿæˆå™¨"""
    return lambda name, id=None: f"{prefix}_{name}_{id}" if id else f"{prefix}_{name}"
# å…¨å±€é»˜è®¤ key ç”Ÿæˆå™¨(æ›¿ä»£è¢«åˆ é™¤çš„è®¡æ•°å™¨ç‰ˆ sk)
sk = make_sk("global")

def render_table(df, *, key=None, hide_index=True, editable=False):
    """
    ç»Ÿä¸€æ¸²æŸ“è¡¨æ ¼(å¯¹æ—§/æ–° Streamlit éƒ½å®‰å…¨):
    - editable=False: åªè¯»(ç”¨ data_editor disabled=True ä»¥ä¿ç•™ key)
    - editable=True : å¯ç¼–è¾‘
    - ä¸å†ä¼  width å‚æ•°.é¿å… 'str' as int çš„æŠ¥é”™
    """
    try:
        if editable:
            return st.data_editor(
                df,
                hide_index=hide_index,
                key=key,
            )
        if key is not None:
            return st.data_editor(
                df,
                hide_index=hide_index,
                disabled=True,
                key=key,
            )
        return st.dataframe(df, hide_index=hide_index)
    except TypeError:
        return st.data_editor(
            df,
            hide_index=hide_index,
            disabled=not editable,
            key=key,
        )
# ========== æ–‡æœ¬é«˜äº®å‡½æ•° ==========
def highlight_terms(text: str, term_pairs: list):
    """é«˜äº®æœ¯è¯­ï¼Œterm_pairs = [(src, tgt), ..]"""
    if not term_pairs:
        return text

    import re
    safe = text

    for s, t in term_pairs:
        if not s:
            continue
        # å¯¹ source term åšé«˜äº®ï¼ˆé»„è‰²ï¼‰
        safe = re.sub(
            re.escape(s),
            fr"<span style='background: #fff3b0'>{s}</span>",
            safe,
            flags=re.IGNORECASE
        )
        # å¯¹ target term ä¹Ÿé«˜äº®ï¼ˆæ·¡ç»¿ï¼‰
        if t:
            safe = re.sub(
                re.escape(t),
                fr"<span style='background: #d4f6d4'>{t}</span>",
                safe,
                flags=re.IGNORECASE
            )

    return safe

def render_index_manager(st, conn, cur):
    """
    ğŸ§  ç»Ÿä¸€çš„ç´¢å¼•ç®¡ç†é¡µé¢:
      - æŒ‰é¡¹ç›®æŸ¥çœ‹: è¯­æ–™æ¡æ•° / å…¶ä¸­æ¥è‡ªå†å²çš„æ¡æ•° / ç´¢å¼•æ¡æ•°æ‹†åˆ†
      - ä¸€é”®é‡å»ºå½“å‰é¡¹ç›®ç´¢å¼•
      - (å¯é€‰) æ‰¹é‡é‡å»º
    """
    st.title("ğŸ§  è¯­ä¹‰ç´¢å¼•ç®¡ç†")

    # === 1. é¡¹ç›®åˆ—è¡¨ + åŸºæœ¬ç»Ÿè®¡ ===
    st.markdown("#### é¡¹ç›®æ¦‚è§ˆ")

    rows = cur.execute(
        """
        SELECT i.id,
               IFNULL(i.title,''), 
               IFNULL(i.domain,''),
               COUNT(DISTINCT c.id)                                           AS corpus_cnt,
               SUM(CASE WHEN c.note LIKE 'from trans_ext%%' THEN 1 ELSE 0 END) AS hist_cnt
        FROM items i
        LEFT JOIN corpus c ON c.project_id = i.id
        GROUP BY i.id, i.title, i.domain
        ORDER BY i.id DESC
        LIMIT 500;
        """
    ).fetchall()

    if not rows:
        st.info("å½“å‰è¿˜æ²¡æœ‰ä»»ä½•é¡¹ç›®æˆ–è¯­æ–™ã€‚è¯·å…ˆåœ¨é¡¹ç›®ç®¡ç†/è¯­æ–™åº“ä¸­æ·»åŠ å†…å®¹ã€‚")
        return

    import pandas as pd

    df_proj = pd.DataFrame(
        [
            {
                "é¡¹ç›®ID": r[0],
                "é¡¹ç›®åç§°": r[1],
                "é¢†åŸŸ": r[2],
                "è¯­æ–™æ¡æ•°": r[3],
                "å…¶ä¸­æ¥è‡ªç¿»è¯‘å†å²": r[4] or 0,
            }
            for r in rows
        ]
    )

    st.dataframe(df_proj, use_container_width=True)

    # === 2. é€‰æ‹©ä¸€ä¸ªé¡¹ç›®ï¼ŒæŸ¥çœ‹ç´¢å¼•çŠ¶æ€ ===
    st.markdown("#### å•é¡¹ç›®ç´¢å¼•çŠ¶æ€ & æ“ä½œ")

    proj_options = {f"[{r[0]}] {r[1] or '(æœªå‘½å)'}": r[0] for r in rows}
    proj_label = st.selectbox(
        "é€‰æ‹©è¦æŸ¥çœ‹/é‡å»ºç´¢å¼•çš„é¡¹ç›®",
        ["(è¯·é€‰æ‹©)"] + list(proj_options.keys()),
    )
    pid_sel = proj_options.get(proj_label)

    if not pid_sel:
        st.info("è¯·é€‰æ‹©ä¸€ä¸ªé¡¹ç›®ä»¥æŸ¥çœ‹ç´¢å¼•è¯¦æƒ…ã€‚")
        return

    # 2.1 ç»Ÿè®¡å½“å‰ç´¢å¼•ä¸­çš„ source=corpus / history æ•°é‡
    idx_total = idx_corpus = idx_hist = idx_other = 0
    try:
        mode, index_obj, mapping, vecs = _load_index(int(pid_sel))
        if isinstance(mapping, list):
            idx_total = len(mapping)
            for m in mapping:
                src_tag = (m.get("source") or "").lower()
                if src_tag == "history":
                    idx_hist += 1
                elif src_tag in ("", "corpus"):
                    idx_corpus += 1
                else:
                    idx_other += 1
    except Exception as e:
        st.warning(f"è¯»å–ç´¢å¼•æ–‡ä»¶å¤±è´¥: {e}")

    st.write(
        f"- å½“å‰ç´¢å¼•æ¡æ•°: **{idx_total}** æ¡\n"
        f"- å…¶ä¸­æ¥è‡ªè¯­æ–™åº“(corpus): **{idx_corpus}** æ¡\n"
        f"- å…¶ä¸­æ¥è‡ªç¿»è¯‘å†å²(history): **{idx_hist}** æ¡\n"
        f"- å…¶ä»–/æœªçŸ¥æ¥æº: **{idx_other}** æ¡"
    )

    # 2.2 å½“å‰é¡¹ç›®åœ¨ DB ä¸­çš„è¯­æ–™ç»Ÿè®¡ï¼Œå’Œä¸Šé¢çš„ df_proj å¯¹åº”
    row_sel = [r for r in rows if r[0] == pid_sel][0]
    st.write(
        f"- æ•°æ®åº“ä¸­è¯­æ–™æ¡æ•°: **{row_sel[3]}** æ¡ "
        f"(å…¶ä¸­æ¥è‡ªç¿»è¯‘å†å²: **{row_sel[4] or 0}** æ¡)"
    )

    # === 3. æ“ä½œåŒº: é‡å»ºç´¢å¼• / æ‰¹é‡é‡å»º ===
    c1, c2 = st.columns(2)

    with c1:
        if st.button("ğŸ” é‡å»ºå½“å‰é¡¹ç›®ç´¢å¼•", type="primary", key=f"rebuild_idx_{pid_sel}"):
            res = rebuild_project_semantic_index(pid_sel)
            if res.get("ok"):
                st.success(
                    f"ç´¢å¼•å·²é‡å»º: æ–°å¢ {res['added']} æ¡, æ€»é‡ {res['total']} æ¡ã€‚"
                )
            else:
                st.error(f"é‡å»ºå¤±è´¥: {res.get('msg','æœªçŸ¥é”™è¯¯')}")

    with c2:
        if st.button("âš  æ‰¹é‡é‡å»ºä¸Šè¡¨åˆ—å‡ºçš„å…¨éƒ¨é¡¹ç›®ç´¢å¼•", key="rebuild_all_idx"):
            ok_cnt = fail_cnt = 0
            for r in rows:
                pid = r[0]
                res = rebuild_project_semantic_index(pid)
                if res.get("ok"):
                    ok_cnt += 1
                else:
                    fail_cnt += 1
            st.success(f"æ‰¹é‡é‡å»ºå®Œæˆ: æˆåŠŸ {ok_cnt} ä¸ªé¡¹ç›®, å¤±è´¥ {fail_cnt} ä¸ªé¡¹ç›®ã€‚")
# ======= è·å–æŸæ¡å†å²è®°å½•å¯¹åº”çš„åŸæ–‡(ä¼˜å…ˆ items.body.å…œåº• src_path ä»…ä½œä¸ºæ ‡é¢˜æç¤º)=======
def get_terms_for_project(cur, pid: int, use_dynamic: bool = True):
    """
    ç»Ÿä¸€æœ¯è¯­åŠ è½½æ¥å£ï¼ˆæ ¸å¿ƒä¸»å¹²ä¹‹ä¸€ï¼‰

    å‚æ•°:
        cur        : SQLite cursor
        pid        : å½“å‰é¡¹ç›® ID
        use_dynamic: æ˜¯å¦åŒ…å«â€œåŠ¨æ€æœ¯è¯­â€ï¼ˆå…¶ä»–é¡¹ç›®æˆ–å…¨å±€æœ¯è¯­ï¼‰

    è¿”å›:
        term_map, term_meta

        term_map : dict[str, str]
            {æºæœ¯è¯­: ç›®æ ‡æœ¯è¯­}ï¼Œç”¨äº prompt æ³¨å…¥ / ä¸€è‡´æ€§æ£€æŸ¥ç­‰ã€‚

        term_meta: list[dict]
            æ¯æ¡æœ¯è¯­çš„å…ƒä¿¡æ¯ï¼Œä¾‹å¦‚:
            {
                "source_term": "...",
                "target_term": "...",
                "domain": "...",
                "origin": "static" | "dynamic",
                "project_id": 123   # æœ¯è¯­æ‰€å±é¡¹ç›®ID, static æ—¶=å½“å‰é¡¹ç›®
            }

    è¯´æ˜:
        - é™æ€æœ¯è¯­: term_ext.project_id = å½“å‰é¡¹ç›® pid
        - åŠ¨æ€æœ¯è¯­: term_ext.project_id IS NULL æˆ– <> pid
        - å»é‡è§„åˆ™: ä¸åŒºåˆ†å¤§å°å†™, (source_term, domain) ç›¸åŒåªä¿ç•™ä¸€æ¡
    """

    # 1) é™æ€æœ¯è¯­ï¼ˆæŒ‚æ¥åˆ°å½“å‰é¡¹ç›®çš„æœ¯è¯­ï¼‰
    rows_static = cur.execute(
        """
        SELECT source_term, target_term, domain
        FROM term_ext
        WHERE project_id = ?
        """,
        (pid,),
    ).fetchall()

    # 2) åŠ¨æ€æœ¯è¯­ï¼ˆå…¶ä»–é¡¹ç›® / å…¨å±€æœ¯è¯­ï¼‰
    if use_dynamic:
        rows_dynamic = cur.execute(
            """
            SELECT source_term, target_term, domain, project_id
            FROM term_ext
            WHERE project_id IS NULL OR project_id <> ?
            """,
            (pid,),
        ).fetchall()
    else:
        rows_dynamic = []

    # 3) å»é‡ï¼škey = (lower(source), lower(domain))
    #    value = (source_term, target_term, domain, origin, term_project_id)
    dedup: dict[tuple[str, str], tuple[str, str, str, str, int | None]] = {}

    # é™æ€
    for s, t, d in rows_static:
        if not s:
            continue
        s_raw = (s or "").strip()
        t_raw = (t or "").strip()
        d_raw = (d or "").strip()
        key = (s_raw.lower(), d_raw.lower())
        if key not in dedup:
            dedup[key] = (s_raw, t_raw, d_raw, "static", int(pid))

    # åŠ¨æ€
    for row in rows_dynamic:
        # row ç»“æ„: (source_term, target_term, domain, project_id)
        if len(row) == 4:
            s, t, d, pid_term = row
        else:
            # å…¼å®¹æ„å¤–æƒ…å†µ
            s, t, d = row[0], row[1], row[2]
            pid_term = None
        if not s:
            continue
        s_raw = (s or "").strip()
        t_raw = (t or "").strip()
        d_raw = (d or "").strip()
        key = (s_raw.lower(), d_raw.lower())
        if key not in dedup:
            dedup[key] = (s_raw, t_raw, d_raw, "dynamic", pid_term if pid_term is not None else None)

    # 4) æ‹¼å‡º term_map å’Œ term_meta
    term_map: dict[str, str] = {}
    term_meta: list[dict] = []

    for (_s_lc, _d_lc), (s_raw, t_raw, d_raw, origin, pid_term) in dedup.items():
        if not s_raw:
            continue
        term_map[s_raw] = t_raw
        term_meta.append(
            {
                "source_term": s_raw,
                "target_term": t_raw,
                "domain": d_raw,
                "origin": origin,
                "project_id": pid_term,
            }
        )

    return term_map, term_meta

# ======= è½»é‡æœ¯è¯­å€™é€‰(ä¸­è‹±éƒ½å¯;ä½ åç»­å¯æ¢æˆ DeepSeek æŠ½å–)=======

def register_project_file(cur, conn, project_id, file_name, data_bytes):
    """
    å°†ä¸Šä¼ çš„æ–‡ä»¶ä¿å­˜åˆ°é¡¹ç›®ç›®å½•ï¼Œå¹¶è®°å½•åœ¨ project_files è¡¨ä¸­ã€‚
    """
    if not project_id or not data_bytes:
        return None
    safe_name = os.path.basename(file_name) or f"project_{project_id}_file"
    proj_dir = os.path.join(PROJECT_DIR, f"project_{project_id}")
    os.makedirs(proj_dir, exist_ok=True)
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    uniq_name = f"{stamp}_{uuid.uuid4().hex[:6]}_{safe_name}"
    full_path = os.path.join(proj_dir, uniq_name)
    with open(full_path, "wb") as f:
        f.write(data_bytes)
    cur.execute(
        """
        INSERT INTO project_files (project_id, file_path, file_name)
        VALUES (?, ?, ?)
        """,
        (project_id, full_path, safe_name),
    )
    conn.commit()
    return full_path


def fetch_project_files(cur, project_id):
    if not project_id:
        return []
    rows = cur.execute(
        """
        SELECT id, IFNULL(file_name,''), IFNULL(file_path,''), IFNULL(uploaded_at,'')
        FROM project_files
        WHERE project_id=?
        ORDER BY id DESC
        """,
        (project_id,),
    ).fetchall()
    items = []
    for fid, name, path, uploaded in rows:
        if not path:
            continue
        display = name or os.path.basename(path) or f"file_{fid}"
        items.append(
            {
                "id": fid,
                "name": display,
                "path": path,
                "uploaded_at": uploaded,
            }
        )
    return items


def ensure_legacy_file_record(cur, conn, project_id, legacy_path):
    """
    æ—§ç‰ˆä»…æ”¯æŒå•æ–‡ä»¶ï¼Œè‹¥æ£€æµ‹åˆ° item_ext.src_pathï¼Œè‡ªåŠ¨åŒæ­¥åˆ° project_filesã€‚
    """
    if not (project_id and legacy_path):
        return
    exists = cur.execute(
        "SELECT 1 FROM project_files WHERE project_id=? AND file_path=?",
        (project_id, legacy_path),
    ).fetchone()
    if exists:
        return
    cur.execute(
        """
        INSERT INTO project_files (project_id, file_path, file_name)
        VALUES (?, ?, ?)
        """,
        (project_id, legacy_path, os.path.basename(legacy_path) or None),
    )
    conn.commit()


def remove_project_file(cur, conn, file_id):
    row = cur.execute("SELECT file_path FROM project_files WHERE id=?", (file_id,)).fetchone()
    if row:
        (path,) = row
        if path and os.path.exists(path):
            try:
                os.remove(path)
            except Exception:
                pass
    cur.execute("DELETE FROM project_files WHERE id=?", (file_id,))
    conn.commit()


def cleanup_project_files(cur, conn, project_id):
    rows = cur.execute("SELECT file_path FROM project_files WHERE project_id=?", (project_id,)).fetchall()
    for (fp,) in rows:
        if fp and os.path.exists(fp):
            try:
                os.remove(fp)
            except Exception:
                pass
    cur.execute("DELETE FROM project_files WHERE project_id=?", (project_id,))
    conn.commit()


def _ensure_project_ref_map():
    """
    ç¡®ä¿ session_state['corpus_refs'] ä¸º {project_id: set(ids)} ç»“æ„ã€‚
    """
    refs = st.session_state.get("corpus_refs")
    if isinstance(refs, dict):
        return refs
    st.session_state["corpus_refs"] = {}
    return st.session_state["corpus_refs"]


def _ensure_project_switch_map():
    """
    ç¡®ä¿ session_state['cor_use_ref'] ä¸º {project_id: bool} ç»“æ„ã€‚
    """
    switches = st.session_state.get("cor_use_ref")
    if isinstance(switches, dict):
        return switches
    st.session_state["cor_use_ref"] = {}
    return st.session_state["cor_use_ref"]


def get_project_ref_ids(project_id: int | None) -> set[int]:
    if not project_id:
        return set()
    ref_map = _ensure_project_ref_map()
    ref_map.setdefault(project_id, set())
    return ref_map[project_id]


def get_project_fewshot_enabled(project_id: int | None) -> bool:
    if not project_id:
        return False
    switch_map = _ensure_project_switch_map()
    return bool(switch_map.get(project_id, False))


def set_project_fewshot_enabled(project_id: int | None, value: bool):
    if not project_id:
        return
    switch_map = _ensure_project_switch_map()
    switch_map[project_id] = bool(value)


def get_project_fewshot_examples(
    cur,
    project_id: int | None,
    *,
    limit: int | None = 5,
    require_enabled: bool = True,
):
    if not project_id:
        return []
    if require_enabled and not get_project_fewshot_enabled(project_id):
        return []

    ref_ids = list(get_project_ref_ids(project_id))
    if not ref_ids:
        return []

    ref_ids = sorted({int(rid) for rid in ref_ids if str(rid).isdigit()}, reverse=True)
    if limit is not None and len(ref_ids) > limit:
        ref_ids = ref_ids[:limit]

    qmarks = ",".join(["?"] * len(ref_ids))
    rows = cur.execute(
        f"SELECT id, title, IFNULL(src_text,''), IFNULL(tgt_text,'') FROM corpus WHERE id IN ({qmarks})",
        ref_ids,
    ).fetchall()
    order_map = {rid: idx for idx, rid in enumerate(ref_ids)}
    rows.sort(key=lambda r: order_map.get(r[0], len(order_map)))

    examples = []
    for rid, title, src, tgt in rows:
        src_norm = (src or "").strip()
        tgt_norm = (tgt or "").strip()
        if not (src_norm and tgt_norm):
            continue
        examples.append(
            {
                "id": rid,
                "title": title or f"ç¤ºä¾‹#{rid}",
                "src": src_norm,
                "tgt": tgt_norm,
            }
        )
    return examples
def run_project_translation_ui(
    pid,
    project_title,
    src_path,
    conn,
    cur
):
    """
    æ‰§è¡Œç¿»è¯‘æ•´ä¸ª UI + é€»è¾‘ã€‚ä¸æ”¹é€»è¾‘ï¼Œåªæ˜¯æŠŠåŸæ¥ Tab1 é‡Œçš„å¤§å—æ¬è¿›æ¥ã€‚
    å‚æ•°å«ä¹‰ï¼š
        pid: å½“å‰é¡¹ç›® ID
        project_title: é¡¹ç›®æ ‡é¢˜
        src_path: æºæ–‡ä»¶è·¯å¾„æˆ–æ–‡æœ¬å†…å®¹
        conn, cur: æ•°æ®åº“å¥æŸ„
    """

    st.subheader(f"ğŸ“˜ é¡¹ç›®ï¼š{project_title}")
    st.info("æ‰“å·¥ä¸æ˜“ï¼Œç‰›é©¬å“­æ³£ã€‚")

    # å…ˆç»Ÿä¸€åŠ è½½æœ¯è¯­ï¼ˆé™æ€ + åŠ¨æ€ï¼‰
    term_map, term_meta = get_terms_for_project(cur, pid, use_dynamic=True)
    proj_terms_all = term_map  # ç»™ _detect_hits ç”¨

    # 1) ç»“æœç¼“å­˜åˆå§‹åŒ–(ç»Ÿä¸€ç”¨ session_state)
    if "all_results" not in st.session_state:
        st.session_state["all_results"] = []
    st.session_state["all_results"].clear()

    # 2) ç¯å¢ƒæ£€æŸ¥
    ak, model = get_deepseek()
    if not ak:
        st.error("æœªæ£€æµ‹åˆ° DeepSeek Key.è¯·åœ¨ `.streamlit/secrets.toml` é…ç½® [deepseek]")
        st.stop()
    if not selected_src_path or not os.path.exists(selected_src_path):
        st.error("ç¼ºå°‘æºæ–‡ä»¶")
        st.stop()

    # 3) è¯»å–ä¸åˆ†æ®µ
    src_text = read_source_file(selected_src_path)
    st.code(repr(src_text[:400]))                     # çœ‹å­—ç¬¦ä¸²é‡Œæœ‰æ²¡æœ‰ '\n'
    st.write({"len": len(src_text), "nl": src_text.count("\n"), "cr": src_text.count("\r")})
    st.write({"preview_lines": src_text.splitlines()[:3]})
    
    # ç”¨ç»Ÿä¸€çš„ split_paragraphs åšåˆ‡åˆ†
    blocks = split_paragraphs(src_text)
    if not blocks:
        st.error("æºæ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œæˆ–æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ®µè½")
        st.stop()

    st.info(f"æŒ‰æ®µè½åˆ‡åˆ†ï¼Œå…± {len(blocks)} æ®µï¼Œå¼€å§‹ç¿»è¯‘â€¦")

    lang_pair_val = st.session_state.get(f"lang_{pid}", "ä¸­è¯‘è‹±")
    use_semantic  = bool(st.session_state.get(f"use_sem_{pid}", True))
    scope_val     = st.session_state.get(f"scope_{pid}", st.session_state.get("scope_newproj", "project"))

    # 4) å¾ªç¯ç¿»è¯‘ï¼ˆç»Ÿä¸€èµ° translate_block_with_kb ç®¡çº¿ï¼‰
    # å…ˆåŠ è½½ few-shot ç¤ºä¾‹ï¼ˆé¡¹ç›®çº§ï¼‰
    fewshot_examples = get_project_fewshot_examples(cur, pid, limit=5)
    if fewshot_examples:
        with st.expander("ğŸ“Œ Few-shot å‚è€ƒç¤ºä¾‹(é¡¹ç›®çº§æ³¨å…¥)", expanded=False):
            for ex in fewshot_examples:
                st.markdown(
                    f"**{ex['title']}**\n\næºæ–‡:\n{ex['src']}\n\nè¯‘æ–‡:\n{ex['tgt']}\n---"
                )

    # ä¸ºæ¯æ¬¡ç¿»è¯‘å‡†å¤‡ä¸€ä¸ªç»“æœåˆ—è¡¨
    if "all_results" not in st.session_state:
        st.session_state["all_results"] = []
    st.session_state["all_results"].clear()

    # DeepSeek key / model åªå–ä¸€æ¬¡
    ak, model = get_deepseek()
    if not ak:
        st.error("æœªæ£€æµ‹åˆ° DeepSeek Key.è¯·åœ¨ `.streamlit/secrets.toml` é…ç½® [deepseek]")
        st.stop()

    # æŒ‰æ®µå¾ªç¯ç¿»è¯‘
    for i, blk in enumerate(blocks, start=1):
        blk = str(blk or "").strip()
        if not blk:
            continue

        # â€”â€” è°ƒç”¨ç»Ÿä¸€ç®¡çº¿å®Œæˆâ€œæœ¯è¯­ + å‚è€ƒ + DeepSeekâ€ â€”â€” 
        out_text = res["tgt"]
        term_map_all = res["term_map_all"]
        terms_in_block = res.get("terms_in_block", {})
        terms_corpus = res.get("terms_corpus_dyn", {})
        terms_final = res.get("terms_final", {})
        ref_context = res["ref_context"]
        violated = res["violated_terms"]

        # â€”â€” å±•ç¤ºæœ¯è¯­ + å‚è€ƒä¾‹å¥ï¼ˆæŠ˜å ï¼Œå¯é€‰ï¼‰â€”â€”
        with st.expander(f"ç¬¬ {i} æ®µ Â· æœ¯è¯­ä¸å‚è€ƒï¼ˆå¯é€‰å±•å¼€ï¼‰", expanded=False):
            if term_map_all:
                df_all = pd.DataFrame(
                    [
                        {
                            "æœ¯è¯­": s,
                            "è¯‘æ–‡": t,
                            "å‘½ä¸­æœ¬æ®µ": s in terms_in_block,
                            "å‘½ä¸­å‚è€ƒä¾‹å¥": s in terms_corpus,
                            "æœ€ç»ˆæ³¨å…¥Prompt": s in terms_final,
                        }
                        for s, t in term_map_all.items()
                    ]
                )
                st.dataframe(df_all, width='stretch')

                # å•ç‹¬åˆ—å‡ºâ€œè¯­æ–™é©±åŠ¨æœ¯è¯­â€ï¼ˆæ–¹ä¾¿ä½ çœ‹ï¼‰
                corpus_only = [
                    {"æœ¯è¯­": s, "è¯‘æ–‡": t}
                    for s, t in terms_corpus.items()
                ]
                if corpus_only:
                    st.markdown("**è¯­æ–™é©±åŠ¨æœ¯è¯­ï¼ˆä»…åœ¨å‚è€ƒä¾‹å¥ä¸­å‘½ä¸­çš„æœ¯è¯­ï¼‰ï¼š**")
                    st.dataframe(pd.DataFrame(corpus_only), width='stretch')

            if ref_context:
                st.text(ref_context[:1500])

        # è®°å½•ç»“æœ(ç»Ÿä¸€ç”¨ session_state)
        st.session_state["all_results"].append(out_text)
        st.write(f"âœ… ç¬¬ {i} æ®µå®Œæˆ")

        # è¯‘åä¸€è‡´æ€§æé†’
        if violated:
            st.warning("ä»¥ä¸‹æœ¯è¯­æœªåœ¨è¯‘æ–‡ä¸­å‡ºç°(å»ºè®®äººå·¥æ ¸å¯¹): " + "ï¼›".join(violated))


    # ===== å¾ªç¯ç»“æŸå:æ±‡æ€»è¾“å‡º =====

    # ç»“æœä¸æºæ®µè½å…œåº•ä¸å¯¹é½
    all_results_safe = list(st.session_state.get("all_results", []))
    blocks_src_safe  = list(blocks if 'blocks' in locals() else [])
    if len(blocks_src_safe) != len(all_results_safe):
        n = min(len(blocks_src_safe), len(all_results_safe))
        blocks_src_safe  = blocks_src_safe[:n]
        all_results_safe = all_results_safe[:n]

    # æ–‡æœ¬æ±‡æ€»
    final_text = "\n\n".join(all_results_safe)

    # ä¸€è‡´æ€§æŠ¥å‘Š(è¯­ä¹‰+æœ¯è¯­)
    try:
        term_map_report, _ = get_terms_for_project(cur, pid, use_dynamic=True)
        df_rep = semantic_consistency_report(
            project_id=pid,
            blocks_src=blocks_src_safe,
            blocks_tgt=all_results_safe,
            term_map=term_map_report,
            topk=3,
            thr=0.70
        )
        st.markdown("### ğŸ” è¯‘åä¸€è‡´æ€§æŠ¥å‘Š(è¯­ä¹‰+æœ¯è¯­)")
        st.dataframe(df_rep, width='stretch')
    except Exception as e:
        st.caption(f"(ä¸€è‡´æ€§æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e})")

    # ä¸‹è½½æŒ‰é’®(TXT)
    proj_title = (title if 'title' in locals() and title else f"project_{pid}")
    st.download_button(
        "â¬‡ï¸ ä¸‹è½½ç¿»è¯‘ç»“æœ (TXT)",
        final_text or "",
        file_name=f"{proj_title}_ç¿»è¯‘ç»“æœ.txt",
        mime="text/plain",
        key=f"dl_txt_{pid}"
    )

    # å†™å…¥å†å² trans_ext
    try:
        # å…¼å®¹å–å€¼(éƒ½åšäº†å…œåº•.é˜²æ­¢æœªå®šä¹‰)
        src_path = selected_src_path if ('selected_src_path' in locals() and selected_src_path) else None
        mode_val = mode if ('mode' in locals() and mode) else "æ ‡å‡†æ¨¡å¼"
        lang_pair_val = st.session_state.get(f"lang_{pid}", "ä¸­è¯‘è‹±")

        # è®¡ç®—æ®µæ•°:å»ºè®®ä»¥ã€Œæºæ–‡æœ¬ã€ç»Ÿè®¡ï¼›è‹¥æ— åˆ™é€€å›æœ€ç»ˆè¯‘æ–‡
        src_for_seg = src_text if ('src_text' in locals() and src_text) else final_text
        seg_count = len(split_sents(src_for_seg, lang_hint="auto"))

        cur.execute("""
            INSERT INTO trans_ext (
                project_id, src_path, lang_pair, mode, output_text,
                stats_json, segments, term_hit_total, created_at
            )
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))
        """, (
            pid,             # é¡¹ç›®ID
            src_path,        # æºæ–‡ä»¶è·¯å¾„(å¯ä¸ºç©º)
            lang_pair_val,   # è¯­å¯¹
            mode_val,        # æ¨¡å¼
            final_text,      # è¾“å‡ºæ–‡æœ¬(æœ€ç»ˆè¯‘æ–‡)
            None,            # ç»Ÿè®¡JSON(å ä½)
            seg_count,       # æ®µæ•°(ä¿®å¤:ä¸ç”¨ blocks_src_safe)
            None             # æœ¯è¯­å‘½ä¸­æ•°(å ä½.å¯åç»­å¡«çœŸå®å€¼)
        ))
        conn.commit()
        st.success("ğŸ“ å·²å†™å…¥ç¿»è¯‘å†å²")
    except Exception as e:
        st.warning(f"å†™å…¥ç¿»è¯‘å†å²å¤±è´¥: {e}")
            

# ======= å¯¹é½å¹¶å¯¼å‡º(ä¾èµ–ä½ å·²æœ‰çš„ split_blocks / align_export)=======
def quick_diagnose_vectors(pid: int):
    """
    æ‰“å°/æç¤ºé¡¹ç›®å‘é‡ç´¢å¼•çŠ¶æ€.å¸®åŠ©æ’æŸ¥â€œæ£€ç´¢ä¸ºç©º/ç»´åº¦ä¸åŒ¹é…/æœªå»ºç´¢å¼•â€ç­‰é—®é¢˜ã€‚
    """
    try:
        mode, index, mapping, vecs = _load_index(pid)
        if mode == "none":
            # æç¤ºä¸€ä¸‹å½“å‰ç´¢å¼•åº”å½“æ‰€åœ¨çš„é¢†åŸŸ/è·¯å¾„
            dom = None
            try:
                if "cur" in globals():
                    dom = _get_domain_for_proj(cur, int(pid))  # type: ignore[name-defined]
            except Exception:
                dom = None
            dom_key = _norm_domain_key(dom)
            st.warning(
                f"é¡¹ç›® {pid} å°šæœªå»ºç«‹å‘é‡ç´¢å¼•(semantic_index/{dom_key}/bilingual ä¸‹æ— ç´¢å¼•æ–‡ä»¶)ã€‚"
            )
            return
        msg = f"ç´¢å¼•æ¨¡å¼: {mode}; æ˜ å°„æ¡æ•°: {len(mapping)}"

        if mode == "faiss" and index is not None:
            msg += f"; FAISS ntotal: {index.ntotal}"
        elif vecs is not None:
            msg += f"; NPY shape: {getattr(vecs, 'shape', None)}"
        st.info(msg)

        # æŠ½æ ·éªŒè¯æ˜ å°„çš„ corpus_id æ˜¯å¦éƒ½èƒ½å›æŸ¥åˆ°æ–‡æœ¬
        bad = 0
        for m in mapping[:10]:
            cid = int(m.get("corpus_id") or -1)
            row = cur.execute("SELECT id FROM corpus WHERE id=?", (cid,)).fetchone()
            if not row:
                bad += 1
        if bad:
            st.warning(f"æ˜ å°„ä¸­æœ‰ {bad} æ¡ corpus_id æ— æ³•å›æŸ¥.è¯·è€ƒè™‘é‡å»ºç´¢å¼•ã€‚")
    except Exception as e:
        st.error(f"å‘é‡è¯Šæ–­å¼‚å¸¸:{e}")

# ---------- æ–‡ä»¶è¯»å–å·¥å…· ----------
def _lazy_docx():
    try:
        import docx  # python-docx
        return docx
    except Exception:
        return None

def _normalize(t: str) -> str:
    if not t: return ""
    t = t.replace("\xa0", " ").replace("\u200b", "")
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{2,}", "\n", t)
    return t.strip()

def read_docx_tables_info(file_like):
    docx = _lazy_docx()
    if not docx: return {}
    try:
        doc = docx.Document(file_like)
    except Exception:
        return {}
    info = {}
    for ti, tbl in enumerate(doc.tables):
        rows = len(tbl.rows)
        cols = len(tbl.columns) if rows else 0
        prev = []
        for r in tbl.rows[: min(6, rows)]:
            prev.append(tuple(_normalize(c.text) for c in r.cells))
        info[ti] = {"rows": rows, "cols": cols, "preview": prev}
    return info

def extract_pairs_from_docx_table(file_like, table_index=0, src_col=0, tgt_col=1,
                                  ffill=True, drop_empty_both=True, dedup=True):
    docx = _lazy_docx()
    if not docx: return []
    try:
        doc = docx.Document(file_like)
    except Exception:
        return []
    if table_index >= len(doc.tables): return []
    tbl = doc.tables[table_index]
    rows = []
    for r in tbl.rows:
        rows.append([_normalize(c.text) for c in r.cells])
    if not rows: return []
    max_cols = max(len(r) for r in rows)
    if src_col >= max_cols or tgt_col >= max_cols: return []

    if ffill:
        for col in (src_col, tgt_col):
            last = ""
            for i in range(len(rows)):
                val = rows[i][col] if col < len(rows[i]) else ""
                if val: last = val
                else: rows[i][col] = last

    pairs = []
    for r in rows:
        s = r[src_col] if src_col < len(r) else ""
        t = r[tgt_col] if tgt_col < len(r) else ""
        s, t = s.strip(), t.strip()
        if drop_empty_both and (not s and not t):
            continue
        pairs.append((s, t))

    if dedup:
        seen, out = set(), []
        for p in pairs:
            if p in seen: continue
            seen.add(p); out.append(p)
        pairs = out
    return pairs

def read_docx_text(file_like) -> str:
    docx = _lazy_docx()
    if not docx: return ""
    try:
        doc = docx.Document(file_like)
    except Exception:
        return ""
    blocks = []
    for p in doc.paragraphs:
        t = _normalize(p.text)
        if t: blocks.append(t)
    # æŠŠè¡¨æ ¼å•å…ƒä¹Ÿæ‹¼æˆè¡Œ.é¿å…æ¼æ‰å†…å®¹
    for tbl in doc.tables:
        for r in tbl.rows:
            line = " ".join(_normalize(c.text) for c in r.cells if _normalize(c.text))
            if line: blocks.append(line)
    return "\n".join(blocks)

def read_txt(file_like_or_bytes) -> str:
    try:
        if hasattr(file_like_or_bytes, "read"):
            data = file_like_or_bytes.read()
        else:
            data = file_like_or_bytes
        if isinstance(data, bytes):
            try: return data.decode("utf-8")
            except: return data.decode("utf-8", errors="ignore")
        return str(data)
    except Exception:
        return ""

def read_pdf_text(file_like) -> str:
    # ä¼˜å…ˆ pypdf;å¤±è´¥å†è¯• pdfminer.six
    try:
        from PyPDF2 import PdfReader
        reader = PdfReader(file_like)
        txts = []
        for page in reader.pages:
            t = page.extract_text() or ""
            t = _normalize(t)
            if t: txts.append(t)
        return "\n".join(txts)
    except Exception:
        try:
            from pdfminer.high_level import extract_text
            if hasattr(file_like, "read"):
                data = file_like.read()
            else:
                data = file_like
            return _normalize(extract_text(io.BytesIO(data)))
        except Exception:
            return ""
def _lazy_import_vec():
    """
    å…¼å®¹æ—§ä»£ç çš„å ä½å‡½æ•°ï¼š
    è¿”å› (np, faiss, SentenceTransformer, FastEmbedModel, TfidfVectorizer, extra)
    å®é™…ä¸Šä½ ç°åœ¨ç³»ç»Ÿåªç”¨ get_embedderï¼Œä¸å†ä¾èµ–è¿™ä¸ªå‡½æ•°çš„è¾“å‡ºã€‚
    """
    import numpy as np
    try:
        import faiss
    except Exception:
        faiss = None

    try:
        from sentence_transformers import SentenceTransformer
    except Exception:
        SentenceTransformer = None

    try:
        from fastembed import TextEmbedding as FastEmbedModel  # å¦‚æœæ²¡æœ‰ fastembed ä¹Ÿæ— æ‰€è°“
    except Exception:
        FastEmbedModel = None

    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
    except Exception:
        TfidfVectorizer = None

    return np, faiss, SentenceTransformer, FastEmbedModel, TfidfVectorizer, None

# ========== å‘é‡å¬å›(å¤šåç«¯:Sentence-Transformers â†’ Fastembed â†’ TF-IDF)==========
@st.cache_resource(show_spinner=False)
def get_embedder():
    """
    è¿”å› (backend, encode):

        - backend: å›ºå®š "st"
        - encode:  encode(texts: list[str]) -> np.ndarray[float32] (L2 å½’ä¸€åŒ–)

    åªä½¿ç”¨ SentenceTransformer å¥å‘é‡ï¼›
    ä¸å†é€€å› TF-IDFï¼Œä¸€æ—¦å¤±è´¥ç›´æ¥æŠ¥é”™ã€‚
    """
    import numpy as np

    # 1) å¯¼å…¥ SentenceTransformer
    try:
        from sentence_transformers import SentenceTransformer
    except Exception as e:
        st.error(f"âŒ æ— æ³•å¯¼å…¥ sentence-transformersï¼Œè¯·å…ˆå®‰è£…ä¾èµ–: {e}")
        # è¿™é‡Œç›´æ¥æŠ›é”™ï¼Œè®©è°ƒç”¨æ–¹åœ¨æ—¥å¿—é‡Œçœ‹åˆ°çœŸå®é—®é¢˜
        raise RuntimeError("sentence-transformers not available") from e

    # 2) å›ºå®šä½¿ç”¨ä¸€ä¸ªæ¨¡å‹ï¼ˆä½ ä¸€ç›´åœ¨ç”¨çš„é‚£åªï¼‰
    model_name = "distiluse-base-multilingual-cased-v1"

    try:
        model = SentenceTransformer(model_name)
    except Exception as e:
        st.error(f"âŒ åŠ è½½å¥å‘é‡æ¨¡å‹ {model_name} å¤±è´¥: {e}")
        raise RuntimeError(f"failed to load sentence transformer model {model_name}") from e

    # 3) å°è£…ç»Ÿä¸€ encode å‡½æ•°
    def encode_st(texts: list[str]):
        if not texts:
            # ç©ºè¾“å…¥æ—¶è¿”å› (0, dim) é¿å…åé¢ shape å¼‚å¸¸
            dim = model.get_sentence_embedding_dimension()
            return np.zeros((0, dim), dtype="float32")

        emb = model.encode(
            texts,
            normalize_embeddings=True,
            batch_size=32,
            convert_to_numpy=True,
        ).astype("float32")

        # åŒä¿é™©å†å½’ä¸€åŒ–ä¸€æ¬¡
        norms = np.linalg.norm(emb, axis=1, keepdims=True) + 1e-12
        return (emb / norms).astype("float32")

    st.info(f"âœ… å·²å¯ç”¨ SentenceTransformer å¥å‘é‡: {model_name}")
    return "st", encode_st

def _load_index(project_id: int):
    np, faiss, *_ = _lazy_import_vec()
    idx_path, map_path, vec_path = _index_paths(project_id)
    mapping = []
    if os.path.exists(map_path):
        with open(map_path, "r", encoding="utf-8") as f:
            mapping = json.load(f)
    # FAISS
    if faiss is not None and os.path.exists(idx_path):
        index = faiss.read_index(idx_path)
        return ("faiss", index, mapping, None)
    # å›é€€:.npy
    if os.path.exists(vec_path):
        vecs = np.load(vec_path).astype("float32")
        return ("fallback", None, mapping, vecs)

    # ç´¢å¼•å®Œå…¨ä¸å­˜åœ¨
    log_event(
        "WARNING",
        "semantic index not found",
        project_id=project_id,
        idx_path=idx_path,
        vec_path=vec_path,
    )
    return ("none", None, mapping, None)


def _save_index(project_id: int, mode: str, index, mapping, vecs=None):
    np, faiss, *_ = _lazy_import_vec()
    idx_path, map_path, vec_path = _index_paths(project_id)
    if mode == "faiss" and index is not None:
        faiss.write_index(index, idx_path)
    elif mode == "fallback" and vecs is not None:
        np.save(vec_path, vecs.astype("float32"))
    with open(map_path, "w", encoding="utf-8") as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)

def _index_paths_domain(domain: str, kb_type: str):
    """æŒ‰â€œé¢†åŸŸ + ç±»å‹â€è¿”å›å¯¹åº”ç´¢å¼•æ–‡ä»¶è·¯å¾„"""
    domain_key = _norm_domain_key(domain)
    base_dir = os.path.join(BASE_DIR, "semantic_index", domain_key, kb_type)
    os.makedirs(base_dir, exist_ok=True)
    idx_path = os.path.join(base_dir, "index.faiss")
    map_path = os.path.join(base_dir, "mapping.json")
    vec_path = os.path.join(base_dir, "vectors.npy")
    return idx_path, map_path, vec_path


def _load_index_domain(domain: str, kb_type: str):
    """æŒ‰â€œé¢†åŸŸ + ç±»å‹â€åŠ è½½ç´¢å¼•. è¿”å› (mode, index, mapping, vecs)"""
    np, faiss, *_ = _lazy_import_vec()
    idx_path, map_path, vec_path = _index_paths_domain(domain, kb_type)
    mapping = []
    if os.path.exists(map_path):
        with open(map_path, "r", encoding="utf-8") as f:
            mapping = json.load(f)
    if faiss is not None and os.path.exists(idx_path):
        index = faiss.read_index(idx_path)
        return "faiss", index, mapping, None
    if os.path.exists(vec_path):
        vecs = np.load(vec_path).astype("float32")
        return "fallback", None, mapping, vecs
    return "none", None, mapping, None


def _save_index_domain(domain: str, kb_type: str, mode: str, index, mapping, vecs=None):
    np, faiss, *_ = _lazy_import_vec()
    idx_path, map_path, vec_path = _index_paths_domain(domain, kb_type)
    if mode == "faiss" and index is not None:
        faiss.write_index(index, idx_path)
        # æœ‰æ—§çš„ numpy ç´¢å¼•å°±é¡ºæ‰‹åˆ ä¸€ä¸‹
        if os.path.exists(vec_path):
            try:
                os.remove(vec_path)
            except OSError:
                pass
    elif mode == "fallback" and vecs is not None:
        np.save(vec_path, vecs.astype("float32"))
        # æœ‰æ—§çš„ faiss ç´¢å¼•å°±é¡ºæ‰‹åˆ ä¸€ä¸‹
        if os.path.exists(idx_path):
            try:
                os.remove(idx_path)
            except OSError:
                pass
    with open(map_path, "w", encoding="utf-8") as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)
def build_strategy_index_for_domain(domain: str):
    """ä¸ºæŒ‡å®šé¢†åŸŸé‡å»ºã€ç¿»è¯‘ç­–ç•¥ã€‘(strategy) å•è¯­ç´¢å¼•.

    æ•°æ®æ¥æº: strategy_texts è¡¨, æ¯æ¡è®°å½•è§†ä¸ºä¸€ä¸ªâ€œç­–ç•¥æ–‡æ®µâ€ã€‚
    ç´¢å¼•å•ä½: ä»¥æ•´æ®µ content ä¸ºä¸€æ¡å‘é‡(å¦‚éœ€æ›´ç»†ç²’åº¦, å¯ä»¥åç»­å†æŒ‰å¥å­æ‹†åˆ†).
    """
    import numpy as _np
    np, faiss, *_ = _lazy_import_vec()
    backend, encode = get_embedder()

    dom = (domain or "").strip() or "æœªåˆ†ç±»"

    # 1) ç¡®ä¿ç­–ç•¥è¡¨å­˜åœ¨
    cur.execute(
        "CREATE TABLE IF NOT EXISTS strategy_texts ("
        "id INTEGER PRIMARY KEY,"
        "domain TEXT,"
        "title TEXT,"
        "content TEXT NOT NULL,"
        "collection TEXT,"
        "source TEXT,"
        "created_at TEXT DEFAULT (datetime('now'))"
        ");"
    )
    conn.commit()

    # 2) æ‹‰å–è¯¥é¢†åŸŸä¸‹çš„å…¨éƒ¨ç­–ç•¥æ–‡æœ¬
    rows = cur.execute(
        """
        SELECT id,
               IFNULL(domain,''), IFNULL(title,''), content,
               IFNULL(collection,''), IFNULL(source,'')
          FROM strategy_texts
         WHERE IFNULL(domain,'') = ?
         ORDER BY id ASC
        """,
        (dom,)
    ).fetchall()

    texts, metas = [], []
    for sid, d, ttl, content, coll, src in rows:
        txt = (content or "").strip()
        if not txt:
            continue
        texts.append(txt)
        metas.append({
            "strategy_id": sid,
            "domain": d or dom,
            "title": ttl,
            "content_preview": txt[:200],
            "collection": coll,
            "source": src,
            "kb_type": "strategy",
        })

    if not texts:
        # æ¸…ç©ºè¯¥é¢†åŸŸä¸‹çš„ç­–ç•¥ç´¢å¼•
        _save_index_domain(dom, "strategy", "none", None, [])
        return {"added": 0, "total": 0}

    # 3) ç¼–ç å‘é‡
    new_vecs = encode(texts)
    if hasattr(new_vecs, "toarray"):
        new_vecs = new_vecs.toarray()
    new_vecs = _np.asarray(new_vecs, dtype="float32")
    if new_vecs.ndim == 1:
        new_vecs = new_vecs.reshape(1, -1)
    new_vecs = new_vecs / (_np.linalg.norm(new_vecs, axis=1, keepdims=True) + 1e-12)

    # 4) å†™å…¥ç´¢å¼•æ–‡ä»¶
    if faiss is not None and backend in ("st", "fastembed"):
        dim = int(new_vecs.shape[1])
        index = faiss.IndexFlatIP(dim)
        index.add(new_vecs)
        _save_index_domain(dom, "strategy", "faiss", index, metas)
        total = int(index.ntotal)
    else:
        vecs = new_vecs
        _save_index_domain(dom, "strategy", "fallback", None, metas, vecs=vecs)
        total = int(vecs.shape[0])

    return {"added": len(texts), "total": total}

def build_project_vector_index(project_id: int,
                               use_src: bool = True,
                               use_tgt: bool = True):
    """
    ä¸ºæŒ‡å®šé¡¹ç›®æ‰€å±ã€é¢†åŸŸã€‘é‡å»ºå‘é‡ç´¢å¼•(å¥å¯¹ç‰ˆï¼Œä¸­è‹±å¯¹ç…§):

    - é€šè¿‡ project_id æ‰¾åˆ°è¯¥é¡¹ç›®çš„ domainï¼›
    - ä» corpus è¡¨ä¸­è¯»å–è¯¥é¢†åŸŸä¸‹æ‰€æœ‰åŒè¯­è¯­æ–™(ä¸å†æŒ‰ project_id é™åˆ¶)ï¼›
    - æŒ‰å¥å¯¹å¯¹é½: split_sents(src) / split_sents(tgt)ï¼›
    - ç”¨â€œä¸­æ–‡å¥å­â€ä½œä¸ºæ£€ç´¢å‘é‡æ–‡æœ¬ï¼›
    - mapping ä¸­ä¿å­˜: corpus_id, idx, src, tgt, project_id, domain, title, lang_pair, kb_type;
    - æ¯æ¬¡é‡å»ºæ—¶ï¼Œä¼šè¦†ç›–è¯¥é¢†åŸŸä¸‹çš„åŒè¯­ç´¢å¼•æ–‡ä»¶(semantic_index/{domain}/bilingual/)ã€‚

    è¿”å›: {"added": æ–°å¢æ¡æ•°, "total": ç´¢å¼•æ€»æ¡æ•°}
    """
    import numpy as _np
    np, faiss, *_ = _lazy_import_vec()
    backend, encode = get_embedder()

    pid = int(project_id)

    # 0) æ ¹æ®é¡¹ç›®å–é¢†åŸŸ
    proj_domain = None
    try:
        row = cur.execute(
            "SELECT IFNULL(domain,'') FROM items WHERE id=?",
            (pid,)
        ).fetchone()
        if row:
            proj_domain = (row[0] or "").strip()
    except Exception:
        proj_domain = None

    if not proj_domain:
        # æ²¡æœ‰è®¾ç½®é¢†åŸŸæ—¶ï¼Œå½’å…¥â€œæœªåˆ†ç±»â€
        proj_domain = "æœªåˆ†ç±»"

    # 1) ä» DB è¯»å–è¯¥é¢†åŸŸä¸‹çš„è¯­æ–™(ä¸å†æŒ‰ project_id é™åˆ¶)
    rows = cur.execute(
        """
        SELECT c.id,
               IFNULL(c.src_text, ''), IFNULL(c.tgt_text, ''),
               IFNULL(c.title, ''),    IFNULL(c.lang_pair, ''),
               IFNULL(c.project_id, 0), IFNULL(c.domain, '')
        FROM corpus c
        WHERE IFNULL(c.domain, '') = ?
        ORDER BY c.id ASC
        """,
        (proj_domain,)
    ).fetchall()

    texts, metas = [], []

    for cid, s, t, ttl, lp, pj, dom in rows:
        s = (s or "").strip()
        t = (t or "").strip()
        if not s and not t:
            continue

        # å¥å­åˆ‡åˆ†(å°½é‡ä½¿ç”¨ä½ å·²æœ‰çš„ split_sents)
        try:
            if "split_sents" in globals():
                src_sents = split_sents(s, lang_hint="zh")
                tgt_sents = split_sents(t, lang_hint="en")
            else:
                src_sents = (s.split("ã€‚") if s else [])
                tgt_sents = (t.split(".") if t else [])
        except Exception:
            src_sents = (s.split("ã€‚") if s else [])
            tgt_sents = (t.split(".") if t else [])

        # å¦‚æœè¦æ±‚åŒå‘å¯¹é½ï¼Œåˆ™å–æœ€å°é•¿åº¦ï¼›å¦åˆ™åªçœ‹ src
        n = min(len(src_sents), len(tgt_sents)) if (use_src and use_tgt) else len(src_sents or [])

        for idx in range(n):
            src_j = (src_sents[idx] if idx < len(src_sents) else "").strip()
            tgt_j = (tgt_sents[idx] if idx < len(tgt_sents) else "").strip()
            if not src_j:
                continue

            texts.append(src_j)
            metas.append({
                "corpus_id": cid,
                "idx": idx,
                "src": src_j,
                "tgt": tgt_j,
                "project_id": pj,
                "domain": dom or proj_domain or "",
                "title": ttl,
                "lang_pair": lp or "",
                "kb_type": "bilingual",
            })

    if not texts:
        # è¯¥é¢†åŸŸæ²¡æœ‰å¯ç”¨è¯­æ–™ï¼›æ¸…ç†ç´¢å¼•æ–‡ä»¶ï¼Œé¿å…æ®‹ç•™æ—§ç´¢å¼•
        try:
            _save_index(pid, "none", None, [], vecs=None)
        except Exception:
            pass
        return {"added": 0, "total": 0}

    # 2) ç¼–ç  & å½’ä¸€åŒ–
    new_vecs = encode(texts)
    if hasattr(new_vecs, "toarray"):
        new_vecs = new_vecs.toarray()
    new_vecs = _np.asarray(new_vecs, dtype="float32")
    if new_vecs.ndim == 1:
        new_vecs = new_vecs.reshape(1, -1)
    new_vecs = new_vecs / (_np.linalg.norm(new_vecs, axis=1, keepdims=True) + 1e-12)

    # 3) ç›´æ¥é‡å»ºç´¢å¼•(ä¸å†è¯»å–æ—§ mapping)
    if faiss is not None and backend in ("st", "fastembed"):
        dim = int(new_vecs.shape[1])
        index = faiss.IndexFlatIP(dim)
        index.add(new_vecs)
        _save_index(pid, "faiss", index, metas)
        total = int(index.ntotal)
    else:
        vecs = new_vecs
        _save_index(pid, "fallback", None, metas, vecs=vecs)
        total = int(vecs.shape[0])

    return {"added": len(texts), "total": total}

def rebuild_project_semantic_index(project_id: int) -> dict:
    """
    ç»Ÿä¸€å¯¹å¤–çš„â€œé‡å»ºè¯­ä¹‰ç´¢å¼•å…¥å£å‡½æ•°â€ã€‚

    ç”¨é€”ï¼š
      - åœ¨ Streamlit UI ä¹‹å¤–çš„è„šæœ¬é‡Œè°ƒç”¨ï¼›
      - ä»¥åå¦‚æœéœ€è¦åŠ æ—¥å¿— / æƒé™æ§åˆ¶ï¼Œåªæ”¹è¿™ä¸ªå‡½æ•°å³å¯ã€‚

    å‚æ•°ï¼š
      project_id: é¡¹ç›® IDï¼ˆint æˆ–å¯ä»¥è½¬æˆ int çš„å­—ç¬¦ä¸²ï¼‰

    è¿”å›ï¼š
      {"ok": True/False, "added": int, "total": int, "msg": str}
    """
    try:
        pid = int(project_id)
    except (TypeError, ValueError):
        return {"ok": False, "added": 0, "total": 0, "msg": f"éæ³•é¡¹ç›®ID: {project_id!r}"}

    try:
        res = build_project_vector_index(pid, use_src=True, use_tgt=True)
        return {
            "ok": True,
            "added": int(res.get("added", 0)),
            "total": int(res.get("total", 0)),
            "msg": "ç´¢å¼•é‡å»ºæˆåŠŸ",
        }
    except Exception as e:
        return {
            "ok": False,
            "added": 0,
            "total": 0,
            "msg": f"ç´¢å¼•é‡å»ºå¤±è´¥: {e}",
        }

# =========================
# è¯­ä¹‰å¬å›(æ”¯æŒèŒƒå›´:project/domain/all)
# è¿”å›: [(score, meta, src_sent, tgt_sent)]
# =========================
def _get_domain_for_proj(cur, project_id: int) -> str | None:
    """
    æ ¹æ®é¡¹ç›®IDè·å–é¡¹ç›®é¢†åŸŸ(domain)ï¼Œç”¨äº scope="domain" æ—¶è¿‡æ»¤å‚è€ƒè¯­æ–™ã€‚
    """
    try:
        row = cur.execute(
            "SELECT IFNULL(domain,'') FROM items WHERE id=?",
            (int(project_id),),
        ).fetchone()
    except Exception:
        return None

    if not row:
        return None

    dom = (row[0] or "").strip()
    return dom or None

def semantic_retrieve(project_id: int,
                      query_text: str,
                      topk: int = 20,
                      scope: str = "project",
                      min_char: int = 3):
    """
    è¯­æ–™åº“è¯­ä¹‰å¬å›(å¥çº§ï¼Œä¸­è‹±å¥å¯¹)

    ç»Ÿä¸€æ¥å£çº¦å®šï¼š
    ----------------------------------------
    å…¥å‚ï¼š
      - project_id : å½“å‰é¡¹ç›® ID
      - query_text : æŸ¥è¯¢æ–‡æœ¬ï¼ˆé€šå¸¸æ˜¯å½“å‰æ®µçš„ä¸­æ–‡ï¼‰
      - topk       : æœ€ç»ˆè¿”å›çš„æœ€å¤šæ¡æ•°
      - scope      : "project" / "domain" / "all"
      - min_char   : æœ€å°å­—ç¬¦æ•°é—¨æ§›

    è¿”å›ï¼š
      List[Tuple[float, dict, str, str]]
      å³ï¼š[(score, meta, src_sent, tgt_sent), ...]
        - score     : ç›¸ä¼¼åº¦åˆ†æ•°(floatï¼Œå·²æ’åºï¼Œè¶Šå¤§è¶Šç›¸ä¼¼)
        - meta      : æ¥è‡ª mapping çš„å­—å…¸ï¼ˆè‡³å°‘åŒ…å« corpus_id, idx, project_id, domain, title, lang_pair ç­‰ï¼‰
        - src_sent  : å‚è€ƒè¯­æ–™ä¸­çš„ä¸­æ–‡å¥å­
        - tgt_sent  : å‚è€ƒè¯­æ–™ä¸­çš„å¯¹åº”è¯‘æ–‡å¥å­ï¼ˆå¦‚æ— åˆ™å¯èƒ½ä¸ºç©ºä¸²ï¼‰

    æ³¨æ„ï¼š
      - å¤–éƒ¨åªéœ€è¦è®°ä½ï¼šæ°¸è¿œæ˜¯å››å…ƒç»„ï¼›æƒ³åªç”¨å‰ä¸‰ä¸ªå°± row[:3]ã€‚
    """
    import numpy as np

    q = (query_text or "").strip()
    if len(q) < min_char:
        return []

    # --- å·¥å…·ï¼šåˆ‡å¥ï¼ˆä¼˜å…ˆç”¨ä½ å·²æœ‰çš„ split_sentsï¼Œå¤±è´¥å°±æ­£åˆ™ç²—åˆ‡ï¼‰ ---
    def _split(text: str) -> list[str]:
        try:
            if "split_sents" in globals():
                segs = split_sents(text, lang_hint="auto")  # type: ignore
                return [s for s in segs if s and len(s.strip()) >= min_char]
        except Exception:
            pass
        import re
        segs = re.split(r"(?<=[\.\!\?;ã€‚ï¼ï¼Ÿï¼›])\s*", text)
        return [s.strip() for s in segs if s and len(s.strip()) >= min_char]

    pieces = _split(q)
    if not pieces:
        return []

    # --- å–å‘é‡ç¼–ç å™¨ ---
    try:
        backend, encode = get_embedder()
    except RuntimeError as e:
        st.error(f"å‘é‡æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # --- å–ç´¢å¼• & mapping ---
    mode, index, mapping, vecs = _load_index(int(project_id))  # å·²åœ¨ä½ æ–‡ä»¶é‡Œå®šä¹‰
    if mode == "none" or not mapping:
        return []

    # æ‡’åŠ è½½ numpy / faiss
    np_mod, faiss, *_ = _lazy_import_vec()  # ä½ å·²æœ‰çš„å·¥å…·å‡½æ•°
    np = np_mod  # ä¸ºäº†å°‘æ•²å‡ ä¸ªå­—

    # ä¸ºäº† domain scopeï¼Œç”¨ä¸€ä¸‹é¡¹ç›®é¢†åŸŸ
    cur_domain = None
    if scope == "domain":
        try:
            cur_domain = _get_domain_for_proj(cur, int(project_id))  # ä½ æ–‡ä»¶é‡Œå·²æœ‰
        except Exception:
            cur_domain = None

    def _scope_ok(meta: dict) -> bool:
        """æŒ‰ scope è¿‡æ»¤å€™é€‰ã€‚"""
        if scope == "project":
            return int(meta.get("project_id", 0) or 0) == int(project_id)
        elif scope == "domain" and cur_domain:
            return (meta.get("domain") or "") == (cur_domain or "")
        else:
            # "all" æˆ–æ‹¿ä¸åˆ° domain æ—¶ï¼Œéƒ½ä¸è¿‡æ»¤
            return True

    all_hits: list[tuple[float, dict, str, str]] = []

    # ä¸ºäº†ç¨³ä¸€ç‚¹ï¼Œæˆ‘ä»¬è®©æ¯ä¸ªæŸ¥è¯¢å¥å­å¤šæ‹¿ä¸€ç‚¹å€™é€‰ï¼Œå†æ•´ä½“å»é‡ã€æˆªæ–­
    per_piece_k = max(topk * 3, topk)

    for piece in pieces:
        if not piece:
            continue

        try:
            # 1) ç”ŸæˆæŸ¥è¯¢å‘é‡ qv
            qv = encode([piece])
            if hasattr(qv, "toarray"):  # tf-idf ç¨€ç–çŸ©é˜µ
                qv = qv.toarray()
            qv = np.asarray(qv, dtype="float32")
            if qv.ndim == 2:
                qv = qv[0]

            # æŸ¥è¯¢å‘é‡å½’ä¸€åŒ–
            q_norm = np.linalg.norm(qv) + 1e-12
            qv = qv / q_norm

            # 2) FAISS åˆ†æ”¯
            if mode == "faiss" and index is not None and faiss is not None:
                k = min(per_piece_k, len(mapping))
                if k <= 0:
                    continue
                D, I = index.search(qv.reshape(1, -1), k)
                for score, idx in zip(D[0].tolist(), I[0].tolist()):
                    idx = int(idx)
                    if idx < 0 or idx >= len(mapping):
                        continue
                    meta = mapping[idx] or {}
                    if not isinstance(meta, dict):
                        continue
                    if not _scope_ok(meta):
                        continue
                    src_sent = (meta.get("src") or "").strip()
                    tgt_sent = (meta.get("tgt") or "").strip()
                    if not src_sent and not tgt_sent:
                        continue
                    all_hits.append((float(score), meta, src_sent, tgt_sent))

            # 3) fallback åˆ†æ”¯ï¼šçº¯ numpy ç›¸ä¼¼åº¦
            elif mode == "fallback" and vecs is not None:
                arr = np.asarray(vecs, dtype="float32")
                if arr.ndim != 2 or arr.shape[0] == 0:
                    continue
                sims = arr @ qv.reshape(-1, 1)  # å†…ç§¯ï¼Œå‘é‡å·²å½’ä¸€åŒ– => cos ç›¸ä¼¼åº¦
                sims = sims.reshape(-1)
                k = min(per_piece_k, sims.shape[0])
                if k <= 0:
                    continue
                idxs = np.argsort(-sims)[:k]
                for idx in idxs:
                    idx = int(idx)
                    score = float(sims[idx])
                    if idx < 0 or idx >= len(mapping):
                        continue
                    meta = mapping[idx] or {}
                    if not isinstance(meta, dict):
                        continue
                    if not _scope_ok(meta):
                        continue
                    src_sent = (meta.get("src") or "").strip()
                    tgt_sent = (meta.get("tgt") or "").strip()
                    if not src_sent and not tgt_sent:
                        continue
                    all_hits.append((score, meta, src_sent, tgt_sent))

            else:
                # æ²¡æœ‰å¯ç”¨ç´¢å¼•
                continue

        except Exception:
            # å¬å›å¤±è´¥æ—¶ï¼Œå®å¯å°‘ç»™ç»“æœï¼Œä¹Ÿä¸è¦æŠŠå¼‚å¸¸ç›´æ¥ç‚¸åˆ° UI
            continue

    if not all_hits:
        return []

    # --- å»é‡ + æŒ‰å¾—åˆ†æ’åºï¼Œä¿ç•™å‰ topk ---
    dedup = {}
    for score, meta, src_sent, tgt_sent in all_hits:
        key = (src_sent, tgt_sent)
        if key not in dedup or score > dedup[key][0]:
            dedup[key] = (score, meta, src_sent, tgt_sent)

    hits = sorted(dedup.values(), key=lambda x: x[0], reverse=True)
    return hits[:topk]

def semantic_consistency_report(project_id: int,
                                blocks_src: list,
                                blocks_tgt: list,
                                term_map: dict,
                                topk: int = 3,
                                thr: float = 0.70):
    """
    è¯‘åä¸€è‡´æ€§æŠ¥å‘Š (è¯­ä¹‰ + æœ¯è¯­ï¼ŒæŒ‰æ®µè½)ã€‚

    å‚æ•°:
        project_id : é¡¹ç›®ID
        blocks_src : æºæ–‡åˆ†æ®µåˆ—è¡¨
        blocks_tgt : è¯‘æ–‡åˆ†æ®µåˆ—è¡¨
        term_map   : {æºæœ¯è¯­: ç›®æ ‡æœ¯è¯­}
        topk       : è¯­ä¹‰å‚è€ƒæ£€ç´¢çš„å€™é€‰æ¡æ•°
        thr        : è®¤ä¸ºâ€œç›¸ä¼¼åº¦è¿‡ä½â€çš„é˜ˆå€¼

    è¿”å›:
        pandas.DataFrameï¼Œåˆ—åŒ…æ‹¬:
            - æ®µå·
            - ç›¸ä¼¼å‚è€ƒå¾—åˆ†
            - ä½äºé˜ˆå€¼ (bool)
            - æœªéµå®ˆæœ¯è¯­ (é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²)
    """
    hits_all = []

    # å¯¹é½é•¿åº¦ï¼Œé¿å…ä¸¤ä¾§é•¿åº¦ä¸ä¸€è‡´å‡ºé”™
    n = min(len(blocks_src or []), len(blocks_tgt or []))
    if n == 0:
        return pd.DataFrame([])

    for i, (s, t) in enumerate(zip(blocks_src[:n], blocks_tgt[:n]), 1):
        s = s or ""
        t = t or ""

        # 1) ç”¨â€œè¯‘æ–‡â€å»æ£€ç´¢å‚è€ƒè¯‘æ–‡ï¼ˆæ›´è´´è¿‘äººå·¥å®¡æ ¡ï¼‰
        try:
            hits = semantic_retrieve(project_id, t, topk=topk)
        except Exception:
            hits = []

        # semantic_retrieve ç»Ÿä¸€è¿”å›: (score, meta, src_sent, tgt_sent)
        if hits:
            top_score = float(hits[0][0])
        else:
            top_score = 0.0

        # 2) æœ¯è¯­éµå®ˆï¼šæºæ®µåŒ…å«æºæœ¯è¯­ï¼Œä½†è¯‘æ–‡é‡Œæ²¡å‡ºç°ç›®æ ‡æœ¯è¯­
        violated = []
        for src_term, tgt_term in (term_map or {}).items():
            if not src_term or not tgt_term:
                continue
            if src_term in s and tgt_term not in t:
                violated.append(f"{src_term}->{tgt_term}")

        hits_all.append(
            {
                "æ®µå·": i,
                "ç›¸ä¼¼å‚è€ƒå¾—åˆ†": round(top_score, 2),
                "ä½äºé˜ˆå€¼": (top_score < thr),
                "æœªéµå®ˆæœ¯è¯­": ", ".join(violated) if violated else "",
            }
        )

    return pd.DataFrame(hits_all)

# ========== è·¯å¾„/DB ==========
conn = sqlite3.connect(DB_PATH, check_same_thread=False)
cur = conn.cursor()

def ensure_domain_columns_and_backfill(conn, cur, corpus_table="corpus"):
    # items.domain
    cols = [r[1] for r in cur.execute("PRAGMA table_info(items)").fetchall()]
    if "domain" not in cols:
        cur.execute("ALTER TABLE items ADD COLUMN domain TEXT;")
        conn.commit()

    # è¯­æ–™è¡¨ domain(å¦‚æœä½ ç”¨ corpus_main å°±æŠŠå‚æ•°æ”¹æˆ corpus_main)
    cols = [r[1] for r in cur.execute(f"PRAGMA table_info({corpus_table})").fetchall()]
    if "domain" not in cols:
        cur.execute(f"ALTER TABLE {corpus_table} ADD COLUMN domain TEXT;")
        conn.commit()

    # å›å¡«:ç”¨ items.domain è¡¥ corpus.domain(æœ‰ project_id çš„è¡Œ)
    try:
        cur.execute(f"""
            UPDATE {corpus_table}
            SET domain = (
              SELECT i.domain FROM items i WHERE i.id = {corpus_table}.project_id
            )
            WHERE domain IS NULL AND project_id IS NOT NULL;
        """)
        conn.commit()
    except Exception:
        pass

# è°ƒç”¨(è€åº“è¡¨åæ˜¯ corpus):
ensure_domain_columns_and_backfill(conn, cur, corpus_table="corpus")
# è‹¥ä½ å·²ç»åˆ‡åˆ° corpus_main / corpus_vec:
# ensure_domain_columns_and_backfill(conn, cur, corpus_table="corpus_main")

def _get_domain_for_proj(cur, project_id: int):
    """
    å·¥å…·å‡½æ•°: æ ¹æ®é¡¹ç›®IDè¯»å– items.domain; è‹¥ä¸å­˜åœ¨æˆ–ä¸ºç©º, è¿”å› Noneã€‚
    """
    try:
        row = cur.execute(
            "SELECT domain FROM items WHERE id=?",
            (int(project_id),)
        ).fetchone()
        if not row:
            return None
        val = (row[0] or "").strip()
        return val or None
    except Exception:
        return None
try:
    cur.execute("CREATE INDEX IF NOT EXISTS idx_term_ext_project ON term_ext(project_id)")
    conn.commit()
except Exception as e:
    print("ç´¢å¼•åˆ›å»ºè·³è¿‡:", e)

def _has_col(table: str, col: str) -> bool:
    cur.execute(f"PRAGMA table_info({table})")
    return any(r[1] == col for r in cur.fetchall())

def ensure_col(table: str, col: str, col_type: str):
    """
    ç¡®ä¿æŒ‡å®šè¡¨å­˜åœ¨æŸåˆ—ï¼›å¦‚ä¸å­˜åœ¨åˆ™æ·»åŠ å¹¶ç«‹å³æäº¤ã€‚
    ä¾èµ–å…¨å±€çš„ conn/curï¼Œè°ƒç”¨æ–¹æ— éœ€å•ç‹¬ commitã€‚
    """
    cur.execute(f"PRAGMA table_info({table})")
    cols = {r[1] for r in cur.fetchall()}
    if col not in cols:
        cur.execute(f"ALTER TABLE {table} ADD COLUMN {col} {col_type}")
        conn.commit()

# â€”â€” å»ºè¡¨
cur.execute("""
CREATE TABLE IF NOT EXISTS items (
    id INTEGER PRIMARY KEY,
    title TEXT NOT NULL,
    body TEXT,
    tags TEXT,
    domain TEXT,
    type TEXT,
    created_at TEXT DEFAULT (datetime('now'))
);
""")

cur.execute("""
CREATE TABLE IF NOT EXISTS item_ext (
    id INTEGER PRIMARY KEY,
    item_id INTEGER,
    src_path TEXT,
    FOREIGN KEY(item_id) REFERENCES items(id)
);
""")

cur.execute("""
CREATE TABLE IF NOT EXISTS project_files (
    id INTEGER PRIMARY KEY,
    project_id INTEGER NOT NULL,
    file_path TEXT NOT NULL,
    file_name TEXT,
    uploaded_at TEXT DEFAULT (datetime('now')),
    note TEXT,
    FOREIGN KEY(project_id) REFERENCES items(id)
);
""")

cur.execute("""
CREATE TABLE IF NOT EXISTS term_ext (
    id INTEGER PRIMARY KEY,
    source_term TEXT NOT NULL,
    target_term TEXT,
    domain TEXT,
    project_id INTEGER,
    strategy TEXT,
    example TEXT,
    category TEXT,
    FOREIGN KEY(project_id) REFERENCES items(id)
);
""")

cur.execute("""
CREATE TABLE IF NOT EXISTS trans_ext (
    id INTEGER PRIMARY KEY,
    project_id INTEGER,
    src_text TEXT,
    tgt_text TEXT,
    mode TEXT,
    segment_count INTEGER,
    created_at TEXT DEFAULT (datetime('now')),
    FOREIGN KEY(project_id) REFERENCES items(id)
);
""")

cur.execute("""
CREATE TABLE IF NOT EXISTS corpus (
    id INTEGER PRIMARY KEY,
    project_id INTEGER,
    text TEXT,
    lang TEXT,
    source TEXT,
    created_at TEXT DEFAULT (datetime('now')),
    FOREIGN KEY(project_id) REFERENCES items(id)
);
""")
conn.commit()


# â€”â€” å…œåº•è¡¥åˆ—
for t, cols in {
    "items": [("type","TEXT"),("tags","TEXT"),("scene","TEXT"),("prompt","TEXT"),
              ("mode","TEXT"),("body","TEXT"),("created_at","TEXT"),("updated_at","TEXT"),("trans_type","TEXT")],
    "item_ext": [("src_path","TEXT")],
    "term_ext": [("domain","TEXT"),("project_id","INTEGER"),("strategy","TEXT"),
                 ("example","TEXT"),("note","TEXT"), ("category","TEXT")],
    "trans_ext": [("stats_json","TEXT"),("segments","INTEGER"),("term_hit_total","INTEGER")],
    "corpus": [
        ("title","TEXT"),("project_id","INTEGER"),("lang_pair","TEXT"),("src_text","TEXT"),("tgt_text","TEXT"),
        ("note","TEXT"),("created_at","TEXT"),("domain","TEXT"),("source","TEXT"),
    ],
}.items():
    for c, tp in cols:
        ensure_col(t, c, tp)
cur.execute("UPDATE items SET type='project' WHERE IFNULL(type,'')=''")
cur.execute("UPDATE items SET created_at = COALESCE(created_at, strftime('%Y-%m-%d %H:%M:%S','now'))")
conn.commit()
ensure_col("term_ext", "example_vector_id", "INTEGER")

# --- æœ¯è¯­è¡¨å­—æ®µå…¼å®¹:ç¼ºå°‘ project_id æ—¶è¡¥å»º ---
try:
    cur.execute("PRAGMA table_info(term_ext)")
    cols = [c[1] for c in cur.fetchall()]
    if "project_id" not in cols:
        cur.execute("ALTER TABLE term_ext ADD COLUMN project_id INTEGER")
        conn.commit()
except Exception as e:
    st.warning(f"æœ¯è¯­è¡¨ç»“æ„æ£€æŸ¥:{e}")

# ========== DeepSeek å‚æ•°/è°ƒç”¨ ==========
def get_deepseek():
    """
    ä» .streamlit/secrets.toml è¯»å–:
    [deepseek]
    api_key="..."
    model="deepseek-chat"
    """
    try:
        ak = st.secrets["deepseek"]["api_key"]
        model = st.secrets["deepseek"].get("model", "deepseek-chat")
        return ak, model
    except Exception:
        return None, None

        # === æ–°å¢:æœ¯è¯­æç¤º + å‚è€ƒä¾‹å¥ ===
def _build_ref_context(project_id: int,
                       query_text: str,
                       topk: int = 20,
                       min_sim: float = 0.25,
                       prefer_side: str = "both",   # å½“å‰æš‚æœªä½¿ç”¨ï¼Œä¿ç•™ä»¥å…¼å®¹æ—§å‚æ•°
                       scope: str = "project",
                       top_n: int = 5) -> str:
    """
    æ„å»ºå‚è€ƒä¾‹å¥(å¥çº§ï¼Œä¸­è‹±å¯¹ç…§)ã€‚

    ä¾èµ– semantic_retrieve è¿”å›: (score, meta, src_sent, tgt_sent)
    è¿”å›:
        ä¸€æ®µå¯ç›´æ¥æ³¨å…¥ Prompt çš„å­—ç¬¦ä¸²ï¼Œå¤šæ¡ä¾‹å¥ç”¨æ¢è¡Œæ‹¼æ¥ã€‚
    """
    try:
        hits = semantic_retrieve(
            project_id,
            query_text,
            topk=topk,
            scope=scope,
        )
    except Exception as e:
        # å¬å›å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œåªåœ¨ UI ç¯å¢ƒä¸‹åšä¸ªè½»æç¤º
        try:
            st.warning(f"å‚è€ƒæ£€ç´¢å¤±è´¥: {e}")
        except Exception:
            pass
        return ""

    if not hits:
        return ""

    # 1) è¿‡æ»¤ä½ç›¸ä¼¼åº¦ + å»é‡
    seen = set()
    selected = []

    for sc, meta, src_sent, tgt_sent in hits:
        try:
            score = float(sc or 0.0)
        except Exception:
            score = 0.0

        if score < min_sim:
            continue

        ch = (src_sent or "").strip()
        en = (tgt_sent or "").strip()

        # ä¸­è‹±æ–‡éƒ½ç©ºï¼Œå°±è·³è¿‡
        if not ch and not en:
            continue

        key = (ch, en)
        if key in seen:
            continue
        seen.add(key)

        selected.append((score, meta, ch, en))
        if len(selected) >= top_n:
            break

    # å¦‚æœç­›å®Œä¸€ä¸ªéƒ½æ²¡æœ‰ï¼Œå°±é€€ä¸€æ­¥ï¼šæ‹¿æœ€é«˜åˆ†é‚£æ¡
    if not selected:
        best = hits[0]
        sc, meta, ch, en = best
        ch = (ch or "").strip()
        en = (en or "").strip()
        if not ch and not en:
            return ""
        try:
            sc = float(sc or 0.0)
        except Exception:
            sc = 0.0
        selected = [(sc, meta, ch, en)]

    # 2) æ‹¼æˆå¤šè¡Œæ–‡æœ¬
    ctx_lines = ["å‚è€ƒä¾‹å¥(ç”¨äºä¿æŒæœ¯è¯­ä¸é£æ ¼ä¸€è‡´):"]
    for idx, (sc, meta, ch, en) in enumerate(selected, 1):
        dom = (meta.get("domain") or "").strip() if isinstance(meta, dict) else ""
        title = (meta.get("title") or "").strip() if isinstance(meta, dict) else ""
        tag_info = " Â· ".join(x for x in [dom, title] if x)

        ch_show = ch.replace("\n", " ").strip()
        en_show = en.replace("\n", " ").strip()

        if en_show:
            line = (
                f"ä¾‹å¥{idx} åŸæ–‡:{ch_show}\n"
                f"       è¯‘æ–‡:{en_show}"
                f"(sim={sc:.2f}{'ï¼Œ'+tag_info if tag_info else ''})"
            )
        else:
            line = f"ä¾‹å¥{idx}:{ch_show}(sim={sc:.2f}{'ï¼Œ'+tag_info if tag_info else ''})"

        ctx_lines.append(line)

        # æ§åˆ¶æ€»é•¿åº¦ï¼Œé¿å… prompt è¿‡é•¿
        if sum(len(x) for x in ctx_lines) > 1800:
            break

    return "\n".join(ctx_lines) if len(ctx_lines) > 1 else ""
# -------- Glossary & Instruction helpers (æ”¾åœ¨ ds_translate ä¸Šæ–¹) --------
def build_term_hint(term_dict: dict, lang_pair: str, max_terms: int = 80) -> str:
    """
    å°†æœ¯è¯­æ˜ å°„è½¬æˆå¯è¯»çš„â€œç¡¬çº¦æŸâ€è§„åˆ™æ–‡æœ¬.æ”¯æŒä»¥ä¸‹å‡ ç§ term_dict ç»“æ„:
      { "contract": "åˆåŒ" }
      { "contract": {"target":"åˆåŒ", "pos":"NOUN", "usage_note":"æ³•å¾‹è¯­å¢ƒ"} }
      { "contract": ("åˆåŒ", "NOUN") }   # å…ƒç»„å½¢å¼ (target, pos)
    ç©º/é dict çš„è¾“å…¥ä¼šè¢«å®‰å…¨å¿½ç•¥; ç©ºç›®æ ‡ä¼šè¢«å¿½ç•¥; è‡ªåŠ¨å»é‡å¹¶æœ€å¤šè¾“å‡º
    max_terms æ¡ï¼Œé¿å…æç¤ºè¿‡é•¿ã€‚
    """
    if not term_dict or not isinstance(term_dict, dict):
        return ""

    lines = []
    seen = set()
    items = list(term_dict.items())[: max_terms * 2]  # ç¨å¤šå–ä¸€äº›.è¿‡æ»¤ç©ºåå†æˆªæ–­

    for src, val in items:
        if src is None: 
            continue
        src = str(src).strip()
        if not src or src in seen:
            continue

        tgt, pos, note = None, None, None
        if isinstance(val, dict):
            tgt  = (val.get("target") or val.get("tgt") or "").strip()
            pos  = (val.get("pos") or "").strip() or None
            note = (val.get("usage_note") or val.get("note") or "").strip() or None
        elif isinstance(val, (list, tuple)) and len(val) >= 1:
            tgt = str(val[0]).strip()
            if len(val) >= 2:
                pos = (str(val[1]).strip() or None)
        else:
            tgt = str(val or "").strip()

        if not tgt:
            continue

        seen.add(src)
        if pos:
            line = f"- When '{src}' is a {pos}, translate it as '{tgt}'."
        else:
            line = f"- Translate '{src}' as '{tgt}'."
        if note:
            line += f" ({note})"
        lines.append(line)

    if not lines:
        return ""  # æ²¡æœ‰å¯ç”¨æœ¯è¯­å°±è¿”å›ç©ºä¸².ä¸è¦å¹²æ‰°æç¤º

    header = "GLOSSARY (STRICT):\n"
    return header + "\n".join(lines[:max_terms]) + "\n"


def build_instruction(lang_pair: str) -> str:
    """
    ç”Ÿæˆç®€æ´çš„ç¿»è¯‘æŒ‡ä»¤ã€‚ä½ ä¹Ÿå¯ä»¥æŒ‰é¡¹ç›®é£æ ¼å†æ‰©å±•ã€‚

    - æ”¯æŒä¸­æ–‡ä¸è‹±æ–‡å†™æ³•ï¼ˆå¦‚ "Chinese to English"/"Englishâ†’Chinese"ï¼‰ã€‚
    - ç»Ÿä¸€æŠŠå„ç§ç®­å¤´/è¿å­—ç¬¦/"to" è½¬æˆ "-"ï¼Œä¾¿äºæ¨¡å¼åŒ¹é…ã€‚
    """
    lp_raw = (lang_pair or "").replace(" ", "")
    lp_norm = lp_raw.lower()
    for sep in ("â†’", "->", "=>", "â€”>", "â€”", "â€”", "â€”-", "â€”â€”"):
        lp_norm = lp_norm.replace(sep, "-")
    lp_norm = (
        lp_norm.replace("to", "-")
        .replace("_", "-")
        .replace("/", "-")
    )

    zh_to_en_tokens = (
        "ä¸­è¯‘è‹±", "ä¸­â†’è‹±", "ä¸­->è‹±", "ä¸­-è‹±", "zh-en", "zh2en", "zh_en", "zh-en",
        "chinese-english", "chinese-en", "zh-english",
    )
    en_to_zh_tokens = (
        "è‹±è¯‘ä¸­", "è‹±â†’ä¸­", "è‹±->ä¸­", "è‹±-ä¸­", "en-zh", "en2zh", "en_zh", "en-zh",
        "english-chinese", "english-zh", "en-chinese",
    )

    def _match(tokens: tuple[str, ...]) -> bool:
        return any(tok in lp_raw or tok in lp_norm for tok in tokens)

    if _match(zh_to_en_tokens):
        return (
            "Translate the source text from Chinese to English. "
            "Use a professional, natural style; follow the GLOSSARY (STRICT) exactly; "
            "preserve proper nouns and numbers; keep paragraph structure. "
            "Do not add explanations."
        )

    if _match(en_to_zh_tokens):
        return (
            "Translate the source text from English to Chinese. "
            "ç”¨ä¸“ä¸šã€é€šé¡ºã€ç¬¦åˆé¢†åŸŸæ–‡ä½“çš„ä¸­æ–‡è¡¨è¾¾;ä¸¥æ ¼éµå®ˆä¸Šæ–¹ GLOSSARY (STRICT);"
            "ä¸“æœ‰åè¯ã€æ•°å­—ä¸è®¡é‡å•ä½ä¿æŒå‡†ç¡®;æ®µè½ç»“æ„ä¿æŒä¸€è‡´ã€‚ä¸å¾—æ·»åŠ è§£é‡Šã€‚"
        )

    # å…œåº•
    return (
        "Translate the source text. Follow the GLOSSARY (STRICT) exactly. "
        "Keep the original structure and do not add explanations."
    )

def ds_translate(
    block: str,
    term_dict: dict,
    lang_pair: str,
    ak: str,
    model: str,
    ref_context: str = "",
    fewshot_examples=None,
) -> str:
    term_hint = build_term_hint(term_dict, lang_pair)  # ç»Ÿä¸€ä½¿ç”¨ä¸¥æ ¼æœ¯è¯­æç¤º
    instr = build_instruction(lang_pair)   # type: ignore

    """
    ä½¿ç”¨ DeepSeek REST API ç¿»è¯‘ä¸€ä¸ªæ–‡æœ¬å—ã€‚term_dict ä¸º {æº: ç›®æ ‡} çš„æ˜ å°„.æ³¨å…¥ä¸ºå¼ºçº¦æŸæç¤ºã€‚
    """
    import requests

    if not block.strip():
        return ""

    # å¦‚æœæœ¯è¯­ä¸ºç©ºï¼Œä¸ºäº†è®©æç¤ºå§‹ç»ˆåŒ…å« GLOSSARY æ®µè½ï¼Œç»™å‡ºä¸€ä¸ªå®‰å…¨çš„å…œåº•
    if not term_hint:
        if term_dict:
            # æœ¯è¯­å­—å…¸å­˜åœ¨ä½†å†…å®¹è¢«è¿‡æ»¤ä¸ºç©ºï¼Œç»™å‡ºç®€æ´çš„é»˜è®¤æç¤º
            term_hint = (
                "GLOSSARY (STRICT):\n"
                "- Follow provided terminology exactly; do not paraphrase fixed terms.\n\n"
            )
        else:
            term_hint = (
                "GLOSSARY (STRICT):\n"
                "- Ensure consistent terminology; avoid paraphrasing fixed terms.\n\n"
            )

    # ä¿è¯ä¸åç»­ INSTRUCTION å—ä¹‹é—´æœ‰ç©ºè¡Œ
    if not term_hint.endswith("\n\n"):
        term_hint = term_hint.rstrip("\n") + "\n\n"

    system_msg = (
        "You are a senior professional translator. Prioritize accuracy, faithfulness, and consistent terminology. "
        "No hallucinations. If a term mapping is provided, follow it strictly."
    )

    user_msg = (
        f"{term_hint}"
        + (f"REFERENCE CONTEXT (use if relevant):\n{ref_context}\n\n" if ref_context else "")
        + "INSTRUCTION:\n" + instr + "\n"
        + "RESPONSE FORMAT:\n- Return ONLY the final translation text, no explanations, no backticks.\n\n"
        + "SOURCE:\n" + block
    )

    messages = [{"role": "system", "content": system_msg}]
    if fewshot_examples:
        for ex in fewshot_examples:
            src_demo = (ex.get("src") or "").strip()
            tgt_demo = (ex.get("tgt") or "").strip()
            if not (src_demo and tgt_demo):
                continue
            title = ex.get("title") or ""
            demo_user = f"ã€å‚è€ƒç¤ºä¾‹:{title}ã€‘\næºæ–‡:\n{src_demo}"
            messages.append({"role": "user", "content": demo_user})
            messages.append({"role": "assistant", "content": tgt_demo})
    messages.append({"role": "user", "content": user_msg})

    url = "https://api.deepseek.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {ak}", "Content-Type": "application/json"}
    payload = {
        "model": model or "deepseek-chat",
        "messages": messages,
        "temperature": 0.2,
    }

    for attempt in range(3):
        try:
            resp = requests.post(url, headers=headers, json=payload, timeout=60)
            if resp.status_code == 200:
                data = resp.json()
                return data["choices"][0]["message"]["content"].strip()
            else:
                txt = f"[DeepSeek {resp.status_code}] {resp.text}"
                if resp.status_code in (429, 500, 502, 503, 504) and attempt < 2:
                    time.sleep(1.5 * (attempt + 1))
                    continue
                # æœ€ç»ˆå¤±è´¥: è®°ä¸€æ¡é”™è¯¯æ—¥å¿—
                log_event(
                    "ERROR",
                    "DeepSeek HTTP error",
                    status_code=resp.status_code,
                    body=resp.text[:500],
                )
                return txt
        except Exception as e:
            if attempt < 2:
                time.sleep(1.5 * (attempt + 1))
                continue
            # é‡è¯•ç”¨å°½ä»ç„¶å¼‚å¸¸: è®°ä¸€æ¡é”™è¯¯æ—¥å¿—
            log_event(
                "ERROR",
                "DeepSeek request exception",
                error=str(e),
            )
            return f"[DeepSeek Request Error] {e}"

    # ç†è®ºä¸Šä¸ä¼šèµ°åˆ°è¿™ä¸€æ­¥ï¼Œå¦‚æœèµ°åˆ°äº†ï¼Œä¹Ÿè®°ä¸€æ¡
    log_event("ERROR", "DeepSeek unknown failure")
    return "[DeepSeek Error] Unknown failure."


def translate_block_with_kb(
    cur,
    project_id: int,
    block_text: str,
    lang_pair: str,
    ak: str,
    model: str,
    use_semantic: bool = True,
    scope: str = "project",
    fewshot_examples=None,
):
    """
    å•æ®µç¿»è¯‘ä¸»ç®¡çº¿(æ ¸å¿ƒæ¥å£ä¹‹ä¸€)ã€‚

    åŠŸèƒ½:
        block_text -> (æœ¯è¯­ + å‚è€ƒä¾‹å¥) -> DeepSeek ç¿»è¯‘ -> ç»“æ„åŒ–ç»“æœ

    å‚æ•°:
        cur            : SQLite cursor
        project_id     : å½“å‰é¡¹ç›® ID
        block_text     : æºæ®µè½æ–‡æœ¬
        lang_pair      : "ä¸­è¯‘è‹±" / "è‹±è¯‘ä¸­"
        ak, model      : DeepSeek çš„ key å’Œæ¨¡å‹å
        use_semantic   : æ˜¯å¦å¯ç”¨è¯­ä¹‰å¬å›å‚è€ƒ
        scope          : è¯­ä¹‰å¬å›èŒƒå›´("project"/"domain"/"all")
        fewshot_examples: few-shot ç¤ºä¾‹(åŒ ds_translate)

    è¿”å›:
        result: dictï¼Œå­—æ®µåŒ…æ‹¬:
            - src               : æºæ–‡æœ¬
            - tgt               : è¯‘æ–‡
            - project_id        : é¡¹ç›®ID
            - lang_pair         : ç¿»è¯‘æ–¹å‘
            - term_map_all      : å…¨é‡é™æ€æœ¯è¯­ {æº: ç›®æ ‡}
            - terms_in_block    : å½“å‰æ®µè½æ–‡æœ¬ä¸­å‘½ä¸­çš„æœ¯è¯­ {æº: ç›®æ ‡}
            - terms_corpus_dyn  : å‚è€ƒä¾‹å¥ä¸­å‘½ä¸­çš„æœ¯è¯­ {æº: ç›®æ ‡}
            - terms_final       : æœ€ç»ˆæ³¨å…¥ Prompt çš„æœ¯è¯­ {æº: ç›®æ ‡}
            - term_meta         : æœ¯è¯­å…ƒä¿¡æ¯åˆ—è¡¨(æ¥æº/é¢†åŸŸç­‰)
            - ref_context       : æ³¨å…¥çš„å‚è€ƒä¾‹å¥æ–‡æœ¬(å¦‚æœå¯ç”¨è¯­ä¹‰å¬å›)
            - violated_terms    : ç²—ç•¥ä¸€è‡´æ€§æ£€æŸ¥ä¸­â€œå¯èƒ½æœªéµå®ˆâ€çš„æœ¯è¯­åˆ—è¡¨
    """
    blk = (block_text or "").strip()
    if not blk:
        return {
            "src": "",
            "tgt": "",
            "project_id": project_id,
            "lang_pair": lang_pair,
            "term_map_all": {},
            "terms_in_block": {},
            "terms_corpus_dyn": {},
            "terms_final": {},
            "term_meta": [],
            "ref_context": "",
            "violated_terms": [],
        }

    # 1) é™æ€æœ¯è¯­ï¼ˆé¡¹ç›®+å…¨å±€ï¼‰ï¼Œç»Ÿä¸€æ¥å£
    term_map_all, term_meta = get_terms_for_project(cur, project_id, use_dynamic=True)

    # 2) å‘½ä¸­æ£€æµ‹å·¥å…·ï¼šç»™â€œæœ¬æ®µâ€å’Œâ€œè¯­æ–™å‚è€ƒâ€å…±ç”¨
    def _detect_hits(text: str, term_map: dict[str, str]) -> dict[str, str]:
        txt_low = (text or "").lower()
        out = {}
        for k, v in (term_map or {}).items():
            if not k:
                continue
            key_low = k.lower()
            if key_low in txt_low or k in text:
                out[k] = v
        return out

    # 2.1 å½“å‰æ®µè½æ–‡æœ¬ä¸­å‘½ä¸­çš„æœ¯è¯­
    terms_in_block = _detect_hits(blk, term_map_all)

    # 3) å‚è€ƒä¾‹å¥(æ¥è‡ªè¯­æ–™åº“è¯­ä¹‰å¬å›)
    if use_semantic:
        try:
            ref_context = _build_ref_context(
                project_id,
                blk,
                topk=20,
                min_sim=0.25,
                prefer_side="both",
                scope=scope,
            )
        except Exception:
            # å¬å›å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            ref_context = ""
    else:
        ref_context = ""

    # 3.1 è¯­æ–™é©±åŠ¨æœ¯è¯­ï¼šåœ¨å‚è€ƒä¾‹å¥æ–‡æœ¬ä¸­å‘½ä¸­çš„é™æ€æœ¯è¯­
    if ref_context:
        terms_corpus_dyn = _detect_hits(ref_context, term_map_all)
    else:
        terms_corpus_dyn = {}

    # 3.2 æœ€ç»ˆæ³¨å…¥ Prompt çš„æœ¯è¯­ = ä¸¤è€…å¹¶é›†
    terms_final = dict(terms_in_block)
    for k, v in terms_corpus_dyn.items():
        if k not in terms_final:
            terms_final[k] = v

    # 4) è°ƒç”¨ DeepSeek ç¿»è¯‘ï¼ˆåªå–‚æœ€ç»ˆæœ¯è¯­ï¼‰
    tgt = ds_translate(
        block=blk,
        term_dict=terms_final,
        lang_pair=lang_pair,
        ak=ak,
        model=model,
        ref_context=ref_context,
        fewshot_examples=fewshot_examples,
    )

    # 5) ç²—ç•¥æœ¯è¯­ä¸€è‡´æ€§æ£€æŸ¥(ä»¥â€œæœ€ç»ˆæ³¨å…¥â€çš„æœ¯è¯­ä¸ºå‡†)
    violated = check_term_consistency(tgt, terms_final, blk)

    return {
        "src": blk,
        "tgt": tgt,
        "project_id": project_id,
        "lang_pair": lang_pair,
        "term_map_all": term_map_all,
        "terms_in_block": terms_in_block,
        "terms_corpus_dyn": terms_corpus_dyn,
        "terms_final": terms_final,
        "term_meta": term_meta,
        "ref_context": ref_context,
        "violated_terms": violated,
    }

def _split_sentences_for_terms(text: str) -> list[str]:
    """ç”¨äºæœ¯è¯­ç¤ºä¾‹æŠ½å–çš„è½»é‡åˆ†å¥ï¼Œå…¼å®¹ä¸­è‹±æ–‡æ ‡ç‚¹ã€‚"""
    if not text:
        return []
    txt = _norm_text(text)
    if not txt:
        return []
    parts = re.split(r"(?<=[ã€‚ï¼ï¼Ÿï¼›.!?])\s+|\n+", txt)
    return [p.strip() for p in parts if p.strip()]


def _locate_example_pair(example: str | None, src_full: str | None, tgt_full: str | None):
    """
    åœ¨ç¿»è¯‘å†å²ä¸­ä¸ºç¤ºä¾‹å¥æ‰¾åˆ°å¯èƒ½çš„å¯¹é½è¯‘æ–‡ã€‚
    è¿”å› (src_example, tgt_example or None)ã€‚
    """
    if not example:
        return None, None

    ex = example.strip()
    if not ex:
        return None, None

    src_sents = split_sents(src_full or "", prefer_newline=True, min_char=2)
    tgt_sents = split_sents(tgt_full or "", prefer_newline=True, min_char=1)

    match_idx = None
    for i, s in enumerate(src_sents):
        if ex in s:
            match_idx = i
            break

    if match_idx is None:
        return ex, None

    tgt = tgt_sents[match_idx] if match_idx < len(tgt_sents) else None
    return ex, tgt or None


def extract_terms_with_corpus_model(
    text: str,
    *,
    max_terms: int = 30,
    src_lang: str = "zh",
    tgt_lang: str = "en",
    default_domain: str | None = None,
):
    """
    ä½¿ç”¨ä¸è¯­æ–™åº“å‘é‡æ£€ç´¢åŒä¸€å¥—æ¨¡å‹(distiluse-base-multilingual-cased-v1)åšæœ¯è¯­æå–ã€‚

    é€»è¾‘:
    1) å€ŸåŠ©æ­£åˆ™ä»æ–‡æœ¬ä¸­æŠ“å–ä¸­è‹±æœ¯è¯­å€™é€‰(2-8 å­—ä¸­æ–‡ã€1-3 è¯è‹±æ–‡çŸ­è¯­)ã€‚
    2) ç”¨ get_embedder() è¿”å›çš„å¥å‘é‡æ¨¡å‹å¯¹å…¨æ–‡å’Œå€™é€‰åšå‘é‡åŒ–ï¼ŒæŒ‰ç›¸ä¼¼åº¦é€‰å‡ºä»£è¡¨æ€§æœ¯è¯­ã€‚
    3) ç»“æ„åŒ–è¿”å›å­—æ®µä¸åŸ DeepSeek æç¤ºä¿æŒä¸€è‡´(source/target/domain/strategy/example)ã€‚
    """

    txt = (text or "").strip()
    if not txt:
        return []

    backend, encode = get_embedder()

    def _dedup_keep(seq):
        seen = set()
        out = []
        for x in seq:
            if x in seen:
                continue
            seen.add(x)
            out.append(x)
        return out

    zh_candidates = re.findall(r"[\u4e00-\u9fa5]{2,8}", txt)
    en_candidates = re.findall(r"[A-Za-z][A-Za-z\-]{2,}(?: [A-Za-z\-]{2,}){0,2}", txt)
    candidates = _dedup_keep(zh_candidates + en_candidates)
    if not candidates:
        return []

    doc_emb = encode([txt])[0]
    cand_emb = encode(candidates)
    scores = cand_emb @ doc_emb

    ranked = sorted(zip(candidates, scores.tolist()), key=lambda x: x[1], reverse=True)[:max_terms]
    sents = _split_sentences_for_terms(txt)

    def _example_for(term: str):
        for s in sents:
            if term in s:
                return s
        return None

    out = []
    domain_val = (default_domain or "").strip() or "å…¶ä»–"

    for term, sc in ranked:
        out.append(
            {
                "source_term": term,
                # ç°é˜¶æ®µç¼ºå°‘ç»Ÿä¸€çš„è‡ªåŠ¨è¯‘æ³•ï¼Œä¿æŒå­—æ®µé½å…¨ä»¥ä¾¿åç»­äººå·¥/æ¨¡å‹è¡¥å…¨
                "target_term": None,
                "domain": domain_val,
                "strategy": None,
                "example": _example_for(term),
                "score": float(sc),
                "model": backend,
                "src_lang": src_lang,
                "tgt_lang": tgt_lang,
            }
        )
    return out


def ds_extract_terms(
    text: str,
    ak: str,
    model: str,
    src_lang: str = "zh",
    tgt_lang: str = "en",
    *,
    prefer_corpus_model: bool = True,
    default_domain: str | None = None,
):
    """æœ¯è¯­æå–ï¼šä¼˜å…ˆèµ°è¯­æ–™åº“åŒæ¬¾å‘é‡æ¨¡å‹ï¼Œå¤±è´¥æ—¶å†å›é€€ DeepSeek Promptã€‚"""

    txt = (text or "").strip()
    if not txt:
        return []

    if prefer_corpus_model:
        try:
            return extract_terms_with_corpus_model(
                txt,
                max_terms=30,
                src_lang=src_lang,
                tgt_lang=tgt_lang,
                default_domain=default_domain,
            )
        except Exception as e:
            log_event("ERROR", "corpus-model term extraction failed", error=str(e))

    if not ak:
        return []

    import requests

    system_msg = (
        "You are a terminology mining assistant. Extract high-value bilingual term pairs suitable for a project glossary. "
        "Return JSON array only. No extra text."
    )
    user_msg = f"""
Source language: {src_lang}
Target language: {tgt_lang}
ä»»åŠ¡:ä»ç»™å®šæ–‡æœ¬ä¸­æŠ½å–åŒè¯­æœ¯è¯­æ¡ç›®.è¾“å‡º JSON æ•°ç»„ã€‚å­—æ®µåä¸å–å€¼å¿…é¡»æ˜¯ä¸­æ–‡ã€‚
å­—æ®µå®šä¹‰:
- source_term: æºè¯­(ä¸­æ–‡æœ¯è¯­æˆ–ä¸“å)
- target_term: è¯‘æ–‡(è‹±æ–‡)
- domain: é¢†åŸŸ.å–å€¼é›†åˆä¹‹ä¸€:["æ”¿æ²»","ç»æµ","æ–‡åŒ–","æ–‡ç‰©","é‡‘è","æ³•å¾‹","å…¶ä»–"]
- strategy: ç¿»è¯‘ç­–ç•¥.å–å€¼é›†åˆä¹‹ä¸€:["ç›´è¯‘","æ„è¯‘","è½¬è¯‘","éŸ³è¯‘","çœç•¥","å¢è¯‘","è§„èŒƒåŒ–","å…¶ä»–"]
- example: ä¾‹å¥(åŸæ–‡ä¸­åŒ…å«è¯¥æœ¯è¯­çš„ä¸€å¥.å°½é‡ä¿ç•™æ ‡ç‚¹)

è¦æ±‚:
1) ä»…è¾“å‡º JSON.ä¸è¦å¤šä½™è¯´æ˜ã€‚
2) åŒä¸€æœ¯è¯­é‡å¤æ—¶åˆå¹¶.é€‰æ‹©æœ€å…¸å‹çš„ä¾‹å¥ã€‚
3) è‹¥æ— æ³•åˆ¤æ–­ domain/strategy.å¡«â€œå…¶ä»–â€ã€‚

Text:
{text}
"""

    url = "https://api.deepseek.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {ak}", "Content-Type": "application/json"}
    payload = {
        "model": model or "deepseek-chat",
        "messages": [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        "temperature": 0.1,
    }
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=60)
        r.raise_for_status()
        data = r.json()
        txt = data["choices"][0]["message"]["content"].strip()
        # åªä¿ç•™ JSON ç‰‡æ®µ
        start = txt.find("[")
        end = txt.rfind("]")
        if start == -1 or end == -1:
            return []
        arr = json.loads(txt[start:end+1])
        out = []
        for o in arr:
            src = (o.get("source_term") or o.get("source") or "").strip()
            tgt = (o.get("target_term") or o.get("target") or "").strip()
            dom = (o.get("domain") or "").strip() or (default_domain or None)
            strat = (o.get("strategy") or "").strip() or None
            ex = (o.get("example") or "").strip() or None
            if src:
                out.append({"source_term": src, "target_term": tgt, "domain": dom, "strategy": strat, "example": ex})
        return out
    except Exception:
        return []

# ========== æ–‡ä»¶è¯»å†™ä¸å¯¼å‡º ==========
def read_source_file(path: str) -> str:
    if not path or not os.path.exists(path):
        return ""
    ext = os.path.splitext(path)[1].lower()
    if ext == ".txt":
        for enc in ["utf-8", "utf-8-sig", "gb18030", "gbk"]:
            try:
                with open(path, "r", encoding=enc) as f:
                    return f.read()
            except Exception:
                continue
        with open(path, "r", errors="ignore") as f:
            return f.read()
    elif ext == ".docx":
        try:
            from docx import Document
            doc = Document(path)
            return "\n".join(p.text for p in doc.paragraphs)
        except Exception:
            return ""
    elif ext == ".xlsx":
        try:
            xls = pd.ExcelFile(path)
            parts = []
            for sheet in xls.sheet_names:
                df = xls.parse(sheet)
                parts.append(df.astype(str).to_csv(sep=" ", index=False, header=False))
            return "\n".join(parts)
        except Exception:
            return ""
    else:
        # å…œåº•å°è¯•æ–‡æœ¬è¯»å–
        try:
            with open(path, "r", encoding="utf-8") as f:
                return f.read()
        except Exception:
            return ""

def build_bilingual_lines(src_text: str, tgt_text: str):
    """
    ç”¨æ®µè½åšå¯¹é½:
    - æ¯ä¸€æ®µä¸­æ–‡å¯¹åº”ä¸€æ®µè‹±æ–‡
    - æ®µå†…ä¸å†æ‹†å¥(é¿å… CSV / Word é”™ä½)
    """
    return pair_paragraphs(src_text, tgt_text)

def export_csv_bilingual(src_text: str, tgt_text: str) -> bytes:
    s, t = build_bilingual_lines(src_text, tgt_text)
    df = pd.DataFrame({"Source": s, "Target": t})
    buf = io.StringIO()
    df.to_csv(buf, index=False)
    return buf.getvalue().encode("utf-8-sig")

def export_docx_bilingual(src_text: str, tgt_text: str) -> bytes:
    try:
        from docx import Document
        from docx.oxml.ns import qn
    except Exception:
        st.error("ç¼ºå°‘ python-docx.è¯·å…ˆå®‰è£…:pip install python-docx")
        return b""
    doc = Document()
    # åŸºç¡€å­—ä½“
    try:
        doc.styles['Normal'].font.name = 'Calibri'
        doc.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), 'å¾®è½¯é›…é»‘')
    except Exception:
        pass
    s, t = build_bilingual_lines(src_text, tgt_text)
    table = doc.add_table(rows=1, cols=2)
    hdr = table.rows[0].cells
    hdr[0].text = "Source"
    hdr[1].text = "Target"
    for a, b in zip(s, t):
        row = table.add_row().cells
        row[0].text = a
        row[1].text = b
    bio = io.BytesIO()
    doc.save(bio)
    return bio.getvalue()

# ==== å·¥å…·:æ–‡ä»¶è¯»å– / åˆ†å¥ / å‘é‡ / å¯¹é½ / ç´¢å¼• ====
import os, re, io, json, numpy as _np

def _lazy_import_doc_pdf():
    docx = pdfplumber = None
    try:
        import docx as _docx
        docx = _docx
    except Exception:
        pass
    try:
        import pdfplumber as _pdfplumber
        pdfplumber = _pdfplumber
    except Exception:
        pass
    return docx, pdfplumber
# æ®µè½åˆ‡åˆ†
def split_paragraphs(text: str) -> list[str]:
    """
    æ®µè½åˆ‡åˆ†(ç”¨äºç¿»è¯‘ & å¯¼å‡º):
    - ç»Ÿä¸€æ¢è¡Œç¬¦
    - ä»¥ã€è‡³å°‘ä¸€ä¸ªç©ºè¡Œã€‘ä½œä¸ºæ®µè½åˆ†éš”
    - æ®µå†…ä¿ç•™å¥å­ï¼Œåªå»æ‰çº¯ç©ºè¡Œ
    """
    text = (text or "").replace("\r\n", "\n").replace("\r", "\n")
    # å¸¸è§æƒ…å†µ:ç”¨â€œä¸€è¡Œä¸€æ®µâ€çš„ç¨¿å­ï¼Œå®é™…ä¸Šä¸­é—´æ²¡æœ‰ç©ºè¡Œ
    # è¿™ç§å°±æŒ‰å•è¡Œå½“ä½œæ®µè½
    if "\n\n" not in text and "\n \n" not in text:
        lines = [ln.strip() for ln in text.split("\n")]
        return [ln for ln in lines if ln]

    # æ­£å¸¸:æœ‰ç©ºè¡Œåˆ†æ®µ
    parts = re.split(r"\n\s*\n+", text)
    paras = []
    for p in parts:
        p = p.strip()
        if not p:
            continue
        # æ®µå†…å¦‚æœè¿˜æœ‰è½¯æ¢è¡Œï¼Œå‹æˆç©ºæ ¼ï¼Œé¿å…å¯¼å‡ºæ—¶è¢«æ‹†æˆå¤šè¡Œ
        p = re.sub(r"\s*\n\s*", " ", p)
        paras.append(p)
    return paras

def pair_paragraphs(src_full: str, tgt_full: str) -> tuple[list[str], list[str]]:
    """
    æ ¹æ®å…¨æ–‡ä¸­è‹±ï¼ŒæŒ‰â€œæ®µè½â€é…å¯¹:
    - æºæ–‡/è¯‘æ–‡å„è‡ªåš split_paragraphs
    - è¡Œæ•°ä¸ä¸€è‡´æ—¶ç”¨ç©ºä¸²è¡¥é½
    - ä¿è¯å¯¼å‡ºæ—¶ä¸€è¡Œä¸­æ–‡å¯¹åº”ä¸€è¡Œè‹±æ–‡
    """
    src_paras = split_paragraphs(src_full or "")
    tgt_paras = split_paragraphs(tgt_full or "")

    n = max(len(src_paras), len(tgt_paras))
    src_paras += [""] * (n - len(src_paras))
    tgt_paras += [""] * (n - len(tgt_paras))
    return src_paras, tgt_paras

# é¢„ç¼–è¯‘(å¯æ”¾å…¨å±€)
_RE_WS = re.compile(r"[ \t\u00A0\u200B\u200C\u200D]+")
_RE_ZH_SENT = re.compile(r"(?<=[ã€‚ï¼ï¼Ÿï¼›])\s*")           # ä¸­æ–‡å¥æœ«
_RE_EN_SENT = re.compile(r"(?<=[\.\?\!;:])\s+")          # è‹±æ–‡å¥æœ«(æ”¾å®½ï¼Œä¸å¼ºåˆ¶å¤§å†™)
_RE_BLANK_PARA = re.compile(r"\n{2,}")                   # ç©ºè¡Œåˆ†æ®µ

def _norm_text(text: str) -> str:
    t = (text or "").replace("\r\n", "\n").replace("\r", "\n").replace("\x0b", "\n")
    t = _RE_WS.sub(" ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)  # è¿‡å¤šç©ºè¡Œå‹åˆ°ä¸¤ä¸ª
    return t.strip()

def _is_zh(text: str) -> bool:
    # ç®€å•åˆ¤å®š:å«æœ‰è¾ƒå¤šä¸­æ–‡å­—ç¬¦
    zh_hits = len(re.findall(r"[\u4e00-\u9fff]", text))
    en_hits = len(re.findall(r"[A-Za-z]", text))
    return zh_hits >= en_hits

def split_sents(
    text: str,
    lang_hint: str = "auto",
    min_char: int = 1,
    prefer_newline: bool = True,
    **kwargs,
):
    """
    ç»Ÿä¸€çš„åˆ†å¥/åˆ†æ®µå‡½æ•°:
    - å…¼å®¹æ—§è°ƒç”¨:split_sents(text, lang="zh")
    - æ”¯æŒæ–°å‚æ•°:prefer_newline=True æ—¶ï¼Œä¼˜å…ˆæŒ‰æ¢è¡Œåˆ‡
    """
    # å…¼å®¹æ—§å‚æ•°å lang=
    lang = kwargs.get("lang", lang_hint)

    t = _norm_text(text)
    if not t:
        return []

    pieces = []

    # A) è‹¥æ–‡æœ¬ä¸­æœ‰æ¢è¡Œ & prefer_newline=True:å…ˆæŒ‰è¡Œåˆ‡ï¼Œå†åœ¨è¡Œå†…æŒ‰å¥æœ«ç»†åˆ†
    if prefer_newline and ("\n" in t):
        lines = [ln.strip() for ln in t.split("\n") if ln.strip()]
        for ln in lines:
            if lang == "auto":
                cur_lang = "zh" if _is_zh(ln) else "en"
            else:
                cur_lang = lang

            if cur_lang.startswith(("zh", "cn")):
                sents = [s.strip() for s in _RE_ZH_SENT.split(ln) if s and s.strip()]
            else:
                sents = [s.strip() for s in _RE_EN_SENT.split(ln) if s and s.strip()]

            pieces.extend(sents if sents else [ln])
    else:
        # B) æ²¡æœ‰æ¢è¡Œæˆ–ä¸åå¥½æ¢è¡Œ:æ•´å—æŒ‰å¥æœ«æ ‡ç‚¹åˆ‡
        if lang == "auto":
            cur_lang = "zh" if _is_zh(t) else "en"
        else:
            cur_lang = lang

        if cur_lang.startswith(("zh", "cn")):
            pieces = [s.strip() for s in _RE_ZH_SENT.split(t) if s and s.strip()]
        else:
            pieces = [s.strip() for s in _RE_EN_SENT.split(t) if s and s.strip()]

        if not pieces:
            pieces = [t]

    # è¿‡æ»¤è¿‡çŸ­ç‰‡æ®µ
    return [x for x in pieces if len(x) >= min_char]

# å…¼å®¹æ—§å‡½æ•°å
split_sentences = split_sents

def split_blocks(text: str, max_len: int = 1200):
    blocks, curbuf = [], ""
    for line in text.splitlines(True):
        if len(curbuf) + len(line) > max_len:
            if curbuf:
                blocks.append(curbuf)
            curbuf = ""
        curbuf += line
    if curbuf:
        blocks.append(curbuf)
    return blocks
# =========================
# æœ¯è¯­æç¤º & è¯‘åä¸€è‡´æ€§æ£€æŸ¥
# =========================
def check_term_consistency(out_text: str, term_dict: dict, source_text: str = "") -> list:
    """
    è¯‘åçš„ä¸€è‡´æ€§ç²—æ£€:å¦‚æœæºæ–‡åŒ…å«æœ¯è¯­é”®.ä½†è¯‘æ–‡æœªå‡ºç°å¯¹åº”å€¼.åˆ™è®°å½•æé†’ã€‚
    ä»…åšæœ€å°ä»£ä»·çš„å­—ç¬¦ä¸²çº§æ£€æŸ¥(ä¸æ”¹åŠ¨åŸè¯‘æ–‡)ã€‚
    è¿”å›å½¢å¦‚ ["contractâ†’åˆåŒ", ...] çš„åˆ—è¡¨;ä¸ºç©ºè¡¨ç¤ºå…¨éƒ¨ç¬¦åˆ/ä¸é€‚ç”¨ã€‚
    """
    if not out_text or not term_dict:
        return []
    warns = []
    s = (source_text or "")[:2000]  # é™é•¿åº¦.é˜²æç«¯é•¿æ–‡æœ¬
    out_low = out_text.lower()
    for k, v in term_dict.items():
        if not k or not v:
            continue
        # å¦‚æœæºæ–‡åŒ…å«è¯¥æœ¯è¯­é”®(å¤§å°å†™å¿½ç•¥è‹±æ–‡;ä¸­æ–‡ç›´æ¥åŒ…å«)
        hit_src = (k.lower() in s.lower()) if any(ord(ch) < 128 for ch in k) else (k in s)
        if hit_src:
            # è¯‘æ–‡æ˜¯å¦å‡ºç°ç›®æ ‡æœ¯è¯­(åŒç†å¤§å°å†™å®¹å¿è‹±æ–‡)
            ok = (v.lower() in out_low) if any(ord(ch) < 128 for ch in v) else (v in out_text)
            if not ok:
                warns.append(f"{k}â†’{v}")
    return warns

def _lazy_embedder():
    # ä¼˜å…ˆ sentence-transformers;å¤±è´¥é€€åŒ–åˆ° TF-IDF
    try:
        from sentence_transformers import SentenceTransformer
        import numpy as np
        mdl = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        def _emb(texts):
            arr = mdl.encode(texts, normalize_embeddings=True)
            return arr.astype("float32")
        return _emb, "sbert"
    except Exception:
        from sklearn.feature_extraction.text import TfidfVectorizer
        import numpy as np
        def _emb(texts):
            vec = TfidfVectorizer(min_df=1).fit_transform(texts)
            # å½’ä¸€åŒ–
            norms = _np.sqrt((vec.multiply(vec)).sum(axis=1)).A.ravel() + 1e-8
            vec = vec.multiply(1/norms[:,None])
            return vec
        return _emb, "tfidf"
def import_corpus_from_upload(
    st,
    cur,
    conn,
    *,
    pid: int | None,
    title: str | None,
    lp: str,
    pairs,
    src_text: str | None,
    tgt_text: str | None,
    default_title: str = "",
    build_after_import: bool = False,
):
    """
    ç»Ÿä¸€çš„â€œä¸Šä¼ è¯­æ–™ â†’ å†™å…¥ corpus â†’ å¯é€‰é‡å»ºç´¢å¼•â€ç®¡çº¿ã€‚

    å‚æ•°ï¼š
    - pid: å…³è”é¡¹ç›® IDï¼Œå¯ä¸ºç©º
    - title: ç”¨æˆ·åœ¨ç•Œé¢å¡«çš„æ ‡é¢˜
    - lp: æ–¹å‘ï¼Œå¦‚ "ä¸­è¯‘è‹±" / "è‹±è¯‘ä¸­" / "è‡ªåŠ¨"
    - pairs: å·²ç»å¯¹é½å¥½çš„ [(src, tgt)] æˆ– [(src, tgt, score)]
    - src_text, tgt_text: åªæœ‰å•è¯­æ—¶ç”¨ src_textï¼Œå¤šè¯­å¯¹ç…§å·²ç»åˆæˆ pairs æ—¶å¯ä»¥ä¸º None
    - default_title: è‹¥ title ä¸ºç©ºæ—¶ç”¨çš„å…œåº•æ ‡é¢˜ï¼ˆæ–‡ä»¶åï¼‰
    - build_after_import: æ˜¯å¦åœ¨å†™å…¥åé‡å»ºè¯¥é¡¹ç›®çš„è¯­ä¹‰ç´¢å¼•
    """
    # ç»Ÿä¸€æ ‡é¢˜
    base_title = (title or default_title or "").strip() or "æœªå‘½åè¯­æ–™"

    # å°å·¥å…·ï¼šæŠŠ [(s,t,score)] / [(s,t)] ç»Ÿä¸€æˆ [(s,t)]
    def normalize_pairs_to2(pairs_in):
        if not pairs_in:
            return []
        if len(pairs_in[0]) == 3:
            return [(s, t) for (s, t, _) in pairs_in]
        return pairs_in

    # 1) åŒè¯­ pairs æƒ…å†µ
    if pairs:
        pairs2 = normalize_pairs_to2(pairs)
        ins = 0
        for s, t in pairs2:
            s = (s or "").strip()
            t = (t or "").strip()
            if not (s or t):
                continue
            cur.execute(
                """
                INSERT INTO corpus(title, project_id, lang_pair, src_text, tgt_text, note, created_at)
                VALUES (?, ?, ?, ?, ?, ?, datetime('now'))
                """,
                (
                    base_title,
                    pid,
                    lp,
                    s or None,
                    t or None,
                    "auto-import",
                ),
            )
            ins += 1
        conn.commit()
        st.success(f"âœ… å·²å†™å…¥è¯­æ–™åº“ {ins} æ¡ã€‚")

        # å¯é€‰ï¼šå¯¼å…¥åé‡å»ºå½“å‰é¡¹ç›®è¯­ä¹‰ç´¢å¼•
        if build_after_import and pid:
            res_idx = rebuild_project_semantic_index(pid)
            if res_idx.get("ok"):
                st.success(
                    f"ğŸ§  å‘é‡ç´¢å¼•å·²æ›´æ–°: æ–°å¢ {res_idx['added']}ï¼Œæ€»é‡ {res_idx['total']}ã€‚"
                )
            else:
                st.warning(f"ç´¢å¼•æœªæ›´æ–°: {res_idx.get('msg','æœªçŸ¥é”™è¯¯')}")

        return

    # 2) å•è¯­ src_text æƒ…å†µï¼ˆç­–ç•¥/å•è¯­è¯­æ–™ï¼‰
    if src_text and not tgt_text:
        lang_hint = "zh" if (lp or "").startswith("ä¸­") else "en"
        sents = split_sents(src_text, lang_hint)
        ins = 0
        for s in sents:
            s = (s or "").strip()
            if not s:
                continue
            cur.execute(
                """
                INSERT INTO corpus(title, project_id, lang_pair, src_text, tgt_text, note, created_at)
                VALUES (?, ?, ?, ?, NULL, ?, datetime('now'))
                """,
                (
                    base_title,
                    pid,
                    lp,
                    s,
                    "mono",  # æ ‡è®°ï¼šå•è¯­ç­–ç•¥/è¯­æ–™
                ),
            )
            ins += 1
        conn.commit()
        st.success(f"âœ… å·²å†™å…¥è¯­æ–™åº“ {ins} æ¡ã€‚")

        if build_after_import and pid:
            res_idx = rebuild_project_semantic_index(pid)
            if res_idx.get("ok"):
                st.success(
                    f"ğŸ§  å‘é‡ç´¢å¼•å·²æ›´æ–°: æ–°å¢ {res_idx['added']}ï¼Œæ€»é‡ {res_idx['total']}ã€‚"
                )
            else:
                st.warning(f"ç´¢å¼•æœªæ›´æ–°: {res_idx.get('msg','æœªçŸ¥é”™è¯¯')}")
        return

    # 3) å…¶ä»–æƒ…å†µï¼šå•¥éƒ½æ²¡æœ‰ï¼Œç»™å‡ºæé†’
    st.warning("åŸæ–‡å’Œè¯‘æ–‡éƒ½ä¸ºç©ºï¼Œæ— æ³•å†™å…¥è¯­æ–™åº“ã€‚")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def align_semantic(src_sents, tgt_sents, max_jump=3):
    """ç®€å•è´ªå¿ƒ + æ»‘çª—çš„ 1-1 å¥å¯¹é½.è¿”å› [(src, tgt, score)]"""
    if not src_sents or not tgt_sents:
        return []

    emb, kind = _lazy_embedder()

    # === ä¼˜å…ˆä½¿ç”¨ SBERT ===
    if kind == "sbert":
        E1 = emb(src_sents)
        E2 = emb(tgt_sents)
        sims = E1 @ E2.T  # (n, m)
    else:
        # === TF-IDF å›é€€:ç¡®ä¿åŒä¸€è¯è¡¨ç»´åº¦ ===
        vec = TfidfVectorizer(
            analyzer="char_wb",  # å­—ç¬¦ n-gram å¯¹ä¸­è‹±æ··åˆæœ€ç¨³
            ngram_range=(1, 2),
            min_df=1
        )
        combo = src_sents + tgt_sents
        X = vec.fit_transform(combo)
        n = len(src_sents)
        E1 = X[:n, :]
        E2 = X[n:, :]

        # ç¨€ç–â†’ç›¸ä¼¼åº¦çŸ©é˜µ
        sims = cosine_similarity(E1, E2, dense_output=True)  # shape (n, m)

    # === è´ªå¿ƒå¯¹é½ ===
    i = j = 0
    n, m = len(src_sents), len(tgt_sents)
    pairs = []
    while i < n and j < m:
        j_min = max(0, j - max_jump)
        j_max = min(m, j + max_jump + 1)
        window = sims[i, j_min:j_max]
        if window.size == 0:
            break
        k = int(window.argmax())
        j_sel = j_min + k
        score = float(sims[i, j_sel])
        pairs.append((src_sents[i], tgt_sents[j_sel], score))
        i += 1
        j = j_sel + 1
    return pairs

# ========== æœ¯è¯­åº“ç®¡ç† ==========
def render_term_management(st, cur, conn, base_dir, key_prefix="term"):
    sk = make_sk(key_prefix)

    st.subheader("ğŸ“˜ æœ¯è¯­åº“ç®¡ç†")
    sub_tabs = st.tabs(["æŸ¥è¯¢ä¸ç¼–è¾‘", "æ‰¹é‡å¯¼å…¥ CSV", "ç»Ÿè®¡ä¸å¯¼å‡º", "å¿«é€Ÿæœç´¢", "æ‰¹é‡æŒ‚æ¥é¡¹ç›®", "ä»å†å²æå–æœ¯è¯­", "åˆ†ç±»ç®¡ç†"])

    # â€”â€” æŸ¥è¯¢ä¸ç¼–è¾‘
    with sub_tabs[0]:
        sk0 = lambda name: f"{key_prefix}_t0_{name}"

        c1, c2, c3, c4 = st.columns(4)
        with c1: kw = st.text_input("å…³é”®è¯(æº/ç›®æ ‡/ä¾‹å¥)", "", key=sk("kw_example"))
        with c2: dom = st.text_input("é¢†åŸŸ", "", key=sk("dom"))
        with c3: strat = st.text_input("ç­–ç•¥", "", key=sk("strat"))
        with c4: pid = st.text_input("é¡¹ç›®IDè¿‡æ»¤", "", key=sk("pid"))
        cat = st.text_input("åˆ†ç±»(æ”¯æŒå­ä¸²)", "", key=sk("cat"))

        # â€”â€” å…¼å®¹è€åº“:å¦‚æ—  category åˆ—åˆ™è¡¥åˆ—(å·²æœ‰ä¼šå¿½ç•¥)
        try:
            cur.execute("ALTER TABLE term_ext ADD COLUMN category TEXT;")
            conn.commit()
        except Exception:
            pass

        # 1) ä»¥æ•°æ®åº“çœŸå®åˆ—ä¸ºå‡†æ£€æµ‹æ˜¯å¦å­˜åœ¨ category
        cols_db = [c[1].lower() for c in cur.execute("PRAGMA table_info(term_ext);").fetchall()]
        has_category = ("category" in cols_db)

        # 2) æ‹¼ SQL(åªåœ¨ DB çœŸçš„æœ‰è¯¥åˆ—æ—¶æ‰ SELECT category)
        base_cols = "id, source_term, target_term, domain, project_id, strategy, example"
        sel_cols = base_cols + (", category" if has_category else "")
        sql = f"SELECT {sel_cols} FROM term_ext WHERE 1=1"
        params = []

        if kw:
            like = f"%{kw}%"
            sql += " AND (IFNULL(source_term,'') LIKE ? OR IFNULL(target_term,'') LIKE ? OR IFNULL(example,'') LIKE ?)"
            params += [like, like, like]

        if dom:
            sql += " AND IFNULL(domain,'') LIKE ?"
            params += [f"%{dom}%"]

        if strat:
            sql += " AND IFNULL(strategy,'') LIKE ?"
            params += [f"%{strat}%"]

        if pid and str(pid).isdigit():
            sql += " AND IFNULL(project_id,0) = ?"
            params += [int(pid)]

        if has_category and cat:
            sql += " AND IFNULL(category,'') LIKE ?"
            params += [f"%{cat}%"]

        sql += " ORDER BY source_term COLLATE NOCASE LIMIT 1000"

        # 3) æŸ¥è¯¢å¹¶æ„é€  DataFrame(è¡¨å¤´ä¸å®é™…åˆ—å¯¹é½)
        rows = cur.execute(sql, params).fetchall()
        headers = ["ID","æºæœ¯è¯­","ç›®æ ‡æœ¯è¯­","é¢†åŸŸ","é¡¹ç›®ID","ç­–ç•¥","ä¾‹å¥"]
        if has_category:
            headers.append("åˆ†ç±»")

        df = pd.DataFrame(rows, columns=headers)
        st.caption(f"å½“å‰æŸ¥è¯¢è¿”å›:{len(df)} æ¡")

        # 4) ç©ºæ•°æ®å°±ä¸æ¸²æŸ“ç¼–è¾‘å™¨
        if df.empty:
            st.info("æ²¡æœ‰åŒ¹é…çš„æœ¯è¯­ã€‚")
        else:
            # === ç”¨ session_state ç»´æŠ¤â€œå½“å‰è¡¨æ ¼â€ï¼Œå«é€‰æ‹©åˆ— ===
            editor_df_key = sk0("editor_df")   # ä¸“é—¨å­˜ DataFrame
            editor_key    = sk0("editor")      # data_editor å°éƒ¨ä»¶æœ¬èº«

            # åˆå§‹åŒ– / å°ºå¯¸å˜åŒ–æ—¶é‡ç½®
            if editor_df_key not in st.session_state:
                work_df = df.copy()
                if "sel" not in work_df.columns:
                    work_df.insert(0, "sel", False)
                st.session_state[editor_df_key] = work_df
            else:
                work_df = st.session_state[editor_df_key]
                if len(work_df) != len(df):
                    work_df = df.copy()
                    if "sel" not in work_df.columns:
                        work_df.insert(0, "sel", False)
                    st.session_state[editor_df_key] = work_df

            # åŠ¨æ€æ„å»ºåˆ—é…ç½®.åªæœ‰å½“ DB çœŸæœ‰â€œåˆ†ç±»â€æ—¶æ‰åŠ å…¥
            col_cfg = {
                "ID": st.column_config.NumberColumn("ID", disabled=True),
                "sel": st.column_config.CheckboxColumn("é€‰æ‹©"),
                "æºæœ¯è¯­": "æºæœ¯è¯­",
                "ç›®æ ‡æœ¯è¯­": "ç›®æ ‡æœ¯è¯­",
                "é¢†åŸŸ": "é¢†åŸŸ",
                "é¡¹ç›®ID": st.column_config.NumberColumn("é¡¹ç›®ID", step=1, required=False),
                "ç­–ç•¥": "ç­–ç•¥",
                "ä¾‹å¥": st.column_config.TextColumn("ä¾‹å¥"),
            }
            if has_category:
                col_cfg["åˆ†ç±»"] = st.column_config.TextColumn("åˆ†ç±»")

            # çœŸæ­£çš„ç¼–è¾‘å™¨:ä»¥ session_state é‡Œçš„ DataFrame ä¸ºå‡†
            edited = st.data_editor(
                st.session_state[editor_df_key],
                num_rows="dynamic",
                key=editor_key,
                column_config=col_cfg,
            )
            # æŠŠç”¨æˆ·è¿™æ¬¡ç¼–è¾‘ç»“æœå†™å› session_state
            st.session_state[editor_df_key] = edited

            c1, c2, c3 = st.columns([1, 1, 2])

            # ---------------- ä¿å­˜ä¿®æ”¹ ----------------
            with c1:
                if st.button("ğŸ’¾ ä¿å­˜ä¿®æ”¹", type="primary", key=sk("save_terms")):
                    updated = inserted = 0
                    for _, row in edited.iterrows():
                        if pd.notna(row["ID"]):  # æ›´æ–°
                            if has_category:
                                cur.execute("""
                                    UPDATE term_ext
                                    SET source_term=?, target_term=?, domain=?, project_id=?, strategy=?, example=?, category=?
                                    WHERE id=?;
                                """, (
                                    (row["æºæœ¯è¯­"] or "").strip(),
                                    (row["ç›®æ ‡æœ¯è¯­"] or None),
                                    (row["é¢†åŸŸ"] or None),
                                    (int(row["é¡¹ç›®ID"]) if pd.notna(row["é¡¹ç›®ID"]) else None),
                                    (row["ç­–ç•¥"] or None),
                                    (row["ä¾‹å¥"] or None),
                                    (row.get("åˆ†ç±»") or None),
                                    int(row["ID"])
                                ))
                            else:
                                cur.execute("""
                                    UPDATE term_ext
                                    SET source_term=?, target_term=?, domain=?, project_id=?, strategy=?, example=?
                                    WHERE id=?;
                                """, (
                                    (row["æºæœ¯è¯­"] or "").strip(),
                                    (row["ç›®æ ‡æœ¯è¯­"] or None),
                                    (row["é¢†åŸŸ"] or None),
                                    (int(row["é¡¹ç›®ID"]) if pd.notna(row["é¡¹ç›®ID"]) else None),
                                    (row["ç­–ç•¥"] or None),
                                    (row["ä¾‹å¥"] or None),
                                    int(row["ID"])
                                ))
                            updated += cur.rowcount
                        else:  # æ–°å¢
                            if str(row["æºæœ¯è¯­"]).strip():
                                if has_category:
                                    cur.execute("""
                                        INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example, category)
                                        VALUES (?, ?, ?, ?, ?, ?, ?)
                                    """, (
                                        (row["æºæœ¯è¯­"] or "").strip(),
                                        (row["ç›®æ ‡æœ¯è¯­"] or None),
                                        (row["é¢†åŸŸ"] or None),
                                        (int(row["é¡¹ç›®ID"]) if pd.notna(row["é¡¹ç›®ID"]) else None),
                                        (row["ç­–ç•¥"] or None),
                                        (row["ä¾‹å¥"] or None),
                                        (row.get("åˆ†ç±»") or None)
                                    ))
                                else:
                                    cur.execute("""
                                        INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example)
                                        VALUES (?, ?, ?, ?, ?, ?)
                                    """, (
                                        (row["æºæœ¯è¯­"] or "").strip(),
                                        (row["ç›®æ ‡æœ¯è¯­"] or None),
                                        (row["é¢†åŸŸ"] or None),
                                        (int(row["é¡¹ç›®ID"]) if pd.notna(row["é¡¹ç›®ID"]) else None),
                                        (row["ç­–ç•¥"] or None),
                                        (row["ä¾‹å¥"] or None),
                                    ))
                                inserted += 1
                    conn.commit()
                    st.success(f"âœ… å·²ä¿å­˜:æ›´æ–° {updated} æ¡.æ–°å¢ {inserted} æ¡ã€‚")
                    st.rerun()

            # ---------------- å…¨é€‰ / æ¸…ç©º / åˆ é™¤ ----------------
            with c2:
                cc2a, cc2b, cc2c = st.columns([1, 1, 2])
                # è¿™é‡Œç»Ÿä¸€æ“ä½œ session_state é‡Œçš„ DataFrame
                cur_df = st.session_state[editor_df_key]

                if cc2a.button("å…¨é€‰", key=sk("sel_all")):
                    cur_df.loc[:, "sel"] = True
                    st.session_state[editor_df_key] = cur_df
                    st.rerun()

                if cc2b.button("æ¸…ç©º", key=sk("sel_clear")):
                    cur_df.loc[:, "sel"] = False
                    st.session_state[editor_df_key] = cur_df
                    st.rerun()

                if cc2c.button("ğŸ—‘ï¸ åˆ é™¤å·²å‹¾é€‰", key=sk("del_sel")):
                    to_delete = cur_df[(cur_df["sel"] == True) & pd.notna(cur_df["ID"])]["ID"].astype(int).tolist()
                    if not to_delete:
                        st.warning("æœªå‹¾é€‰ä»»ä½•è®°å½•")
                    else:
                        cur.executemany("DELETE FROM term_ext WHERE id=?", [(i,) for i in to_delete])
                        conn.commit()
                        st.success(f"ğŸ—‘ï¸ å·²åˆ é™¤ {len(to_delete)} æ¡")
                        st.rerun()

                with c3:
                    proj_opts = cur.execute(
                        "SELECT id, title FROM items WHERE COALESCE(type,'')='project' ORDER BY id DESC"
                    ).fetchall()
                    proj_map = {"(ä¸æŒ‚æ¥/ç½®ç©º)": None, **{f"#{i} {t}": i for (i, t) in proj_opts}}

                    cc3a, cc3b = st.columns([2, 1])
                    target_proj_label = cc3a.selectbox("æ‰¹é‡æŒ‚æ¥åˆ°é¡¹ç›®", list(proj_map.keys()), key=sk("bind_proj_sel"))
                    if cc3b.button("æ‰§è¡ŒæŒ‚æ¥", type="primary", key=sk("bind_apply")):
                        if "ID" in edited.columns:
                            to_update = edited[(edited["sel"] == True) & pd.notna(edited["ID"])]["ID"].astype(int).tolist()
                        else:
                            to_update = []
                        if not to_update:
                            st.warning("æœªå‹¾é€‰ä»»ä½•è®°å½•")
                        else:
                            pid_val = proj_map.get(target_proj_label)
                            q_marks = ",".join("?" for _ in to_update)
                            cur.execute(f"UPDATE term_ext SET project_id=? WHERE id IN ({q_marks})", (pid_val, *to_update))
                            conn.commit()
                            st.success(f"âœ… å·²æŒ‚æ¥ {len(to_update)} æ¡åˆ°é¡¹ç›®:{target_proj_label or '(ç©º)'}")
                            st.rerun()

            st.markdown("### å•æ¡æ–°å¢ / ç¼–è¾‘")
            with st.form(sk0("term_edit")):
                col1, col2, col3 = st.columns(3)
                with col1:
                    rid_edit = st.text_input("è¦ç¼–è¾‘çš„è®°å½• ID(ç•™ç©ºåˆ™æ–°å¢)", "", key=sk("rid_edit"))
                    source_term = st.text_input("æºè¯­è¨€æœ¯è¯­(å¿…å¡«)*", key=sk("source_term"))
                    target_term = st.text_input("ç›®æ ‡è¯­è¨€æœ¯è¯­", key=sk("target_term"))
                with col2:
                    domain = st.text_input("é¢†åŸŸ", key=sk("domain"))
                    project_id = st.text_input("é¡¹ç›®ID(å¯ç©º)", key=sk("project_id"))
                    strategy = st.text_input("ç­–ç•¥(ç›´è¯‘/æ„è¯‘/è½¬è¯‘/éŸ³è¯‘/çœç•¥/å¢è¯‘/è§„èŒƒåŒ–â€¦)", key=sk("strategy"))
                with col3:
                    example = st.text_area("ä¾‹å¥", height=80, key=sk("example"))
                    category = st.text_input("åˆ†ç±»(å¯é€‰)", key=sk("category"))

                b1, b2 = st.columns(2)
                add = b1.form_submit_button("ä¿å­˜(æ–°å¢æˆ–æ›´æ–°)")
                delbtn = b2.form_submit_button("åˆ é™¤(æŒ‰ ID)")

            if add:
                if not source_term.strip():
                    st.error("æºæœ¯è¯­å¿…å¡«")
                else:
                    if rid_edit and rid_edit.isdigit():
                        if _has_col("term_ext", "category"):
                            cur.execute("""
                            UPDATE term_ext
                            SET source_term=?, target_term=?, domain=?, project_id=?, strategy=?, example=?, category=?
                            WHERE id=?;
                            """, (source_term.strip(), target_term.strip() or None, domain or None,
                                int(project_id) if project_id.isdigit() else None, strategy or None, example or None,
                                category or None, int(rid_edit)))
                        else:
                            cur.execute("""
                            UPDATE term_ext
                            SET source_term=?, target_term=?, domain=?, project_id=?, strategy=?, example=?
                            WHERE id=?;
                            """, (source_term.strip(), target_term.strip() or None, domain or None,
                                int(project_id) if project_id.isdigit() else None, strategy or None, example or None,
                                int(rid_edit)))
                        conn.commit(); st.success("âœ… å·²æ›´æ–°"); st.rerun()
                    else:
                        if _has_col("term_ext", "category"):
                            cur.execute("""
                            INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example, category)
                            VALUES (?, ?, ?, ?, ?, ?, ?)
                            """, (
                                source_term.strip(),
                                (target_term.strip() or None) if target_term else None,
                                (domain or None),
                                (int(project_id) if project_id.isdigit() else None) if project_id else None,
                                (strategy or None),
                                (example or None),
                                (category or None)
                            ))
                        else:
                            cur.execute("""
                            INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example)
                            VALUES (?, ?, ?, ?, ?, ?)
                            """, (
                                source_term.strip(),
                                (target_term.strip() or None) if target_term else None,
                                (domain or None),
                                (int(project_id) if project_id.isdigit() else None) if project_id else None,
                                (strategy or None),
                                (example or None),
                            ))
                        conn.commit(); st.success("âœ… å·²æ–°å¢"); st.rerun()

            if delbtn:
                if rid_edit and rid_edit.isdigit():
                    cur.execute("DELETE FROM term_ext WHERE id=?", (int(rid_edit),))
                    conn.commit(); st.success("ğŸ—‘ï¸ å·²åˆ é™¤"); st.rerun()
                else:
                    st.error("è¯·å¡«å†™è¦åˆ é™¤çš„ ID")

    # â€”â€” æ‰¹é‡å¯¼å…¥ CSV(å¢å¼ºç‰ˆ:åˆ—åè§„èŒƒåŒ– / åŠ¨æ€å¸¦æˆ–ä¸å¸¦ category / å»é‡æˆ–Upsert / é€è¡Œå®¹é”™)
    with sub_tabs[1]:
        sk1 = lambda n: f"{key_prefix}_t1_{n}"
        st.caption("CSV æ¨èåˆ—:source_term, target_term, domain, project_id, strategy, example(å¯é€‰:category / åˆ†ç±»)")
        up = st.file_uploader("ä¸Šä¼  CSV æ–‡ä»¶", type=["csv"], key=sk1("csv"))

        # å°å·¥å…·:è¡¥åˆ— + å”¯ä¸€ç´¢å¼• + è§„èŒƒå‡½æ•°
        def _ensure_col(table, col, type_):
            try:
                cur.execute(f"ALTER TABLE {table} ADD COLUMN {col} {type_};")
                conn.commit()
            except Exception:
                pass

        def _ensure_unique_index():
            try:
                cur.execute("""
                CREATE UNIQUE INDEX IF NOT EXISTS ux_term_proj
                ON term_ext(LOWER(TRIM(source_term)), IFNULL(project_id,-1));
                """)
                conn.commit()
            except Exception:
                pass

        def _norm_cols(df):
            # åˆ—å:å»BOM/ä¸¤ç«¯ç©ºæ ¼ -> å°å†™ -> æ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿ -> ä¸­è‹±åˆ—åæ˜ å°„
            df.columns = [str(c).replace("\ufeff","").strip() for c in df.columns]
            df.columns = [c.lower().replace(" ", "_") for c in df.columns]
            mapping = {
                "æºæœ¯è¯­": "source_term", "source": "source_term",
                "ç›®æ ‡æœ¯è¯­": "target_term", "target": "target_term",
                "é¢†åŸŸ": "domain",
                "é¡¹ç›®id": "project_id", "é¡¹ç›®_id": "project_id",
                "ç­–ç•¥": "strategy",
                "ä¾‹å¥": "example",
                "åˆ†ç±»": "category",
            }
            df = df.rename(columns={k.lower(): v for k, v in mapping.items()})
            return df

        def _norm(v):
            if v is None: return None
            v = str(v).strip().replace("\u3000", " ")
            return v if v else None

        def _to_int(v):
            try:
                return int(v) if v is not None and str(v).strip() != "" else None
            except Exception:
                return None

        if up is not None:
            try:
                df_up = pd.read_csv(up, encoding="utf-8-sig")
            except Exception:
                df_up = pd.read_csv(up, encoding="utf-8", errors="ignore")

            df_up = _norm_cols(df_up)
            st.write("æ£€æµ‹åˆ°åˆ—:", list(df_up.columns))
            render_table(df_up.head(10), key=sk("csv_preview"))

            # DB ä¾§ç¡®ä¿æœ‰ category åˆ—(å…¼å®¹è€åº“;è‹¥å·²æœ‰ä¼šå¿½ç•¥)
            _ensure_col("term_ext", "category", "TEXT")

            # ä¾§è¾¹é€‰é¡¹
            c1, c2, c3 = st.columns(3)
            with c1:
                dedup = st.checkbox("å»é‡(æºæœ¯è¯­+é¡¹ç›®ID)", value=True, key=sk1("dedup"))
            with c2:
                use_upsert = st.checkbox("å·²å­˜åœ¨åˆ™æ›´æ–°(Upsert)", value=False, key=sk1("upsert"))
            with c3:
                skip_empty = st.checkbox("è·³è¿‡ç©ºè¯‘æ–‡", value=False, key=sk1("skip_empty"))

            # Upsert éœ€è¦å”¯ä¸€ç´¢å¼•
            if use_upsert:
                _ensure_unique_index()

            if st.button("å¯¼å…¥æœ¯è¯­åº“", key=sk1("import_btn")):
                # æ˜¯å¦åŒ…å« category åˆ—(ä»¥CSVä¸ºå‡†;DBå·²æœ‰ä¸å¼ºåˆ¶CSVå¿…é¡»æœ‰)
                has_category_col = ("category" in df_up.columns)

                # å»é‡ç¼“å­˜(ä»…åœ¨éUpsertæ¨¡å¼ä¸‹ä½¿ç”¨)
                existing = set()
                if dedup and not use_upsert:
                    rows_exist = cur.execute("""
                        SELECT LOWER(TRIM(source_term)), IFNULL(project_id,-1)
                        FROM term_ext
                    """).fetchall()
                    existing = set(rows_exist)

                ins = skp = upd = 0
                errors = []

                for idx, row in df_up.iterrows():
                    src = _norm(row.get("source_term"))
                    if not src:
                        skp += 1
                        continue

                    tgt = _norm(row.get("target_term"))
                    if skip_empty and not tgt:
                        skp += 1
                        continue

                    dom = _norm(row.get("domain"))
                    pid = _to_int(row.get("project_id"))
                    stg = _norm(row.get("strategy"))
                    exa = _norm(row.get("example"))
                    cat = _norm(row.get("category")) if has_category_col else None

                    try:
                        if use_upsert:
                            # Upsert åˆ†æ”¯(éœ€è¦å”¯ä¸€ç´¢å¼•)
                            if has_category_col:
                                cur.execute("""
                                INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example, category)
                                VALUES (?,?,?,?,?,?,?)
                                ON CONFLICT(LOWER(TRIM(source_term)), IFNULL(project_id,-1))
                                DO UPDATE SET
                                    target_term=COALESCE(excluded.target_term, term_ext.target_term),
                                    domain     =COALESCE(excluded.domain,      term_ext.domain),
                                    strategy   =COALESCE(excluded.strategy,    term_ext.strategy),
                                    example    =COALESCE(excluded.example,     term_ext.example),
                                    category   =COALESCE(excluded.category,    term_ext.category);
                                """, (src, tgt, dom, pid, stg, exa, cat))
                            else:
                                cur.execute("""
                                INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example)
                                VALUES (?,?,?,?,?,?)
                                ON CONFLICT(LOWER(TRIM(source_term)), IFNULL(project_id,-1))
                                DO UPDATE SET
                                    target_term=COALESCE(excluded.target_term, term_ext.target_term),
                                    domain     =COALESCE(excluded.domain,      term_ext.domain),
                                    strategy   =COALESCE(excluded.strategy,    term_ext.strategy),
                                    example    =COALESCE(excluded.example,     term_ext.example);
                                """, (src, tgt, dom, pid, stg, exa))
                            upd += 1  # è®¡ä¸ºâ€œå¤„ç†æˆåŠŸâ€.ä¸åŒºåˆ†æ–°æ—§
                        else:
                            # é Upsert:å»é‡(æºæœ¯è¯­+é¡¹ç›®ID)
                            key = (src.lower(), pid if pid is not None else -1)
                            if dedup and key in existing:
                                skp += 1
                                continue

                            if has_category_col:
                                cur.execute("""
                                    INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example, category)
                                    VALUES (?,?,?,?,?,?,?)
                                """, (src, tgt, dom, pid, stg, exa, cat))
                            else:
                                cur.execute("""
                                    INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example)
                                    VALUES (?,?,?,?,?,?)
                                """, (src, tgt, dom, pid, stg, exa))
                            ins += 1
                            if dedup:
                                existing.add(key)

                    except Exception as e:
                        errors.append((idx+1, src, str(e)))
                        skp += 1
                        continue

                conn.commit()

                # ç»“æœæç¤º
                if use_upsert:
                    st.success(f"âœ… å·²å¤„ç† {ins+upd} æ¡(å…¶ä¸­å¯èƒ½å«æ–°å¢+æ›´æ–°).è·³è¿‡ {skp} æ¡ã€‚")
                else:
                    st.success(f"âœ… æ–°å¢ {ins} æ¡.è·³è¿‡ {skp} æ¡ã€‚")

                if errors:
                    with st.expander("â— è¡Œçº§é”™è¯¯æ˜ç»†(ä¸å½±å“å…¶ä»–è¡Œå†™å…¥)", expanded=False):
                        for i, s, e in errors:
                            st.write(f"ç¬¬ {i} è¡Œ({s}):{e}")


    # â€”â€” ç»Ÿè®¡ä¸å¯¼å‡º
    with sub_tabs[2]:
        sk2 = lambda n: f"{key_prefix}_t2_{n}"
        st.markdown("#### æœ¯è¯­ç»Ÿè®¡")
        df_stats = pd.read_sql_query("SELECT strategy, domain, category FROM term_ext WHERE source_term IS NOT NULL", conn)
        if df_stats.empty:
            st.info("æœ¯è¯­åº“ä¸ºç©º.è¯·å…ˆæ·»åŠ æˆ–å¯¼å…¥")
        else:
            # ç»Ÿä¸€é¢„å¤„ç†ï¼šç©ºå€¼ â†’ æœªæ ‡æ³¨
            df_stats["strategy"] = df_stats["strategy"].fillna("æœªæ ‡æ³¨").replace("", "æœªæ ‡æ³¨")
            df_stats["domain"]   = df_stats["domain"].fillna("æœªæ ‡æ³¨").replace("", "æœªæ ‡æ³¨")

            # é€‰æ‹©ç»Ÿè®¡ç»´åº¦
            dim_label = st.selectbox(
                "ç»Ÿè®¡ç»´åº¦",
                ["æŒ‰é¢†åŸŸ (domain)", "æŒ‰ç¿»è¯‘ç­–ç•¥ (strategy)"],
                index=0,
                key=sk2("dim_sel"),
            )

            if "é¢†åŸŸ" in dim_label:
                dim_col = "domain"
                dim_title = "é¢†åŸŸ"
            else:
                dim_col = "strategy"
                dim_title = "ç¿»è¯‘ç­–ç•¥"

            # é€‰æ‹©å±•ç¤ºæ–¹å¼
            chart_type = st.radio(
                "å±•ç¤ºæ–¹å¼",
                ["æŸ±çŠ¶å›¾", "é¥¼å›¾", "æ•°æ®è¡¨"],
                index=0,
                horizontal=True,
                key=sk2("chart_type"),
            )

            # åšè®¡æ•°
            count_df = (
                df_stats.groupby(dim_col)[dim_col]
                .count()
                .reset_index(name="term_count")
                .sort_values("term_count", ascending=False)
            )

            # ===== ä¸åŒå±•ç¤ºæ–¹å¼ =====
            if chart_type == "æŸ±çŠ¶å›¾":
                st.markdown(f"**{dim_title} åˆ†å¸ƒï¼ˆæŸ±çŠ¶å›¾ï¼‰**")

                chart = (
                    alt.Chart(count_df)
                    .mark_bar()
                    .encode(
                        x=alt.X("term_count:Q", title="æœ¯è¯­æ•°é‡"),
                        y=alt.Y(f"{dim_col}:N", sort="-x", title=dim_title),
                        tooltip=[dim_col, "term_count"],
                    )
                    .properties(height=320)
                )
                st.altair_chart(chart, width='stretch')

            elif chart_type == "é¥¼å›¾":
                st.markdown(f"**{dim_title} åˆ†å¸ƒï¼ˆé¥¼å›¾ï¼‰**")

                chart = (
                    alt.Chart(count_df)
                    .mark_arc()
                    .encode(
                        theta=alt.Theta("term_count:Q", title="æœ¯è¯­æ•°é‡"),
                        color=alt.Color(f"{dim_col}:N", title=dim_title),
                        tooltip=[dim_col, "term_count"],
                    )
                    .properties(height=320)
                )
                st.altair_chart(chart, width='stretch')

            else:  # æ•°æ®è¡¨
                st.markdown(f"**{dim_title} åˆ†å¸ƒï¼ˆæ•°æ®è¡¨ï¼‰**")
                tbl = count_df.rename(
                    columns={
                        dim_col: dim_title,
                        "term_count": "æœ¯è¯­æ•°é‡",
                    }
                )
                render_table(tbl, hide_index=True, key=sk2("tbl"))

        st.markdown("---")
        st.markdown("#### å¯¼å‡ºæœ¯è¯­è¡¨")
        df_exp = pd.read_sql_query("""
            SELECT source_term AS 'æºæœ¯è¯­',
                   target_term AS 'ç›®æ ‡æœ¯è¯­',
                   domain AS 'é¢†åŸŸ',
                   strategy AS 'ç¿»è¯‘ç­–ç•¥',
                   example AS 'ç¤ºä¾‹å¥',
                   category AS 'åˆ†ç±»'
            FROM term_ext ORDER BY source_term COLLATE NOCASE
        """, conn)

        from io import BytesIO
        buff = BytesIO()
        with pd.ExcelWriter(buff, engine="xlsxwriter") as writer:
            df_exp.to_excel(writer, index=False, sheet_name="æœ¯è¯­åº“")
        st.download_button("ğŸ“¥ ä¸‹è½½ Excel",
                           buff.getvalue(),
                           file_name="terms.xlsx",
                           mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                           key=sk2("dl_terms"))

    # â€”â€” å¿«é€Ÿæœç´¢
    with sub_tabs[3]:
        sk3 = lambda n: f"{key_prefix}_t3_{n}"
        q = st.text_input("å¿«é€Ÿæœç´¢(å‰ç¼€/å­ä¸²)", "", key=sk3("q"))
        limit = st.number_input("è¿”å›ä¸Šé™", 1, 5000, 1000, 100, key=sk3("limit"))
        if st.button("æœç´¢", key=sk3("q_btn")):
            if q:
                like = f"%{q}%"
                rows = cur.execute("""
                    SELECT id, source_term, target_term, domain, project_id
                    FROM term_ext
                    WHERE source_term LIKE ? OR target_term LIKE ?
                    ORDER BY id DESC
                    LIMIT ?
                """, (like, like, int(limit))).fetchall()
                render_table(pd.DataFrame(rows, columns=["ID","æºæœ¯è¯­","ç›®æ ‡æœ¯è¯­","é¢†åŸŸ","é¡¹ç›®"]),
                             key=sk3("q_grid"),editable=True)
            else:
                st.warning("è¯·è¾“å…¥å…³é”®è¯")

    # â€”â€” æ‰¹é‡æŒ‚æ¥é¡¹ç›®
    with sub_tabs[4]:
        sk4 = lambda n: f"{key_prefix}_t4_{n}"
        st.caption("å°†ä¸€æ‰¹æœ¯è¯­ç»Ÿä¸€è®¾ç½® project_id.ä¾¿äºé¡¹ç›®å†…ä¼˜å…ˆåŒ¹é…")
        ids_txt = st.text_area("æœ¯è¯­IDåˆ—è¡¨(é€—å·/ç©ºæ ¼/æ¢è¡Œåˆ†éš”)", key=sk4("ids"))
        pid_to = st.text_input("ç›®æ ‡é¡¹ç›®ID", key=sk4("pid_to"))
        if st.button("æ‰¹é‡æŒ‚æ¥", key=sk4("batch_btn")):
            import re
            if not pid_to.isdigit():
                st.error("é¡¹ç›®IDéœ€ä¸ºæ•°å­—")
            else:
                raw = re.split(r"[,\s]+", ids_txt.strip())
                ids = [int(x) for x in raw if x.isdigit()]
                if not ids:
                    st.warning("æœªè¯†åˆ«åˆ°æœ‰æ•ˆID")
                else:
                    qmarks = ",".join(["?"]*len(ids))
                    cur.execute(f"UPDATE term_ext SET project_id=? WHERE id IN ({qmarks})", (int(pid_to), *ids))
                    conn.commit()
                    st.success(f"âœ… å·²æŒ‚æ¥ {len(ids)} æ¡åˆ°é¡¹ç›® {pid_to}")

    # â€”â€” ä»å†å²æå–æœ¯è¯­
    with sub_tabs[5]:
        sk5 = lambda n: f"{key_prefix}_t5_{n}"
        st.markdown("#### ä»ç¿»è¯‘å†å²è®°å½•æŠ½å–æœ¯è¯­(DeepSeek)")
        ak, model = get_deepseek()
        if not ak:
            st.info("æœªæ£€æµ‹åˆ° DeepSeek Keyï¼Œå°†ç›´æ¥ä½¿ç”¨è¯­æ–™åº“åŒæ¬¾æ¨¡å‹åšç»“æ„åŒ–æœ¯è¯­æŠ½å–ã€‚")

        mode_pick = st.radio(
            "é€‰æ‹©æ¥æº",
            ["æŒ‰é¡¹ç›®æŠ½å–(åˆå¹¶å¤šæ¡)", "æŒ‰å•æ¡è®°å½•æŠ½å–"],
            horizontal=True,
            key=sk5("ext_mode"),
        )
        if mode_pick == "æŒ‰é¡¹ç›®æŠ½å–(åˆå¹¶å¤šæ¡)":
            pid_ext = st.text_input("é¡¹ç›®ID", key=sk5("ext_pid"))
            max_chars = st.number_input("é‡‡æ ·æœ€å¤§å­—ç¬¦æ•°", 1000, 20000, 5000, 500, key=sk5("ext_max"))
            if st.button("å¼€å§‹æŠ½å–", key=sk5("ext_go_proj")):
                if pid_ext.isdigit():
                    rows = cur.execute(
                        "SELECT src_path, output_text FROM trans_ext WHERE project_id=? ORDER BY id DESC LIMIT 10",
                        (int(pid_ext),),
                    ).fetchall()
                    buf = []
                    total = 0
                    for sp, ot in rows:
                        src = read_source_file(sp) if sp else ""
                        txt = (src + "\n" + (ot or "")).strip()
                        if not txt:
                            continue
                        if total + len(txt) > int(max_chars):
                            remain = max(0, int(max_chars) - total)
                            buf.append(txt[:remain])
                            break
                        else:
                            buf.append(txt)
                            total += len(txt)
                    big = "\n\n".join(buf)
                    big = "\n\n".join(buf)

                    # âœ… å…ˆçœ‹è¿™ä¸ªé¡¹ç›®åˆ°åº•æœ‰æ²¡æœ‰å¯ç”¨çš„å†å²æ–‡æœ¬
                    if not big.strip():
                        st.warning("è¯¥é¡¹ç›®ä¸‹æ²¡æœ‰å¯ç”¨çš„ç¿»è¯‘å†å²æ–‡æœ¬ï¼Œæ— æ³•æŠ½å–æœ¯è¯­ã€‚")
                        return

                    # è°ƒè¯•ç”¨:ä½ å¯ä»¥å…ˆçœ‹çœ‹é‡‡æ ·äº†å¤šå°‘å­—ã€å‰å‡ è¡Œæ˜¯ä»€ä¹ˆ
                    st.write({
                        "history_rows": len(rows),
                        "sample_chars": len(big),
                        "sample_preview": big[:300]
                    })

                    try:
                        res = ds_extract_terms(big, ak, model, src_lang="zh", tgt_lang="en", prefer_corpus_model=True)
                    except Exception as e:
                        st.error(f"è°ƒç”¨æœ¯è¯­æŠ½å–æ—¶å‡ºé”™: {e}")
                        return

                    # è°ƒè¯•ç”¨:å…ˆçœ‹ä¸€ä¸‹åŸå§‹ç»“æœé•¿ä»€ä¹ˆæ ·
                    st.write({"extract_result_preview": str(res)[:500]})
                    if not res:
                        st.info("æœªæŠ½å–åˆ°æœ¯è¯­æˆ–è§£æå¤±è´¥")
                    else:
                        st.success(f"æŠ½å–åˆ° {len(res)} æ¡.å‡†å¤‡æ‰¹é‡å†™å…¥â€¦â€¦")
                        ins = 0
                        for o in res:
                            cur.execute(
                                "INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example) VALUES (?, ?, ?, ?, ?, ?)",
                                (o["source_term"], o.get("target_term") or None, o.get("domain"), int(pid_ext), o.get("strategy"), o.get("example")),
                            )
                            ins += 1
                        conn.commit()
                        st.success(f"âœ… å·²å†™å…¥æœ¯è¯­åº“ {ins} æ¡")
                else:
                    st.warning("è¯·è¾“å…¥æ•°å­—å‹é¡¹ç›®ID")
        else:
            rid_ext = st.text_input("å†å²è®°å½•ID", key=sk5("ext_rid"))
            if st.button("å¼€å§‹æŠ½å–", key=sk5("ext_go_rec")):
                if rid_ext.isdigit():
                    row = cur.execute(
                        "SELECT src_path, output_text, project_id FROM trans_ext WHERE id=?",
                        (int(rid_ext),),
                    ).fetchone()
                    if not row:
                        st.warning("æœªæ‰¾åˆ°è¯¥è®°å½•")
                    else:
                        sp, ot, pid0 = row
                        src = read_source_file(sp) if sp else ""
                        big = (src + "\n" + (ot or "")).strip()
                        res = ds_extract_terms(big, ak, model, src_lang="zh", tgt_lang="en", prefer_corpus_model=True)
                        if not res:
                            st.info("æœªæŠ½å–åˆ°æœ¯è¯­æˆ–è§£æå¤±è´¥")
                        else:
                            st.success(f"æŠ½å–åˆ° {len(res)} æ¡.å‡†å¤‡æ‰¹é‡å†™å…¥â€¦â€¦")
                            ins = 0
                            for o in res:
                                cur.execute(
                                    "INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example) VALUES (?, ?, ?, ?, ?, ?)",
                                    (o["source_term"], o.get("target_term") or None, o.get("domain"), pid0, o.get("strategy"), o.get("example")),
                                )
                                ins += 1
                            conn.commit()
                            st.success(f"âœ… å·²å†™å…¥æœ¯è¯­åº“ {ins} æ¡(project_id={pid0})")

    # â€”â€” åˆ†ç±»ç®¡ç†
    with sub_tabs[6]:
        sk6 = lambda n: f"{key_prefix}_t6_{n}"
        st.markdown("#### åˆ†ç±»ç®¡ç†")
        c1, c2 = st.columns(2)
        with c1:
            ids_txt = st.text_area("æŒ‰ ID æ‰¹é‡è®¾ç½®åˆ†ç±»(é€—å·/ç©ºæ ¼/æ¢è¡Œåˆ†éš”)", key=sk6("cat_ids"))
            cat_to = st.text_input("è¦è®¾ç½®çš„åˆ†ç±»å", key=sk6("cat_name"))
            if st.button("æ‰¹é‡è®¾ç½®åˆ†ç±»", key=sk6("cat_set_ids")):
                import re
                raw = re.split(r"[,\s]+", (ids_txt or "").strip())
                ids = [int(x) for x in raw if x.isdigit()]
                if not ids or not cat_to.strip():
                    st.warning("è¯·å¡«å…¥IDåˆ—è¡¨ä¸åˆ†ç±»åç§°")
                else:
                    qmarks = ",".join(["?"] * len(ids))
                    cur.execute(f"UPDATE term_ext SET category=? WHERE id IN ({qmarks})", (cat_to.strip(), *ids))
                    conn.commit()
                    st.success(f"âœ… å·²è®¾ç½® {len(ids)} æ¡ä¸ºåˆ†ç±»:{cat_to.strip()}")

        with c2:
            pid_cat = st.text_input("å°†æŸé¡¹ç›®IDå…¨éƒ¨æœ¯è¯­è®¾ç½®ä¸ºåˆ†ç±»", key=sk6("cat_pid"))
            cat2_to = st.text_input("åˆ†ç±»å", key=sk6("cat2_name"))
            if st.button("æŒ‰é¡¹ç›®IDç»Ÿä¸€åˆ†ç±»", key=sk6("cat_set_pid")):
                if pid_cat.isdigit() and cat2_to.strip():
                    cur.execute("UPDATE term_ext SET category=? WHERE project_id=?", (cat2_to.strip(), int(pid_cat)))
                    conn.commit()
                    st.success(f"âœ… å·²å°†é¡¹ç›® {pid_cat} çš„æœ¯è¯­åˆ†ç±»è®¾ä¸º:{cat2_to.strip()}")
                else:
                    st.warning("è¯·å¡«å†™é¡¹ç›®IDä¸åˆ†ç±»å")

# ========== Session åˆå§‹åŒ– ==========
if "kb_embedder" not in st.session_state and KBEmbedder:
    st.session_state["kb_embedder"] = KBEmbedder(lazy=True)

def render_index_manager_by_domain(st, conn, cur):
    """é¢†åŸŸ + ç±»å‹è§†è§’çš„ç´¢å¼•ç®¡ç†é¡µé¢"""
    st.subheader("ğŸ§  é¢†åŸŸçº§ç´¢å¼•ç®¡ç† (åŒè¯­å¯¹ç…§ + ç¿»è¯‘ç­–ç•¥)")

    # 1) æ”¶é›†æ‰€æœ‰å¯èƒ½çš„é¢†åŸŸå€¼
    domains = set()

    # from items
    try:
        rows = cur.execute("SELECT DISTINCT IFNULL(domain,'æœªåˆ†ç±»') FROM items").fetchall()
        for (d,) in rows:
            if d and d.strip():
                domains.add(d.strip())
            else:
                domains.add("æœªåˆ†ç±»")
    except Exception:
        pass

    # from corpus.domain (å¦‚æœæœ‰è¯¥å­—æ®µ)
    try:
        cols = [r[1] for r in cur.execute("PRAGMA table_info(corpus)").fetchall()]
        if "domain" in cols:
            rows = cur.execute("SELECT DISTINCT IFNULL(domain,'æœªåˆ†ç±»') FROM corpus").fetchall()
            for (d,) in rows:
                if d and d.strip():
                    domains.add(d.strip())
                else:
                    domains.add("æœªåˆ†ç±»")
    except Exception:
        pass

    # from strategy_texts(å¦‚æœå·²ç»å­˜åœ¨)
    try:
        cur.execute(
            "CREATE TABLE IF NOT EXISTS strategy_texts ("
            "id INTEGER PRIMARY KEY,"
            "domain TEXT,"
            "title TEXT,"
            "content TEXT NOT NULL,"
            "collection TEXT,"
            "source TEXT,"
            "created_at TEXT DEFAULT (datetime('now'))"
            ");"
        )
        conn.commit()
        rows = cur.execute("SELECT DISTINCT IFNULL(domain,'æœªåˆ†ç±»') FROM strategy_texts").fetchall()
        for (d,) in rows:
            if d and d.strip():
                domains.add(d.strip())
            else:
                domains.add("æœªåˆ†ç±»")
    except Exception:
        pass

    if not domains:
        st.info("å½“å‰å°šæœªè®¾ç½®ä»»ä½•é¢†åŸŸ(domain)ã€‚è¯·å…ˆåœ¨é¡¹ç›®æˆ–è¯­æ–™ä¸­è®¾ç½®é¢†åŸŸã€‚")
        return

    domains_list = sorted(domains)
    dom_sel = st.selectbox("é€‰æ‹©è¦ç®¡ç†çš„é¢†åŸŸ", domains_list)
    dom_key = (dom_sel or "").strip() or "æœªåˆ†ç±»"

    st.markdown(f"### å½“å‰é¢†åŸŸ: `{dom_key}`")

    # 2) ç»Ÿè®¡è¯¥é¢†åŸŸä¸‹çš„é¡¹ç›® & è¯­æ–™ & ç´¢å¼•æƒ…å†µ
    # 2.1 é¡¹ç›®åˆ—è¡¨
    proj_rows = cur.execute(
        "SELECT id, title FROM items WHERE IFNULL(domain,'æœªåˆ†ç±»') = ? ORDER BY id ASC",
        (dom_key,)
    ).fetchall()
    proj_ids = [pid for (pid, _) in proj_rows]

    # 2.2 è¯­æ–™æ¡æ•°(å¦‚æœ corpus æœ‰ domain å­—æ®µ)
    corpus_cnt = None
    try:
        cols = [r[1] for r in cur.execute("PRAGMA table_info(corpus)").fetchall()]
        if "domain" in cols:
            corpus_cnt = cur.execute(
                "SELECT COUNT(*) FROM corpus WHERE IFNULL(domain,'æœªåˆ†ç±»') = ?",
                (dom_key,)
            ).fetchone()[0]
    except Exception:
        corpus_cnt = None

    # 2.3 åŒè¯­ç´¢å¼•æ¡æ•° = è¯¥é¢†åŸŸæ‰€æœ‰é¡¹ç›®ç´¢å¼•çš„ mapping é•¿åº¦ä¹‹å’Œ
    idx_bilingual_total = 0
    for pid in proj_ids:
        try:
            mode, index, mapping, vecs = _load_index(int(pid))
        except Exception:
            continue
        if isinstance(mapping, list):
            idx_bilingual_total += len(mapping)

    # 2.4 ç­–ç•¥æ–‡æœ¬æ•°é‡ & ç­–ç•¥ç´¢å¼•æ¡æ•°
    try:
        strategy_cnt = cur.execute(
            "SELECT COUNT(*) FROM strategy_texts WHERE IFNULL(domain,'æœªåˆ†ç±»') = ?",
            (dom_key,)
        ).fetchone()[0]
    except Exception:
        strategy_cnt = 0

    mode_s, index_s, mapping_s, vecs_s = _load_index_domain(dom_key, "strategy")
    if isinstance(mapping_s, list):
        idx_strategy_total = len(mapping_s)
    else:
        idx_strategy_total = 0

    c1, c2 = st.columns(2)
    with c1:
        st.markdown("#### ğŸ“˜ åŒè¯­å¯¹ç…§ç´¢å¼•(ä¾‹å¥åº“)")
        st.write(f"- è¯¥é¢†åŸŸä¸‹é¡¹ç›®æ•°: **{len(proj_ids)}**")
        if corpus_cnt is not None:
            st.write(f"- è¯­æ–™åº“ä¸­åŒè¯­æ¡ç›®(æŒ‰ domain è®¡): **{corpus_cnt}**")
        st.write(f"- å·²å»ºç«‹ç´¢å¼•çš„å¥å¯¹æ¡æ•°(åˆè®¡): **{idx_bilingual_total}**")

        if proj_ids:
            if st.button("ğŸ” é‡å»ºè¯¥é¢†åŸŸæ‰€æœ‰é¡¹ç›®çš„ã€åŒè¯­å¯¹ç…§ã€‘ç´¢å¼•", key=f"rebuild_bi_{dom_key}"):
                added_sum = 0
                total_sum = 0
                for pid in proj_ids:
                    try:
                        res = build_project_vector_index(int(pid))
                        added_sum += res.get("added", 0)
                        total_sum = res.get("total", total_sum)
                    except Exception as e:
                        st.warning(f"é¡¹ç›® {pid} é‡å»ºç´¢å¼•æ—¶å‡ºé”™: {e}")
                st.success(
                    f"å·²é‡å»ºè¯¥é¢†åŸŸæ‰€æœ‰é¡¹ç›®ç´¢å¼•ã€‚"
                    f"æ–°å¢å¥å¯¹: {added_sum}ï¼Œæœ€åä¸€ä¸ªé¡¹ç›®è¿”å›çš„ç´¢å¼•æ€»é‡: {total_sum}"
                )
        else:
            st.info("è¯¥é¢†åŸŸä¸‹æš‚æ—¶æ²¡æœ‰ä»»ä½•é¡¹ç›®ã€‚")

    with c2:
        st.markdown("#### ğŸ“ ç¿»è¯‘ç­–ç•¥ç´¢å¼•(strategy)")
        st.write(f"- è¯¥é¢†åŸŸä¸‹ç­–ç•¥æ–‡æœ¬æ¡æ•°: **{strategy_cnt}**")
        st.write(f"- å·²å»ºç«‹ç´¢å¼•çš„ç­–ç•¥å‘é‡æ¡æ•°: **{idx_strategy_total}**")

        if st.button("ğŸ” é‡å»ºè¯¥é¢†åŸŸçš„ã€ç¿»è¯‘ç­–ç•¥ã€‘ç´¢å¼•", key=f"rebuild_strategy_{dom_key}"):
            try:
                res = build_strategy_index_for_domain(dom_key)
                st.success(
                    f"å·²é‡å»ºç­–ç•¥ç´¢å¼•ã€‚æ–°å¢ç­–ç•¥æ®µè½: {res.get('added', 0)}ï¼Œ"
                    f"ç´¢å¼•æ€»é‡: {res.get('total', 0)}"
                )
            except Exception as e:
                st.error(f"é‡å»ºç­–ç•¥ç´¢å¼•æ—¶å‡ºé”™: {e}")

    st.markdown("---")
    with st.expander("ğŸ” æŸ¥çœ‹è¯¥é¢†åŸŸä¸‹çš„é¡¹ç›®åˆ—è¡¨", expanded=False):
        if proj_rows:
            for pid, title in proj_rows:
                st.write(f"- é¡¹ç›® {pid}: {title}")
        else:
            st.write("æš‚æ— é¡¹ç›®ã€‚")

    with st.expander("ğŸ” æŸ¥çœ‹è¯¥é¢†åŸŸä¸‹çš„ç­–ç•¥æ–‡æœ¬(å‰å‡ æ¡)", expanded=False):
        try:
            rows = cur.execute(
                "SELECT id, title, substr(content,1,200) FROM strategy_texts "
                "WHERE IFNULL(domain,'æœªåˆ†ç±»') = ? ORDER BY id DESC LIMIT 20",
                (dom_key,)
            ).fetchall()
            if not rows:
                st.write("æš‚æ— ç­–ç•¥æ–‡æœ¬ã€‚")
            else:
                for sid, ttl, preview in rows:
                    st.write(f"**[{sid}] {ttl or '(æ— æ ‡é¢˜)'}**")
                    st.write(preview + ("..." if len(preview) >= 200 else ""))
                    st.markdown("---")
        except Exception as e:
            st.write(f"è¯»å–ç­–ç•¥æ–‡æœ¬å‡ºé”™: {e}")

# ========== é¡µé¢ç»“æ„ ==========
st.sidebar.title("å¯¼èˆª")

choice = st.sidebar.radio(
    "åŠŸèƒ½é€‰æ‹©",
    [
        "ğŸ“‚ ç¿»è¯‘é¡¹ç›®ç®¡ç†",
        "ğŸ“˜ æœ¯è¯­åº“ç®¡ç†",
        "ğŸ“Š ç¿»è¯‘å†å²",
        "ğŸ“š è¯­æ–™åº“ç®¡ç†",
        "ğŸ§  ç´¢å¼•ç®¡ç†",
    ],
)

st.title("ä¸ªäººç¿»è¯‘çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ3.0")


# ========== Tab1:ç¿»è¯‘é¡¹ç›®ç®¡ç† ==========
# ========== Tab1:ç¿»è¯‘é¡¹ç›®ç®¡ç† ==========
if choice.startswith("ğŸ“‚"):
    st.subheader("ç¿»è¯‘é¡¹ç›®ç®¡ç†")
    with st.form("new_project"):
        TAG_OPTIONS = ["æ”¿æ²»", "ç»æµ", "æ–‡åŒ–", "æ–‡ç‰©", "é‡‘è", "æ³•å¾‹"]
        SCENE_OPTIONS = ["å­¦æœ¯", "é…éŸ³ç¨¿", "æ­£å¼ä¼šè®®"]
        use_semantic = st.checkbox("åœ¨ç¿»è¯‘æ—¶å¯ç”¨è¯­ä¹‰å¬å›å‚è€ƒ", value=True)
        
        # === è¯­ä¹‰å¬å›èŒƒå›´é€‰æ‹© ===
        scope_label = "è¯­ä¹‰å¬å›èŒƒå›´"
        scope_options = {
            "ä»…å½“å‰é¡¹ç›®": "project",
            "åŒé¢†åŸŸ + å½“å‰é¡¹ç›®": "domain",
            "å…¨åº“": "all"
        }
        default_scope = "ä»…å½“å‰é¡¹ç›®"
        sel = st.selectbox(
            scope_label,
            list(scope_options.keys()),
            index=list(scope_options.keys()).index(default_scope),
            key="scope_sel_newproj"
        )
        st.session_state["scope_newproj"] = scope_options[sel]

        c1, c2 = st.columns([3, 2])
        with c1:
            title = st.text_input("é¡¹ç›®åç§°")
            tags_sel = st.multiselect("é¡¹ç›®æ ‡ç­¾(å¯å¤šé€‰)", TAG_OPTIONS)
            scene_sel = st.selectbox("åœºåˆ", SCENE_OPTIONS, index=0)
        with c2:
            translation_type = st.selectbox("ç¿»è¯‘æ–¹å¼", ["ä½¿ç”¨æœ¯è¯­åº“", "çº¯æœºå™¨ç¿»è¯‘"])
            translation_mode = st.radio("æ¨¡å¼", ["æ ‡å‡†æ¨¡å¼", "æœ¯è¯­çº¦æŸæ¨¡å¼"], horizontal=True)
            prompt_text = st.text_area(
                "ç¿»è¯‘æç¤º(æ³¨å…¥æ¨¡å‹ System Prompt)",
                placeholder="å†™ä¸‹å¯¹ DeepSeek çš„ç¡¬æ€§/ä¼˜å…ˆçº§æŒ‡ä»¤.å¦‚:æ—¶æ€ç»Ÿä¸€ä¸ºè¿‡å»å¼.ä¸“æœ‰åè¯ä¿æŒåŸæ–‡â€¦â€¦",
                height=120,
                key="new_proj_prompt"
            )
        # === é¢†åŸŸè‡ªåŠ¨ç»‘å®šé€»è¾‘ ===
        # è‹¥ç”¨æˆ·é€‰æ‹©äº†å¤šä¸ªæ ‡ç­¾.åˆ™é»˜è®¤ä»¥ç¬¬ä¸€ä¸ªæ ‡ç­¾ä¸ºé¢†åŸŸ
        domain_val = tags_sel[0] if tags_sel else None

        # === æäº¤æŒ‰é’® ===
        submitted = st.form_submit_button("ğŸ’¾ åˆ›å»ºé¡¹ç›®")
        if submitted:
            if not title:
                st.error("è¯·å¡«å†™é¡¹ç›®åç§°")
            else:
                try:
                    # ç¡®ä¿ items è¡¨å­˜åœ¨ domain å­—æ®µ
                    cur.execute("PRAGMA table_info(items);")
                    cols = [r[1] for r in cur.fetchall()]
                    if "domain" not in cols:
                        cur.execute("ALTER TABLE items ADD COLUMN domain TEXT;")
                        conn.commit()

                    # æ’å…¥æ–°é¡¹ç›®(å« domain)
                    cur.execute("""
                        INSERT INTO items(title, body, tags, domain,type)
                        VALUES (?, ?, ?, ?, 'project')
                    """, (
                        title,
                        prompt_text or "",
                        ",".join(tags_sel or []),
                        domain_val
                    ))
                    conn.commit()

                    st.success(f"âœ… é¡¹ç›® '{title}' å·²åˆ›å»º(é¢†åŸŸ:{domain_val or 'æœªæŒ‡å®š'})")
                except Exception as e:
                    st.error(f"âŒ åˆ›å»ºé¡¹ç›®å¤±è´¥: {e}")

    rows = cur.execute(
        """
        SELECT
            i.id,
            i.title,
            COALESCE(i.tags,'')              AS tags,
            COALESCE(MIN(e.src_path),'')     AS src_path,
            COALESCE(i.created_at,'')        AS created_at,
            COALESCE(i.scene,'')             AS scene,
            COALESCE(i.prompt,'')            AS prompt,
            COALESCE(i.mode,'')              AS mode,
            COALESCE(i.trans_type,'')        AS trans_type
        FROM items i
        LEFT JOIN item_ext e ON e.item_id = i.id
        WHERE COALESCE(i.type,'')='project'
        GROUP BY i.id
        ORDER BY i.id DESC
        """
    ).fetchall()

    if not rows:
        st.info("æš‚æ— é¡¹ç›®")
    else:
        # ç”¨äºæ”¶é›†æœ¬è½®å‹¾é€‰çš„é¡¹ç›®(ID + æ–‡ä»¶è·¯å¾„)
        batch_to_delete = []

        for pid, title, tags_str, path, ct, scene, prompt_ro, mode, trans_type in rows:
            ensure_legacy_file_record(cur, conn, pid, path or None)
            file_records = fetch_project_files(cur, pid)
            tag_display = tags_str or "æ— "
            file_display = f"{len(file_records)} ä¸ªæ–‡ä»¶" if file_records else "æ— "
            selected_src_path = None

            with st.expander(f"{title}ï½œæ–¹å¼:{mode or 'æœªè®¾'}ï½œæ ‡ç­¾:{tag_display}ï½œåœºåˆ:{scene or 'æœªå¡«'}ï½œæ–‡ä»¶:{file_display}ï½œåˆ›å»º:{ct}"):
                # âœ… æ‰¹é‡æ“ä½œç”¨çš„å‹¾é€‰æ¡†
                sel = st.checkbox("é€‰æ‹©æ­¤é¡¹ç›®(ç”¨äºæ‰¹é‡åˆ é™¤)", key=f"sel_proj_{pid}")
                if sel:
                    batch_to_delete.append(pid)

                c1, c2, c3 = st.columns([2, 2, 1])
                with c1:
                    st.selectbox("ç¿»è¯‘æ–¹å‘", ["ä¸­è¯‘è‹±", "è‹±è¯‘ä¸­"], key=f"lang_{pid}")
                with c2:
                    max_len = st.number_input("åˆ†å—é•¿åº¦", 600, 2000, 1200, 100, key=f"len_{pid}")
                with c3:
                    use_terms = st.checkbox("ä½¿ç”¨æœ¯è¯­åº“", value=(mode == "æœ¯è¯­çº¦æŸæ¨¡å¼"), key=f"ut_{pid}")

                st.caption(f"æ ‡ç­¾:{tag_display}")
                st.caption(f"åœºåˆ:{scene or 'æœªå¡«å†™'}")
             
                # === é¢†åŸŸ(domain)è®¾ç½®:è·Ÿéšç¬¬ä¸€ä¸ªæ ‡ç­¾ æˆ– æ‰‹åŠ¨é€‰æ‹© ===
                # è¯»å–å½“å‰é¡¹ç›®çš„ domain / tags
                # ä¿åº•:items è¡¨è‹¥æ²¡æœ‰ domain åˆ—.åŠ¨æ€è¡¥åˆ—(å…¼å®¹æ—§åº“)
                cols_items = [r[1] for r in cur.execute("PRAGMA table_info(items)").fetchall()]
                if "domain" not in cols_items:
                    try:
                        cur.execute("ALTER TABLE items ADD COLUMN domain TEXT;")
                        conn.commit()
                    except Exception:
                        pass  # å¹¶å‘æˆ–å·²æœ‰åˆ—æ—¶å¿½ç•¥
 
                row = cur.execute(
                    "SELECT IFNULL(domain,''), IFNULL(tags,'') FROM items WHERE id=?",
                    (pid,)
                ).fetchone()
                domain0, tags0 = (row or ["", ""])
                tags_list = [t for t in (tags0.split(",") if tags0 else []) if t]

                DOMAIN_OPTIONS = ["æ”¿æ²»", "ç»æµ", "æ–‡åŒ–", "æ–‡ç‰©", "é‡‘è", "æ³•å¾‹"]

                dom_mode = st.radio(
                    "é¢†åŸŸè®¾ç½®æ–¹å¼",
                    ["è·Ÿéšç¬¬ä¸€ä¸ªæ ‡ç­¾", "æ‰‹åŠ¨é€‰æ‹©"],
                    horizontal=True,
                    key=f"dom_mode_{pid}"
                )

                if dom_mode == "è·Ÿéšç¬¬ä¸€ä¸ªæ ‡ç­¾":
                    domain_val = (tags_list[0] if tags_list else (domain0 or None))
                    st.caption(f"å½“å‰é¢†åŸŸ(è‡ªåŠ¨):{domain_val or 'æœªæŒ‡å®š'}(ç”±ç¬¬ä¸€ä¸ªæ ‡ç­¾å†³å®š)")
                else:
                    idx = DOMAIN_OPTIONS.index(domain0) if domain0 in DOMAIN_OPTIONS else 0
                    domain_val = st.selectbox(
                        "é¢†åŸŸ(æ‰‹åŠ¨é€‰æ‹©)",
                        DOMAIN_OPTIONS,
                        index=idx,
                        key=f"dom_sel_{pid}"
                    )

                sync_corpus = st.checkbox(
                    "åŒæ—¶å›å¡«è¯¥é¡¹ç›®ä¸‹è¯­æ–™çš„é¢†åŸŸ(ä»…è¡¥ç©ºæˆ–åŸé¢†åŸŸç›¸åŒæ—¶è¦†ç›–)",
                    value=True,
                    key=f"sync_corpus_{pid}"
                )

                if st.button("ğŸ’¾ ä¿å­˜é¢†åŸŸè®¾ç½®", key=f"save_dom_{pid}", type="secondary"):
                    try:
                        # ç¡®ä¿ items.domain å­˜åœ¨
                        cols_items = [r[1] for r in cur.execute("PRAGMA table_info(items)").fetchall()]
                        if "domain" not in cols_items:
                            cur.execute("ALTER TABLE items ADD COLUMN domain TEXT;")
                            conn.commit()

                        # æ›´æ–° items.domain
                        cur.execute("UPDATE items SET domain=? WHERE id=?", (domain_val, pid))
                        conn.commit()

                        # åŒæ­¥è¯­æ–™åº“çš„ domain(ä¼˜å…ˆ corpus_main.é€€å› corpus)
                        def _table_exists(tb: str) -> bool:
                            return bool(cur.execute(
                                "SELECT name FROM sqlite_master WHERE type='table' AND name=?;", (tb,)
                            ).fetchone())

                        corpus_tb = "corpus_main" if _table_exists("corpus_main") else ("corpus" if _table_exists("corpus") else None)
                        if sync_corpus and corpus_tb and domain_val:
                            # ç¡®ä¿åˆ—å­˜åœ¨
                            cols_corpus = [r[1] for r in cur.execute(f"PRAGMA table_info({corpus_tb})").fetchall()]
                            if "domain" not in cols_corpus:
                                cur.execute(f"ALTER TABLE {corpus_tb} ADD COLUMN domain TEXT;")
                                conn.commit()

                            # ä»…è¡¥ç©ºæˆ–åŸ domain ä¸ domain0 ç›¸åŒæ—¶è¦†ç›–.é¿å…è¯¯ä¼¤è·¨é¢†åŸŸæ•°æ®
                            cur.execute(f"""
                                UPDATE {corpus_tb}
                                SET domain = ?
                                WHERE project_id = ?
                                AND (domain IS NULL OR TRIM(domain)='' OR domain = ?)
                            """, (domain_val, pid, domain0 or ""))
                            conn.commit()

                        st.success("âœ… å·²ä¿å­˜é¢†åŸŸè®¾ç½®")
                        st.rerun()

                    except Exception as e:
                        st.error(f"âŒ ä¿å­˜å¤±è´¥:{e}")

                if prompt_ro:
                    try:
                        cur.execute("SELECT IFNULL(prompt, '') FROM items WHERE id=?", (pid,))
                        prompt_ro = (cur.fetchone() or [""])[0]
                    except Exception:
                        prompt_ro = ""

                    st.text_area("ç¿»è¯‘æç¤º(åªè¯»)", prompt_ro or "", height=120, key=f"proj_prompt_ro_{pid}")

                file_col, action_col = st.columns([3, 1])

                with file_col:
                    if file_records:
                        option_labels = []
                        option_map = {}
                        for rec in file_records:
                            label = f"[#{rec['id']}] {rec['name']}"
                            if rec["uploaded_at"]:
                                label += f"ï½œ{rec['uploaded_at']}"
                            option_labels.append(label)
                            option_map[label] = rec
                        sel_key = f"file_sel_{pid}"
                        default_label = st.session_state.get(sel_key)
                        if default_label not in option_labels:
                            default_label = option_labels[0]
                            st.session_state[sel_key] = default_label
                        chosen_label = st.selectbox(
                            "é€‰æ‹©è¦ç¿»è¯‘çš„æºæ–‡ä»¶",
                            option_labels,
                            index=option_labels.index(default_label),
                            key=sel_key,
                        )
                        selected_src_path = option_map[chosen_label]["path"]
                        st.caption(f"å·²ä¸Šä¼  {len(file_records)} ä¸ªé™„ä»¶ï¼Œå½“å‰é€‰ä¸­:{option_map[chosen_label]['name']}")
                    else:
                        selected_src_path = path or None
                        st.info("å°šæœªä¸Šä¼ æºæ–‡ä»¶ï¼Œå¯åœ¨ä¸‹æ–¹ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªæ–‡ä»¶ã€‚")                      

                    upload_key = f"up_multi_{pid}"
                    processed_key = f"up_multi_processed_{pid}"
                    if upload_key not in st.session_state:
                        st.session_state[processed_key] = set()
                    uploads = st.file_uploader(
                        "æ–°å¢/è¡¥ä¼ æ–‡ä»¶(å¯å¤šé€‰)",
                        type=["txt", "docx", "xlsx", "pdf"],
                        accept_multiple_files=True,
                        key=upload_key
                    )
                    if uploads:
                        processed_names = st.session_state.setdefault(processed_key, set())
                        saved = 0
                        for uf in uploads:
                            if not uf or uf.name in processed_names:
                                continue
                            data = uf.read()
                            new_path = register_project_file(cur, conn, pid, uf.name, data)
                            if new_path:
                                cur.execute("SELECT id FROM item_ext WHERE item_id=?", (pid,))
                                r = cur.fetchone()
                                if r:
                                    cur.execute("UPDATE item_ext SET src_path=? WHERE id=?", (new_path, r[0]))
                                else:
                                    cur.execute("INSERT INTO item_ext (item_id, src_path) VALUES (?, ?)", (pid, new_path))
                                conn.commit()
                                saved += 1
                                processed_names.add(uf.name)
                        if saved:
                            st.success(f"âœ… å·²ä¸Šä¼  {saved} ä¸ªæ–‡ä»¶")
                    else:
                        st.session_state.pop(processed_key, None)

                    if file_records:
                        st.markdown("é™„ä»¶åˆ—è¡¨:")
                        for rec in file_records:
                            info_cols = st.columns([5, 1])
                            info = f"[#{rec['id']}] {rec['name']}ï½œ{os.path.basename(rec['path'])}"
                            if rec["uploaded_at"]:
                                info += f"ï½œ{rec['uploaded_at']}"
                            info_cols[0].write(info)
                            if info_cols[1].button("åˆ é™¤", key=f"del_file_{rec['id']}"):
                                remove_project_file(cur, conn, rec["id"])
                                st.rerun()

                with action_col:
                    if st.button("åˆ é™¤é¡¹ç›®", key=f"del_proj_{pid}"):
                        cleanup_project_files(cur, conn, pid)
                        cur.execute("DELETE FROM items WHERE id=?", (pid,))
                        cur.execute("DELETE FROM item_ext WHERE item_id=?", (pid,))
                        conn.commit()
                        st.success("é¡¹ç›®å·²åˆ é™¤")
                        st.rerun()

                # â€”â€” æ‰§è¡Œç¿»è¯‘
                if st.button("æ‰§è¡Œç¿»è¯‘", key=f"run_{pid}", type="primary"):
                    run_project_translation_ui(
                        pid=pid,
                        project_title=title,
                        src_path=selected_src_path,
                        conn=conn,
                        cur=cur
                    )

                # â€”â€” æ–°å¢ï¼šè¿›å…¥ç¿»è¯‘å·¥ä½œå°ï¼ˆå¯ç¼–è¾‘ï¼‰
                if st.button("è¿›å…¥ç¿»è¯‘å·¥ä½œå°ï¼ˆå¯ç¼–è¾‘ï¼‰", key=f"workspace_{pid}", type="secondary"):
                    # 1) ç¯å¢ƒæ£€æŸ¥ï¼šæœ‰æºæ–‡ä»¶å—ï¼Ÿ
                    if not selected_src_path or not os.path.exists(selected_src_path):
                        st.error("ç¼ºå°‘æºæ–‡ä»¶ï¼Œè¯·å…ˆåœ¨ä¸Šé¢é€‰æ‹©æˆ–ä¸Šä¼ æºæ–‡ä»¶ã€‚")
                        st.stop()
                    st.session_state[f"workspace_activated_{pid}"] = True

                    # 2) è¯»å–æºæ–‡æœ¬ï¼ˆè¿™é‡Œå®šä¹‰ src_textï¼‰
                    src_text = read_source_file(selected_src_path)

                    # 3) åˆ†æ®µ
                    blocks = split_paragraphs(src_text)
                    if not blocks:
                        st.error("æºæ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œæˆ–æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ®µè½")
                        st.stop()

                    # 4) æœ¯è¯­ï¼šç”¨ç»Ÿä¸€æ¥å£ + è½¬æˆ term_pairs ä¾›é«˜äº®ç”¨
                    term_map_all, term_meta = get_terms_for_project(cur, pid, use_dynamic=True)
                    term_pairs = list(term_map_all.items())

                    # 5) ç”¨ç»Ÿä¸€ç®¡çº¿ translate_block_with_kb åšåˆè¯‘
                    ak, model = get_deepseek()
                    if not ak:
                        st.error("æœªæ£€æµ‹åˆ° DeepSeek Keyï¼Œè¯·é…ç½® deepseek")
                        st.stop()

                    # ç¿»è¯‘æ–¹å‘ï¼ˆæ²¿ç”¨ä½ é¡¹ç›®é‡Œå·²æœ‰çš„å˜é‡ï¼‰
                    lang_pair_val = st.session_state.get(f"lang_{pid}", "ä¸­è¯‘è‹±")

                    # æ˜¯å¦å¯ç”¨è¯­ä¹‰å¬å› / å¬å›èŒƒå›´ï¼Œç›´æ¥æ²¿ç”¨ä¸Šé¢ Tab1 çš„è®¾ç½®
                    use_semantic_val = use_semantic
                    scope_val_local = scope_label

                    draft = []
                    for blk in blocks:
                        blk = (blk or "").strip()
                        if not blk:
                            draft.append("")
                            continue

                        res = translate_block_with_kb(
                            cur=cur,
                            project_id=pid,
                            block_text=blk,
                            lang_pair=lang_pair_val,
                            ak=ak,
                            model=model,
                            use_semantic=use_semantic_val,
                            scope=scope_val_local,
                            fewshot_examples=None,  # å·¥ä½œå°æ¨¡å¼å…ˆä¸æ³¨å…¥ few-shot
                        )
                        draft.append(res["tgt"])

                    # 6) ä¿å­˜åˆ° session_stateï¼Œä¾›ä¸‹é¢ç¼–è¾‘ç•Œé¢ä½¿ç”¨
                    st.session_state[f"workspace_src_{pid}"] = blocks
                    st.session_state[f"workspace_draft_{pid}"] = draft
                    st.session_state[f"workspace_terms_{pid}"] = term_pairs

                    st.success("è‰ç¨¿å·²ç”Ÿæˆï¼Œè¯·ä¸‹æ–¹å¼€å§‹ç¼–è¾‘ â†“")


                # â‘¢ ç¿»è¯‘å·¥ä½œå° UIï¼šåªæœ‰å½“ session é‡Œæœ‰è‰ç¨¿æ—¶æ‰æ˜¾ç¤º
                if st.session_state.get(f"workspace_draft_{pid}") and st.session_state.get(f"workspace_activated_{pid}", False):

                    st.markdown("## ğŸ“ ç¿»è¯‘å·¥ä½œå°ï¼ˆå¯ç¼–è¾‘ï¼‰")

                    # ä» session ä¸­å–å›è‰ç¨¿å’Œæœ¯è¯­
                    blocks = st.session_state.get(f"workspace_src_{pid}", [])
                    draft  = st.session_state.get(f"workspace_draft_{pid}", [])
                    terms  = st.session_state.get(f"workspace_terms_{pid}", [])

                    if not blocks or not draft:
                        st.info("å½“å‰æš‚æ— è‰ç¨¿ï¼Œè¯·å…ˆç‚¹å‡»â€œè¿›å…¥ç¿»è¯‘å·¥ä½œå°ï¼ˆå¯ç¼–è¾‘ï¼‰â€ç”Ÿæˆåˆç¨¿ã€‚")
                    else:
                        edited_blocks = []

                        for i, (src, trg) in enumerate(zip(blocks, draft), 1):
                            st.markdown(f"### æ®µè½ {i}")

                            col1, col2 = st.columns(2)

                            with col1:
                                st.markdown("**åŸæ–‡**")
                                st.markdown(
                                    f"<div style='padding:8px;border:1px solid #ccc;background:#f8f8f8'>{src}</div>",
                                    unsafe_allow_html=True
                                )

                            with col2:
                                st.markdown("**è¯‘æ–‡ï¼ˆå¯ç¼–è¾‘ï¼‰**")
                                new_trg = st.text_area(
                                    label="ç¼–è¾‘åçš„è¯‘æ–‡",
                                    value=trg,
                                    key=f"edit_{pid}_{i}",
                                    height=120
                                )
                                edited_blocks.append(new_trg)

                                # æœ¯è¯­é«˜äº®ï¼ˆå¦‚æœä½ å‰é¢å·²ç»å®šä¹‰äº† highlight_termsï¼‰
                                if "highlight_terms" in globals():
                                    highlighted = highlight_terms(new_trg, terms)
                                    st.markdown("æœ¯è¯­é«˜äº®ï¼š")
                                    st.markdown(
                                        f"<div style='padding:8px;border:1px solid #ccc;background:#f0fff0'>{highlighted}</div>",
                                        unsafe_allow_html=True
                                    )

                        # â€”â€” ç¡®è®¤ç”Ÿæˆæœ€ç»ˆè¯‘æ–‡ â€”â€” 
                        if st.button("âœ… ç¡®è®¤ç”Ÿæˆæœ€ç»ˆè¯‘æ–‡", key=f"confirm_{pid}", type="primary"):
                            final_text = "\n\n".join(edited_blocks)

                            # è¯­è¨€æ–¹å‘ä» session é‡Œæ‹¿ï¼ˆè·Ÿä½ ç¿»è¯‘æ—¶ä¿æŒä¸€è‡´ï¼‰
                            lang_pair_val = st.session_state.get(f"lang_{pid}", "ä¸­è¯‘è‹±")

                            cur.execute("""
                                INSERT INTO trans_ext (project_id, src_path, lang_pair, mode, output_text, created_at)
                                VALUES (?, ?, ?, ?, ?, datetime('now'))
                            """, (pid, selected_src_path, lang_pair_val, "å·¥ä½œå°æ¨¡å¼", final_text))
                            conn.commit()

                            st.success("æœ€ç»ˆè¯‘æ–‡å·²ç”Ÿæˆå¹¶å†™å…¥å†å²ï¼")

                            # æ¸…ç©ºå·¥ä½œå°è‰ç¨¿
                            st.session_state.pop(f"workspace_src_{pid}", None)
                            st.session_state.pop(f"workspace_draft_{pid}", None)
                            st.session_state.pop(f"workspace_terms_{pid}", None)
 
        # â€”â€” æ‰¹é‡åˆ é™¤æŒ‰é’®(åœ¨é¡¹ç›®åˆ—è¡¨åº•éƒ¨)
        if batch_to_delete:
            st.warning(f"å·²å‹¾é€‰ {len(batch_to_delete)} ä¸ªé¡¹ç›®ï¼Œæ“ä½œä¸å¯æ’¤é”€ã€‚")
            if st.button("ğŸ—‘ï¸ æ‰¹é‡åˆ é™¤é€‰ä¸­é¡¹ç›®", key="batch_del_projects"):
                deleted = 0
                for pid_del in batch_to_delete:
                    cleanup_project_files(cur, conn, pid_del)
                    cur.execute("DELETE FROM items WHERE id=?", (pid_del,))
                    cur.execute("DELETE FROM item_ext WHERE item_id=?", (pid_del,))
                    deleted += 1
                conn.commit()
                st.success(f"å·²æ‰¹é‡åˆ é™¤ {deleted} ä¸ªé¡¹ç›®")
                st.rerun()
        else:
            st.caption("æç¤º:å¦‚éœ€æ‰¹é‡åˆ é™¤ï¼Œå¯åœ¨ä¸Šæ–¹å‹¾é€‰å¤šä¸ªé¡¹ç›®ã€‚")

# ========== Tab2:æœ¯è¯­åº“ç®¡ç† ==========
elif choice.startswith("ğŸ“˜"):
    render_term_management(st, cur, conn, BASE_DIR, key_prefix="term")


# ========== Tab3:ç¿»è¯‘å†å²(å¢å¼ºç‰ˆ) ==========
elif choice.startswith("ğŸ“Š"):
    st.subheader("ğŸ“Š ç¿»è¯‘å†å²è®°å½•(å¯å†™å…¥è¯­æ–™ / æŠ½å–æœ¯è¯­ / ä¸‹è½½å¯¹ç…§ / åˆ é™¤)")

    rows = cur.execute("""
        SELECT id, project_id, lang_pair,
               substr(IFNULL(output_text,''),1,120) AS prev, created_at
        FROM trans_ext
        ORDER BY datetime(created_at) DESC
        LIMIT 200
    """).fetchall()

    if not rows:
        st.info("æš‚æ— å†å²è®°å½•ã€‚")
    else:
        for rid, pid, lp, prev, ts in rows:
            # é¡¹ç›®æ ‡é¢˜(åšè¯­æ–™æ ‡é¢˜/å±•ç¤º)
            ttl_row = cur.execute("SELECT IFNULL(title,'') FROM items WHERE id=?", (pid,)).fetchone()
            proj_title = (ttl_row or [""])[0] or f"project#{pid}"
            proj_domain = _project_domain(pid)

            with st.expander(f"#{rid}ï½œé¡¹ç›® {pid}ï½œ{proj_title}ï½œ{lp}ï½œ{ts}", expanded=False):
                # è¯‘æ–‡å…¨æ–‡ & æºæ–‡ä»¶è·¯å¾„
                det = cur.execute("SELECT output_text, src_path FROM trans_ext WHERE id=?", (rid,)).fetchone()
                tgt_full, src_path = det or ("", "")
                st.code(prev or "", language="text")
                st.text_area("è¯‘æ–‡å…¨æ–‡", tgt_full or "", height=220, key=f"hist_full_{rid}")

                # å°è¯•è¯»å–åŸæ–‡(å¦‚æœå½“æ—¶ä¿å­˜äº†æºæ–‡ä»¶è·¯å¾„)
                try:
                    src_full = read_source_file(src_path) if src_path else ""
                except Exception:
                    src_full = ""

                with st.expander("åŸæ–‡é¢„è§ˆ(è‹¥ä¸Šä¼ äº†æºæ–‡ä»¶)", expanded=False):
                    st.text_area("åŸæ–‡å…¨æ–‡", src_full or "(æœªä¿å­˜/æœªä¸Šä¼ æºæ–‡ä»¶)", height=160, key=f"hist_src_{rid}")

                c1, c2, c3, c4, c5 = st.columns(5)

                # 1) æ·»åŠ è¿›è¯­æ–™åº“ / æ·»åŠ +é‡å»ºç´¢å¼•
                with c1:
                    # 1-1 åªå†™å…¥è¯­æ–™åº“
                    if st.button("â• æ·»åŠ è¿›è¯­æ–™åº“", key=f"hist_add_corpus_{rid}"):
                        if not src_full and not tgt_full:
                            st.warning("åŸæ–‡å’Œè¯‘æ–‡éƒ½ä¸ºç©ºï¼Œæ— æ³•å†™å…¥è¯­æ–™åº“ã€‚")
                        else:
                            cur.execute("""
                                INSERT INTO corpus (title, project_id, lang_pair, src_text, tgt_text, note, created_at)
                                VALUES (?, ?, ?, ?, ?, ?, datetime('now'))
                            """, (
                                f"{proj_title} Â· history#{rid}",
                                pid,
                                lp or "",
                                src_full or None,
                                tgt_full or "",
                                f"from trans_ext#{rid}",
                            ))
                            conn.commit()
                            st.success("âœ… å·²å†™å…¥è¯­æ–™åº“")

                    # 1-2 å†™å…¥è¯­æ–™åº“å¹¶é‡å»ºç´¢å¼•
                    if st.button("â• æ·»åŠ å¹¶é‡å»ºç´¢å¼•", key=f"hist_add_corpus_rebuild_{rid}_{idx}"):
                        if not src_full and not tgt_full:
                            st.warning("åŸæ–‡å’Œè¯‘æ–‡éƒ½ä¸ºç©ºï¼Œæ— æ³•å†™å…¥è¯­æ–™åº“ã€‚")
                        else:
                            # å…ˆå†™å…¥è¯­æ–™åº“
                            cur.execute("""
                                INSERT INTO corpus (title, project_id, lang_pair, src_text, tgt_text, note, created_at)
                                VALUES (?, ?, ?, ?, ?, ?, datetime('now'))
                            """, (
                                f"{proj_title} Â· history#{rid}",
                                pid,
                                lp or "",
                                src_full or None,
                                tgt_full or "",
                                f"from trans_ext#{rid}",
                            ))
                            conn.commit()

                            # å†é‡å»ºè¯¥é¡¹ç›®çš„è¯­ä¹‰ç´¢å¼•
                            res_idx = rebuild_project_semantic_index(pid)
                            if res_idx.get("ok"):
                                st.success(
                                    f"âœ… å·²å†™å…¥è¯­æ–™åº“å¹¶é‡å»ºç´¢å¼•: æ–°å¢ {res_idx['added']} æ¡, æ€»é‡ {res_idx['total']} æ¡"
                                )
                            else:
                                st.warning(
                                    f"å·²å†™å…¥è¯­æ–™åº“ï¼Œä½†é‡å»ºç´¢å¼•å¤±è´¥: {res_idx.get('msg','æœªçŸ¥é”™è¯¯')}"
                                )

                # 2) æå–æœ¯è¯­(ä¼˜å…ˆè¯­æ–™åº“åŒæ¬¾æ¨¡å‹ï¼Œç¼ºçœå›é€€ DeepSeek)
                with c2:
                    if st.button("ğŸ§  æå–æœ¯è¯­", key=f"hist_extract_terms_{rid}"):
                        ak, model = get_deepseek()
                        if not ak:
                            st.info("æœªæ£€æµ‹åˆ° DeepSeek Keyï¼Œå°†ä»…ä½¿ç”¨è¯­æ–™åº“åŒæ¬¾æ¨¡å‹è¿›è¡ŒæŠ½å–ã€‚")

                        # åˆå¹¶åŸæ–‡+è¯‘æ–‡.æé«˜å€™é€‰è´¨é‡
                        big = ((src_full or "") + "\n" + (tgt_full or "")).strip()
                        res = ds_extract_terms(
                            big,
                            ak,
                            model,
                            src_lang="zh",
                            tgt_lang="en",
                            prefer_corpus_model=True,
                            default_domain=proj_domain,
                        )
                        res, dup_terms = dedup_terms_against_db(cur, res, pid)
                        if not res:
                            st.info("æœªæŠ½å–åˆ°æœ¯è¯­æˆ–è§£æå¤±è´¥")
                        else:
                            ins_term = ins_corpus = 0
                            for o in res:
                                domain_val = (o.get("domain") or proj_domain or "å…¶ä»–")
                                strategy_val = (o.get("strategy") or "history-extract")
                                cur.execute("""
                                    INSERT INTO term_ext (source_term, target_term, domain, project_id, strategy, example)
                                    VALUES (?, ?, ?, ?, ?, ?)
                                """, (
                                    o.get("source_term") or "",
                                    (o.get("target_term") or None),
                                    domain_val,
                                    pid,
                                    strategy_val,
                                    (o.get("example") or None),
                                ))
                                ins_term += 1

                                src_ex, tgt_ex = _locate_example_pair(o.get("example"), src_full, tgt_full)
                                if src_ex:
                                    cur.execute(
                                        """
                                        INSERT INTO corpus(title, project_id, lang_pair, src_text, tgt_text, note, domain, source, created_at)
                                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))
                                        """,
                                        (
                                            f"{proj_title} Â· term#{rid}",
                                            pid,
                                            lp or "",
                                            src_ex,
                                            tgt_ex,
                                            "term-example",
                                            domain_val,
                                            "history-term",
                                        ),
                                    )
                                    ins_corpus += 1
                            conn.commit()
                            msg = f"âœ… å·²å†™å…¥æœ¯è¯­åº“ {ins_term} æ¡ï¼ŒåŒæ­¥è¯­æ–™åº“ {ins_corpus} æ¡"
                            if dup_terms:
                                msg += f"ï¼›è·³è¿‡é‡å¤ {len(dup_terms)} æ¡"
                            st.success(msg)

                # 3) ä¸‹è½½åŒè¯­å¯¹ç…§(CSV / DOCX)
                with c3:
                    if st.button("â¬‡ï¸ CSV å¯¹ç…§", key=f"hist_dl_bicsv_btn_{rid}"):
                        if not src_full:
                            st.warning("æ‰¾ä¸åˆ°åŸæ–‡(æœªä¸Šä¼ æºæ–‡ä»¶).æ— æ³•ç”Ÿæˆ CSV å¯¹ç…§")
                        else:
                            try:
                                csv_name = f"bilingual_history_{rid}.csv"
                                csv_bytes = export_csv_bilingual((src_full, tgt_full),
                                    filename=f"bilingual_history_{rid}.csv"
                                )
                            except TypeError:
                                # å¦‚æœä½ çš„å¯¼å‡ºå‡½æ•°æ˜¯ textâ†’bytes ç‰ˆæœ¬
                                csv_name = f"bilingual_history_{rid}.csv"
                                csv_bytes = export_csv_bilingual(src_full, tgt_full)
                            st.download_button("ä¸‹è½½ CSV", data=csv_bytes,
                                               file_name=csv_name, mime="text/csv",
                                               key=f"hist_dl_bicsv_{rid}")

                with c4:
                    if st.button("â¬‡ï¸ DOCX å¯¹ç…§", key=f"hist_dl_bidocx_btn_{rid}"):
                        if not src_full:
                            st.warning("æ‰¾ä¸åˆ°åŸæ–‡(æœªä¸Šä¼ æºæ–‡ä»¶).æ— æ³•ç”Ÿæˆ DOCX å¯¹ç…§")
                        else:
                            try:
                                docx_path = export_docx_bilingual(
                                    filename=f"bilingual_history_{rid}.docx"
                                )
                                with open(docx_path, "rb") as f:
                                    data_docx = f.read()
                            except TypeError:
                                # å¦‚æœä½ çš„å¯¼å‡ºå‡½æ•°æ˜¯ textâ†’bytes ç‰ˆæœ¬
                                data_docx = export_docx_bilingual(src_full, tgt_full)
                            st.download_button("ä¸‹è½½ DOCX", data=data_docx,
                                               file_name=f"bilingual_history_{rid}.docx",
                                               mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                               key=f"hist_dl_bidocx_{rid}")

                # 4) ğŸ—‘ åˆ é™¤æœ¬æ¡å†å²(å®‰å…¨ç¡®è®¤)
                with c5:
                    with st.expander("ğŸ—‘ åˆ é™¤æœ¬æ¡å†å²(ä¸å¯æ¢å¤)", expanded=False):
                        st.warning("æ­¤æ“ä½œå°†æ°¸ä¹…åˆ é™¤è¯¥æ¡ trans_ext è®°å½•(ä¸å½±å“å·²å†™å…¥è¯­æ–™åº“/æœ¯è¯­è¡¨çš„æ•°æ®)ã€‚")
                        ok = st.checkbox(f"æˆ‘ç¡®è®¤åˆ é™¤ #{rid}", key=f"hist_del_ck_{rid}")
                        if st.button("ç¡®è®¤åˆ é™¤", key=f"hist_del_btn_{rid}") and ok:
                            cur.execute("DELETE FROM trans_ext WHERE id=?", (rid,))
                            conn.commit()
                            st.success("å·²åˆ é™¤.è¯·åˆ·æ–°é¡µé¢æŸ¥çœ‹ç»“æœã€‚")
                            st.stop()  # ç»ˆæ­¢æœ¬æ¬¡æ¸²æŸ“.é¿å…åœ¨å·²åˆ é™¤æ•°æ®ä¸Šç»§ç»­æ“ä½œ

                # åŸæœ‰çš„â€œä¸‹è½½è¯‘æ–‡ TXTâ€
                st.download_button("ä¸‹è½½è¯‘æ–‡ (TXT)", tgt_full or "",
                                   file_name=f"history_{rid}.txt",
                                   mime="text/plain",
                                   key=f"hist_dl_txt_{rid}")

# ========== Tab4:è¯­æ–™åº“ç®¡ç† ==========
elif choice.startswith("ğŸ“š"):
    def render_corpus_manager(st, cur, conn, pid_prefix="corpus"):
        st.header("ğŸ“š è¯­æ–™åº“ç®¡ç†")
        sk = make_sk(pid_prefix)

    render_corpus_manager(st, cur, conn)

    _ensure_project_ref_map()
    _ensure_project_switch_map()
    st.session_state.setdefault("corpus_target_project", None)
    st.session_state.setdefault("corpus_target_label", "(è¯·é€‰æ‹© Few-shot ç›®æ ‡é¡¹ç›®)")

    sub = st.tabs(["æ–°å»ºè¯­æ–™", "æµè§ˆ/æ£€ç´¢", "ä½¿ç”¨ä¸å¯¼å‡º"])

    # -------- æ–°å»ºè¯­æ–™ --------
    with sub[0]:
        st.subheader("ğŸ“¥ ä¸Šä¼  / å¯¹é½ / å…¥åº“")

        colA, colB = st.columns(2)
        with colA:
            one_file = st.file_uploader("â‘  å•ä¸ªæ–‡ä»¶(DOCX è¡¨æ ¼å¯¹ç…§ / å•è¯­ DOCX/TXT/PDF)",
                                        type=["docx", "txt", "pdf"], key="up_one")
        with colB:
            two_src = st.file_uploader("â‘¡ åŸæ–‡æ–‡ä»¶(å¯é€‰:ä¸ â‘¢ æ­é…åšå¯¹é½)",
                                       type=["docx", "txt", "csv", "pdf"], key="up_src")
            two_tgt = st.file_uploader("â‘¢ è¯‘æ–‡æ–‡ä»¶(å¯é€‰:ä¸ â‘¡ æ­é…åšå¯¹é½)",
                                       type=["docx", "txt", "csv", "pdf"], key="up_tgt")

        st.divider()
        meta1, meta2, meta3 = st.columns([2, 1, 1])
        with meta1:
            title = st.text_input("è¯­æ–™æ ‡é¢˜", value="æœªå‘½åè¯­æ–™")
        with meta2:
            lp = st.selectbox("æ–¹å‘", ["è‡ªåŠ¨", "ä¸­è¯‘è‹±", "è‹±è¯‘ä¸­"])
        with meta3:
            pid_val = st.text_input("é¡¹ç›®ID(å¯ç•™ç©º)")
        pid = int(pid_val) if pid_val.strip().isdigit() else None

        ins = 0
        pairs = []
        src_text = tgt_text = ""
        preview_df = None

        # ========== è·¯å¾„ A:å•ä¸ªæ–‡ä»¶ ==========
        if one_file is not None and (two_src is None and two_tgt is None):
            ext = (one_file.name.split(".")[-1] or "").lower()
            bio = io.BytesIO(one_file.getvalue())

            if ext == "docx":
                # å…ˆå°è¯•â€œè¡¨æ ¼å¯¹ç…§â€
                tables = read_docx_tables_info(io.BytesIO(bio.getvalue()))
                if tables:
                    st.caption("æ£€æµ‹åˆ° DOCX è¡¨æ ¼ï¼Œä¼˜å…ˆä½œä¸ºåŒè¯­å¯¹ç…§å¯¼å…¥ã€‚")
                    # ç®€å•èµ·è§ï¼Œé»˜è®¤ç¬¬ 0 å¼ è¡¨çš„ç¬¬ 0/1 åˆ—;ä½ ä¹Ÿå¯ä»¥åŠ å…¥ä¸‹æ‹‰é€‰æ‹©
                    pairs = extract_pairs_from_docx_table(
                        io.BytesIO(bio.getvalue()),
                        table_index=0,
                        src_col=0,
                        tgt_col=1
                    )
                else:
                    # æ²¡æœ‰è¡¨æ ¼ â†’ å•è¯­æ–‡æœ¬
                    src_text = read_docx_text(io.BytesIO(bio.getvalue()))

            elif ext == "txt":
                src_text = read_txt(bio)

            elif ext == "pdf":
                src_text = read_pdf_text(io.BytesIO(bio.getvalue()))

        # ========== è·¯å¾„ B:ä¸¤ä¸ªæ–‡ä»¶(åŸæ–‡ + è¯‘æ–‡)==========
        elif two_src is not None and two_tgt is not None:
            def read_any(f):
                e = (f.name.split(".")[-1] or "").lower()
                b = io.BytesIO(f.getvalue())
                if e == "docx":
                    return read_docx_text(b)
                if e == "txt":
                    return read_txt(b)
                if e == "csv":
                    try:
                        df = pd.read_csv(b)
                        return "\n".join(df.iloc[:, 0].astype(str).fillna(""))
                    except Exception:
                        return ""
                if e == "pdf":
                    return read_pdf_text(b)
                return ""
            src_text = read_any(two_src)
            tgt_text = read_any(two_tgt)

        # ========== é¢„è§ˆä¸å†³å®šå…¥åº“æ–¹å¼ ==========
        # æƒ…å†µ 1:æœ‰ pairs(æ¥è‡ª DOCX è¡¨æ ¼)
        if pairs:
            st.success(f"è§£æåˆ° {len(pairs)} å¯¹(DOCX è¡¨æ ¼)")
            preview_df = pd.DataFrame(pairs[:200], columns=["æºå¥", "ç›®æ ‡å¥"])

        # æƒ…å†µ 2:æ²¡æœ‰ pairsï¼Œä½†æ‹¿åˆ°äº† src/tgt æ–‡æœ¬ â†’ åˆ‡å¥/å¯¹é½
        elif src_text and tgt_text:
            sents_src = split_sents(src_text, "zh" if lp.startswith("ä¸­") else "auto")
            sents_tgt = split_sents(tgt_text, "en" if lp.startswith("è‹±") else "auto")
            st.caption(f"å°†å¯¹é½: src={len(sents_src)}  tgt={len(sents_tgt)}")
            if st.button("ğŸ” æ‰§è¡Œè¯­ä¹‰å¯¹é½", key="do_align"):
                pairs_aligned = align_semantic(sents_src, sents_tgt, max_jump=5)
                st.info(f"å¯¹é½å¾—åˆ° {len(pairs_aligned)} å¯¹")
                pairs = [(s, t) for (s, t, score) in pairs_aligned]
                if pairs:
                    preview_df = pd.DataFrame(pairs[:200], columns=["æºå¥", "ç›®æ ‡å¥"])

        # æƒ…å†µ 3:åªæœ‰å•è¯­æ–‡æœ¬(PDF/DOCX/TXT)
        elif src_text and not tgt_text:
            sents_src = split_sents(src_text, "zh" if lp.startswith("ä¸­") else "auto")
            st.info(f"æ£€æµ‹åˆ°å•è¯­æ–‡æœ¬ï¼Œå…± {len(sents_src)} å¥ã€‚å°†ä»¥å•è¯­è¯­æ–™å†™å…¥(è¯‘æ–‡ä¸ºç©º)ã€‚")
            preview_df = pd.DataFrame(
                [{"æºå¥": s, "ç›®æ ‡å¥": ""} for s in sents_src[:200]]
            )

        if preview_df is not None:
            st.dataframe(preview_df, width='stretch')

        # â€”â€” æŒ‰é’® + é€‰é¡¹:å¯¼å…¥è¯­æ–™åº“ | åŒæ—¶é‡å»ºç´¢å¼•
        c_imp, c_opt, c_build = st.columns([1, 1, 1])
        do_import = c_imp.button("ğŸ“¥ å†™å…¥è¯­æ–™åº“", type="primary", key="write_pairs_btn")
        do_build_opt = c_opt.checkbox("å¯¼å…¥åç«‹å³é‡å»ºç´¢å¼•", value=True, key="build_vec_opt")
        only_build_now = c_build.button("ğŸ§  ä»…é‡å»ºç´¢å¼•(ä¸å¯¼å…¥)", key="only_build")

        st.caption("æç¤º: ç´¢å¼•ä¹Ÿå¯ä»¥ç¨ååœ¨â€œä½¿ç”¨ä¸ç´¢å¼• / å¯¼å‡ºâ€é¡µçš„ C åŒºç»Ÿä¸€é‡å»ºã€‚")

        # ===== è¿™é‡Œå¼€å§‹ç”¨ç»Ÿä¸€ç®¡çº¿ =====
        # ç»Ÿä¸€ç®—ä¸€ä¸ªå…œåº•æ ‡é¢˜ï¼ˆä¼˜å…ˆç”¨ç•Œé¢ä¸Šçš„ titleï¼Œå†ç”¨æ–‡ä»¶åï¼‰
        default_title = ""
        if one_file is not None:
            default_title = one_file.name
        elif two_src is not None:
            default_title = two_src.name

        if do_import:
            import_corpus_from_upload(
                st,
                cur,
                conn,
                pid=pid,              # ä½ ä¸Šé¢ meta åŒºçš„é¡¹ç›® ID
                title=title,          # ä½ ä¸Šé¢è¾“å…¥çš„è¯­æ–™æ ‡é¢˜
                lp=lp,                # æ–¹å‘é€‰æ‹©
                pairs=pairs,
                src_text=src_text,
                tgt_text=tgt_text,
                default_title=default_title,
                build_after_import=do_build_opt,
            )

        if only_build_now:
            # ä»…é‡å»ºå½“å‰é¡¹ç›®çš„è¯­ä¹‰ç´¢å¼•(ä¸å¯¼å…¥æ–°è¯­æ–™)
            if pid:
                res_idx = rebuild_project_semantic_index(pid)
                if res_idx.get("ok"):
                    st.success(
                        f"ğŸ§  ç´¢å¼•å·²é‡å»º: æ–°å¢ {res_idx['added']}ï¼Œæ€»é‡ {res_idx['total']}ã€‚"
                    )
                else:
                    st.error(f"é‡å»ºå¤±è´¥: {res_idx.get('msg','æœªçŸ¥é”™è¯¯')}")
            else:
                st.warning("è¯·å…ˆåœ¨ä¸Šæ–¹å¡«å†™æœ‰æ•ˆçš„é¡¹ç›®IDï¼Œå†é‡å»ºç´¢å¼•ã€‚")

    # -------- æµè§ˆ/æ£€ç´¢ --------
    with sub[1]:
        st.subheader("ğŸ” æµè§ˆ/æ£€ç´¢")
        k1, k2, k3 = st.columns([2, 1, 1])
        with k1:
            kw = st.text_input("å…³é”®è¯(æ ‡é¢˜/å¤‡æ³¨/è¯‘æ–‡)", "", key=sk("kw"))
        with k2:
            lp_filter = st.selectbox("æ–¹å‘è¿‡æ»¤", ["å…¨éƒ¨", "ä¸­è¯‘è‹±", "è‹±è¯‘ä¸­", "è‡ªåŠ¨"], key=sk("lp_filter"))
        with k3:
            limit = st.number_input("æ¡æ•°", min_value=10, max_value=1000, value=200, step=10, key=sk("limit"))

        sql = """
        SELECT id, title, IFNULL(project_id,''), IFNULL(lang_pair,''), 
               substr(IFNULL(tgt_text,''),1,80), created_at
        FROM corpus
        WHERE 1=1
        """
        params = []
        if kw.strip():
            like = f"%{kw.strip()}%"
            sql += " AND (title LIKE ? OR IFNULL(note,'') LIKE ? OR IFNULL(tgt_text,'') LIKE ?)"
            params.extend([like, like, like])
        if lp_filter != "å…¨éƒ¨":
            sql += " AND lang_pair=?"
            params.append(lp_filter)
        sql += " ORDER BY id DESC LIMIT ?"
        params.append(int(limit))

        rows = cur.execute(sql, params).fetchall()
        if not rows:
            st.info("æš‚æ— åŒ¹é…è¯­æ–™ã€‚")

        else:
            for rid, ttl, pj, lpv, prev, ctime in rows:
                with st.expander(f"[{rid}] {ttl} | é¡¹ç›®:{pj} | æ–¹å‘:{lpv} | {ctime}"):
                    st.write(f"**ID**: {rid}  **é¡¹ç›®ID**: {pj}  **æ–¹å‘**: {lpv}  **æ—¶é—´**: {ctime}")
                    st.write(f"**é¢„è§ˆ(è¯‘æ–‡å‰80å­—)**: {prev}")
                    det = cur.execute(
                        "SELECT IFNULL(src_text,''), IFNULL(tgt_text,'') FROM corpus WHERE id=?",
                        (rid,)
                    ).fetchone()
                    src_all, tgt_all = det or ("", "")
                    st.text_area("æºæ–‡", src_all, height=160, key=sk(f"src_{rid}"))
                    st.text_area("è¯‘æ–‡", tgt_all, height=160, key=sk(f"tgt_{rid}"))

                    c1, c2, c3, c4 = st.columns(4)
                    with c1:
                        if st.button("åŠ å…¥å‚è€ƒé›†åˆ", key=sk(f"add_ref_{rid}")):
                            target_pid = st.session_state.get("corpus_target_project")
                            if not target_pid:
                                st.warning("è¯·å…ˆåœ¨ä¸Šæ–¹é€‰æ‹© Few-shot ç›®æ ‡é¡¹ç›®ï¼Œå†æ·»åŠ å‚è€ƒè¯­æ–™ã€‚")
                            else:
                                refs = get_project_ref_ids(target_pid)
                                refs.add(int(rid))
                                st.success(f"âœ… å·²åŠ å…¥é¡¹ç›® #{target_pid} çš„å‚è€ƒé›†åˆ(â€œä½¿ç”¨ä¸å¯¼å‡ºâ€æŸ¥çœ‹)")
                    with c2:
                        if st.button("å¯¼å‡ºTXT", key=sk(f"cor_txt_{rid}")):
                            st.download_button(
                                "ä¸‹è½½è¯‘æ–‡TXT",
                                tgt_all or "",
                                file_name=f"corpus_{rid}.txt",
                                mime="text/plain",
                                key=sk(f"cor_txt_dl_{rid}")
                            )
                    with c3:
                        if st.button("å¯¼å‡ºCSV(ä¸­è‹±å¯¹ç…§)", key=sk(f"cor_csv_{rid}")):
                            df_out = pd.DataFrame(
                                [{"source": src_all, "target": tgt_all}]
                            )
                            csv_data = df_out.to_csv(index=False)
                            st.download_button(
                                "ä¸‹è½½CSV",
                                csv_data,
                                file_name=f"corpus_{rid}.csv",
                                mime="text/csv",
                                key=sk(f"cor_csv_dl_{rid}")
                            )
                    with c4:
                        if st.button("åˆ é™¤", key=sk(f"del_{rid}")):
                            cur.execute("DELETE FROM corpus WHERE id=?", (rid,))
                            conn.commit()
                            st.warning("ğŸ—‘ï¸ å·²åˆ é™¤ï¼Œåˆ·æ–°åç”Ÿæ•ˆ")
                            st.rerun()

    # -------- ä½¿ç”¨ä¸å¯¼å‡º --------
    with sub[2]:
        st.subheader("ğŸ§© ä½¿ç”¨ä¸ç´¢å¼• / å¯¼å‡º")

        proj_rows = cur.execute("SELECT id, IFNULL(title,'(æœªå‘½å)') FROM items WHERE COALESCE(type,'')='project' ORDER BY id DESC LIMIT 200").fetchall()
        proj_options = {"(è¯·é€‰æ‹© Few-shot ç›®æ ‡é¡¹ç›®)": None}
        proj_options.update({f"[{pid}] {ttl}": pid for pid, ttl in proj_rows})
        option_labels = list(proj_options.keys())
        saved_label = st.session_state.get("corpus_target_label")
        if saved_label not in option_labels:
            saved_label = option_labels[0]
        selection_label = st.selectbox(
            "Few-shot å‚è€ƒé›†åˆå°†ç»‘å®šåˆ°å“ªä¸ªé¡¹ç›®ï¼Ÿ",
            option_labels,
            index=option_labels.index(saved_label),
            key="corpus_proj_select",
        )
        st.session_state["corpus_target_label"] = selection_label
        st.session_state["corpus_target_project"] = proj_options.get(selection_label)
        if st.session_state["corpus_target_project"]:
            st.caption(f"å½“å‰ Few-shot ç›®æ ‡: {selection_label}")
        else:
            st.caption("æœªé€‰æ‹©é¡¹ç›®:è¯·å…ˆåœ¨æ­¤æŒ‡å®šç›®æ ‡é¡¹ç›®ï¼Œå†å»å…¶ä»–å­é¡µæ·»åŠ å‚è€ƒç¤ºä¾‹ã€‚")

        # A åŒº:å‚è€ƒé›†åˆåˆå¹¶ä¸å¯¼å‡º
        target_pid = st.session_state.get("corpus_target_project")
        if not target_pid:
            st.info("è¯·å…ˆé€šè¿‡é¡µé¢ä¸Šæ–¹çš„ä¸‹æ‹‰æ¡†é€‰æ‹© Few-shot ç›®æ ‡é¡¹ç›®ï¼Œå†ç®¡ç†å‚è€ƒé›†åˆã€‚")
        else:
            ids = sorted({int(x) for x in get_project_ref_ids(target_pid)}, reverse=True)
            st.caption(f"é¡¹ç›® #{target_pid} çš„å·²é€‰å‚è€ƒæ•°: {len(ids)}")
            if ids:
                qmarks = ",".join(["?"] * len(ids))
                dets = cur.execute(
                    f"SELECT id, title, lang_pair, IFNULL(src_text,''), IFNULL(tgt_text,'') "
                    f"FROM corpus WHERE id IN ({qmarks})",
                    ids
                ).fetchall()
                order_map = {rid: idx for idx, rid in enumerate(ids)}
                dets.sort(key=lambda row: order_map.get(row[0], len(order_map)))
                merged_demo = "\n\n---\n\n".join(
                    [f"\n\næºæ–‡:\n{src}\n\nè¯‘æ–‡:\n{tgt}" for (_, _, _, src, tgt) in dets]
                )
                st.text_area("åˆå¹¶é¢„è§ˆ", merged_demo, height=240, key=sk("merge_preview"))

                cxa, cxb = st.columns(2)
                with cxa:
                    if st.button("æ¸…ç©ºå‚è€ƒé›†åˆ", key=sk(f"clear_refs_{target_pid}")):
                        refs = get_project_ref_ids(target_pid)
                        refs.clear()
                        st.info(f"å·²æ¸…ç©ºé¡¹ç›® #{target_pid} çš„å‚è€ƒé›†åˆ")
                with cxb:
                    if st.button("å¯¼å‡ºå‚è€ƒTXT", key=sk(f"export_refs_txt_{target_pid}")):
                        st.download_button(
                            "ä¸‹è½½TXT",
                            merged_demo,
                            file_name=f"corpus_refs_{target_pid}.txt",
                            mime="text/plain",
                            key=sk(f"export_refs_txt_dl_{target_pid}")
                        )
            else:
                st.info("è¿˜æ²¡æœ‰é€‰æ‹©ä»»ä½•å‚è€ƒè¯­æ–™ï¼Œå¯ä»¥åœ¨â€œæµè§ˆ/æ£€ç´¢â€ä¸­å‹¾é€‰åå†æ¥ã€‚")

        st.markdown("---")

        # B åŒº:Few-shot æ³¨å…¥å¼€å…³
        if not target_pid:
            st.info("é€‰æ‹©ç›®æ ‡é¡¹ç›®åæ‰èƒ½é…ç½® Few-shot æ³¨å…¥å¼€å…³ã€‚")
            use_fs = False
        else:
            curr_state = get_project_fewshot_enabled(target_pid)
            use_fs = st.checkbox(
                "ç¿»è¯‘æ—¶è‡ªåŠ¨æ³¨å…¥è¿™äº›å‚è€ƒè¯­æ–™ä½œä¸º Few-shot æç¤º",
                value=curr_state,
                key=sk(f"use_fs_{target_pid}")
            )
        if st.button("ä¿å­˜å‚è€ƒæ³¨å…¥å¼€å…³", key=sk("save_fs")):
            if not target_pid:
                st.warning("è¯·å…ˆé€‰æ‹© Few-shot ç›®æ ‡é¡¹ç›®")
            else:
                set_project_fewshot_enabled(target_pid, use_fs)
                st.success(
                    f"å·²æ›´æ–°:é¡¹ç›® #{target_pid} "
                    + ("å°†æ³¨å…¥å‚è€ƒ few-shot" if use_fs else "ä¸ä¼šè‡ªåŠ¨æ³¨å…¥å‚è€ƒ")
                )

        st.markdown("---")

        # C åŒº:é¡¹ç›®çº§ç´¢å¼•ç®¡ç† + è¯­ä¹‰å¬å›æµ‹è¯•
        st.subheader("ğŸ§  è¯­ä¹‰ç´¢å¼•ç®¡ç† & å¬å›æµ‹è¯•")

        # é€‰æ‹©é¡¹ç›®
        proj_rows = cur.execute(
            """
            SELECT id, title
            FROM items
            WHERE COALESCE(type,'')='project'
            ORDER BY id DESC
            LIMIT 200
            """
        ).fetchall()
        proj_map = {"(è¯·é€‰æ‹©)": None}
        proj_map.update({f"[{i}] {t}": i for (i, t) in proj_rows})
        proj_sel = st.selectbox("é€‰æ‹©è¦æµ‹è¯•/é‡å»ºç´¢å¼•çš„é¡¹ç›®", list(proj_map.keys()), key=sk("vec_proj"))
        pid_sel = proj_map.get(proj_sel)
        # æ˜¾ç¤ºå½“å‰é¡¹ç›®ç´¢å¼•ä¸­ corpus / history ç»Ÿè®¡
        idx_total = idx_corpus = idx_hist = idx_other = 0
        if pid_sel:
            try:
                mode, index_obj, mapping, vecs = _load_index(int(pid_sel))
                if isinstance(mapping, list):
                    idx_total = len(mapping)
                    for m in mapping:
                        src_tag = (m.get("source") or "").lower()
                        if src_tag == "history":
                            idx_hist += 1
                        elif src_tag in ("", "corpus"):
                            idx_corpus += 1
                        else:
                            idx_other += 1
            except Exception:
                # ç´¢å¼•æ–‡ä»¶æŸå/ä¸å­˜åœ¨æ—¶ç›´æ¥å¿½ç•¥ç»Ÿè®¡
                pass

        if pid_sel:
            st.caption(
                f"å½“å‰ç´¢å¼•æ¡æ•°: {idx_total} æ¡ "
                f"(è¯­æ–™åº“: {idx_corpus} æ¡, æ¥è‡ªç¿»è¯‘å†å²: {idx_hist} æ¡, å…¶ä»–: {idx_other} æ¡)ã€‚"
            )
        else:
            st.caption("è¯·é€‰æ‹©é¡¹ç›®ä»¥æŸ¥çœ‹è¯¥é¡¹ç›®çš„ç´¢å¼•çŠ¶æ€ã€‚")


        c_build1, c_build2 = st.columns(2)
        with c_build1:
            if st.button("æ„å»º/æ›´æ–°è¯¥é¡¹ç›®ç´¢å¼•", key=sk("build_idx_btn")):
                if pid_sel:
                    res_idx = rebuild_project_semantic_index(pid_sel)
                    if res_idx.get("ok"):
                        st.success(
                            f"ç´¢å¼•å·²æ›´æ–°: æ–°å¢ {res_idx['added']}ï¼Œæ€»é‡ {res_idx['total']}ã€‚"
                        )
                    else:
                        st.error(f"æ„å»ºå¤±è´¥: {res_idx.get('msg','æœªçŸ¥é”™è¯¯')}")
                else:
                    st.warning("è¯·å…ˆé€‰æ‹©å…·ä½“é¡¹ç›®ã€‚")

        with c_build2:
            st.caption("è¯´æ˜:ç´¢å¼•ç”¨äºè¯­ä¹‰å¬å›å‚è€ƒä¾‹å¥ï¼Œç¿»è¯‘æ—¶ç”±ç³»ç»Ÿè‡ªåŠ¨è°ƒç”¨ã€‚")

        q_demo = st.text_area("è¯•æœä¸€å¥è¯(å°†ä»¥è¯­ä¹‰ç›¸ä¼¼æ£€ç´¢å‚è€ƒ)", "", height=80, key=sk("q_demo"))
        topk = st.number_input("Top-K", 1, 10, 5, key=sk("q_topk"))
        if st.button("ğŸ” è¯­ä¹‰å¬å›æµ‹è¯•", key=sk("q_vec")):
            if pid_sel and q_demo.strip():
                hits = semantic_retrieve(pid_sel, q_demo.strip(), topk=int(topk))
                if not hits:
                    st.info("ç´¢å¼•ä¸ºç©ºæˆ–æœªå‘½ä¸­ã€‚è¯·å…ˆæ„å»ºç´¢å¼•æˆ–å¢åŠ è¯­æ–™ã€‚")
                else:
                    for row in hits:
                        sc, m, txt = row[:3]   # åªæ‹¿å‰ 3 ä¸ªï¼Œåé¢å¤šä½™çš„å¿½ç•¥
                        st.write(f"**{sc:.3f}** | {m.get('title','')} | {m.get('lang_pair','')}")
                        st.code(txt, language="text")
                        st.markdown("---")
            else:
                st.warning("è¯·å…ˆé€‰æ‹©é¡¹ç›®å¹¶è¾“å…¥è¦æ£€ç´¢çš„å†…å®¹ã€‚")

elif choice.startswith("ğŸ§ "):
    render_index_manager_by_domain(st, conn, cur)

